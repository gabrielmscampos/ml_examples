{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessingLayer(torch.nn.Module):\n",
    "    def forward(self, inputs: torch.Tensor):\n",
    "        min_val = inputs.min(axis=0).values\n",
    "        max_val = inputs.max(axis=0).values\n",
    "        normalized_inputs = torch.where(\n",
    "            (max_val - min_val) != 0,\n",
    "            (inputs - min_val) / (max_val - min_val + 1e-8),\n",
    "            torch.zeros_like(inputs)\n",
    "        )\n",
    "        return normalized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_shape: tuple[int], l2_lambda: float):\n",
    "        super().__init__()\n",
    "        self.l2_lambda = torch.tensor(l2_lambda)\n",
    "        self.prep_layer = PreprocessingLayer()\n",
    "        self.encoder1 = torch.nn.Linear(input_shape[0], 18)\n",
    "        self.encoder1_activation = torch.nn.ReLU()\n",
    "        self.encoder2 = torch.nn.Linear(18, 8)\n",
    "        self.encoder2_activation = torch.nn.Sigmoid()\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(8, 18),\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Linear(18, input_shape[0]),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "        self.mse_loss = torch.nn.MSELoss(reduction='none')\n",
    "\n",
    "    def forward(self, x):\n",
    "        prep = self.prep_layer(x)\n",
    "        encoded = self.encoder1(prep)\n",
    "        encoded = self.encoder1_activation(encoded)\n",
    "        l2_activity_loss = self.l2_lambda * torch.sum(encoded ** 2)\n",
    "        encoded = self.encoder2(encoded)\n",
    "        encoded = self.encoder2_activation(encoded)\n",
    "        decoded = self.decoder(encoded)\n",
    "        mse = self.mse_loss(prep, decoded)\n",
    "        avg_mse_per_input = torch.mean(mse, dim=1)\n",
    "        return decoded, avg_mse_per_input, l2_activity_loss\n",
    "    \n",
    "    def fit(self, inputs: np.ndarray, batch_size: int, epochs: int, optimizer, criterion):\n",
    "        inputs = torch.tensor(inputs.astype(np.float32))\n",
    "        targets = PreprocessingLayer()(inputs)\n",
    "        batched_data = DataLoader(targets, batch_size=batch_size, shuffle=True)\n",
    "        batch_steps = len(batched_data)\n",
    "        training_loss = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.train()\n",
    "            epoch_decoder_losses = []\n",
    "            epoch_total_losses = []\n",
    "            for idx, batch in enumerate(batched_data):\n",
    "                # Forward pass\n",
    "                decoded, _, l2_loss = self(batch)\n",
    "                loss = criterion(batch, decoded)\n",
    "                loss_value = loss.item()\n",
    "                epoch_decoder_losses.append(loss_value)\n",
    "\n",
    "                # Compute total loss\n",
    "                total_loss = loss + l2_loss\n",
    "                total_loss_value = total_loss.item()  # This slows down the code if using GPU, since we convert this value from CUDA to a python float\n",
    "                epoch_total_losses.append(total_loss_value)\n",
    "                \n",
    "                # Backward pass and optimization\n",
    "                optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                print(f\"Epoch [{epoch + 1}/{epochs}], Step [{idx + 1}/{batch_steps}], decoder loss: {loss_value:.4f}, encoder1_activation l2 loss: {l2_loss:.4f}, total loss: {total_loss_value:.4f}\")\n",
    "\n",
    "            epoch_decoder_avg_loss = np.array(epoch_decoder_losses).mean()\n",
    "            epoch_total_avg_loss = np.array(epoch_total_losses).mean()\n",
    "            training_loss.append(epoch_total_avg_loss)\n",
    "\n",
    "            print(f\"Epoch [{epoch + 1}/{epochs}] (avg {batch_steps} steps), decoder loss: {epoch_decoder_avg_loss:.4f}, total loss: {epoch_total_avg_loss:.4f}\")\n",
    "        \n",
    "        return training_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1940., 1987.,  670., ...,    0.,    0.,    0.],\n",
       "       [1869., 1872.,  714., ...,    0.,    0.,    0.],\n",
       "       [1819., 1924.,  672., ...,    0.,    0.,    0.],\n",
       "       ...,\n",
       "       [1171.,  989.,  293., ...,    0.,    0.,    0.],\n",
       "       [1225.,  960.,  289., ...,    0.,    0.,    0.],\n",
       "       [1190.,  994.,  257., ...,    0.,    0.,    0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = np.load(\"../data/data_386642.npy\")\n",
    "train_label = np.load(\"../data/label_386642.npy\")\n",
    "\n",
    "# We want to feed the Autoencoder with GOOD data, so we filter the data by the label == 1\n",
    "train_data = train_data[train_label == 1]\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae = Autoencoder(input_shape=(51,), l2_lambda=1e-4)\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(ae.parameters(), lr=0.001, eps=1e-07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Step [1/6], decoder loss: 0.1990, encoder1_activation l2 loss: 0.0018, total loss: 0.2008\n",
      "Epoch [1/200], Step [2/6], decoder loss: 0.1967, encoder1_activation l2 loss: 0.0016, total loss: 0.1982\n",
      "Epoch [1/200], Step [3/6], decoder loss: 0.1939, encoder1_activation l2 loss: 0.0014, total loss: 0.1953\n",
      "Epoch [1/200], Step [4/6], decoder loss: 0.1929, encoder1_activation l2 loss: 0.0014, total loss: 0.1943\n",
      "Epoch [1/200], Step [5/6], decoder loss: 0.1908, encoder1_activation l2 loss: 0.0014, total loss: 0.1921\n",
      "Epoch [1/200], Step [6/6], decoder loss: 0.1889, encoder1_activation l2 loss: 0.0007, total loss: 0.1896\n",
      "Epoch [1/200] (avg 6 steps), decoder loss: 0.1937, total loss: 0.1951\n",
      "Epoch [2/200], Step [1/6], decoder loss: 0.1870, encoder1_activation l2 loss: 0.0013, total loss: 0.1883\n",
      "Epoch [2/200], Step [2/6], decoder loss: 0.1849, encoder1_activation l2 loss: 0.0010, total loss: 0.1859\n",
      "Epoch [2/200], Step [3/6], decoder loss: 0.1824, encoder1_activation l2 loss: 0.0010, total loss: 0.1833\n",
      "Epoch [2/200], Step [4/6], decoder loss: 0.1805, encoder1_activation l2 loss: 0.0008, total loss: 0.1813\n",
      "Epoch [2/200], Step [5/6], decoder loss: 0.1786, encoder1_activation l2 loss: 0.0008, total loss: 0.1794\n",
      "Epoch [2/200], Step [6/6], decoder loss: 0.1763, encoder1_activation l2 loss: 0.0004, total loss: 0.1768\n",
      "Epoch [2/200] (avg 6 steps), decoder loss: 0.1816, total loss: 0.1825\n",
      "Epoch [3/200], Step [1/6], decoder loss: 0.1750, encoder1_activation l2 loss: 0.0007, total loss: 0.1757\n",
      "Epoch [3/200], Step [2/6], decoder loss: 0.1729, encoder1_activation l2 loss: 0.0006, total loss: 0.1736\n",
      "Epoch [3/200], Step [3/6], decoder loss: 0.1713, encoder1_activation l2 loss: 0.0006, total loss: 0.1719\n",
      "Epoch [3/200], Step [4/6], decoder loss: 0.1694, encoder1_activation l2 loss: 0.0006, total loss: 0.1700\n",
      "Epoch [3/200], Step [5/6], decoder loss: 0.1669, encoder1_activation l2 loss: 0.0005, total loss: 0.1675\n",
      "Epoch [3/200], Step [6/6], decoder loss: 0.1649, encoder1_activation l2 loss: 0.0003, total loss: 0.1651\n",
      "Epoch [3/200] (avg 6 steps), decoder loss: 0.1701, total loss: 0.1706\n",
      "Epoch [4/200], Step [1/6], decoder loss: 0.1638, encoder1_activation l2 loss: 0.0004, total loss: 0.1642\n",
      "Epoch [4/200], Step [2/6], decoder loss: 0.1621, encoder1_activation l2 loss: 0.0004, total loss: 0.1625\n",
      "Epoch [4/200], Step [3/6], decoder loss: 0.1599, encoder1_activation l2 loss: 0.0003, total loss: 0.1602\n",
      "Epoch [4/200], Step [4/6], decoder loss: 0.1577, encoder1_activation l2 loss: 0.0004, total loss: 0.1581\n",
      "Epoch [4/200], Step [5/6], decoder loss: 0.1564, encoder1_activation l2 loss: 0.0003, total loss: 0.1567\n",
      "Epoch [4/200], Step [6/6], decoder loss: 0.1540, encoder1_activation l2 loss: 0.0002, total loss: 0.1542\n",
      "Epoch [4/200] (avg 6 steps), decoder loss: 0.1590, total loss: 0.1593\n",
      "Epoch [5/200], Step [1/6], decoder loss: 0.1528, encoder1_activation l2 loss: 0.0003, total loss: 0.1531\n",
      "Epoch [5/200], Step [2/6], decoder loss: 0.1511, encoder1_activation l2 loss: 0.0002, total loss: 0.1513\n",
      "Epoch [5/200], Step [3/6], decoder loss: 0.1490, encoder1_activation l2 loss: 0.0002, total loss: 0.1492\n",
      "Epoch [5/200], Step [4/6], decoder loss: 0.1475, encoder1_activation l2 loss: 0.0002, total loss: 0.1477\n",
      "Epoch [5/200], Step [5/6], decoder loss: 0.1464, encoder1_activation l2 loss: 0.0002, total loss: 0.1466\n",
      "Epoch [5/200], Step [6/6], decoder loss: 0.1432, encoder1_activation l2 loss: 0.0001, total loss: 0.1433\n",
      "Epoch [5/200] (avg 6 steps), decoder loss: 0.1483, total loss: 0.1485\n",
      "Epoch [6/200], Step [1/6], decoder loss: 0.1422, encoder1_activation l2 loss: 0.0001, total loss: 0.1423\n",
      "Epoch [6/200], Step [2/6], decoder loss: 0.1406, encoder1_activation l2 loss: 0.0002, total loss: 0.1407\n",
      "Epoch [6/200], Step [3/6], decoder loss: 0.1393, encoder1_activation l2 loss: 0.0002, total loss: 0.1394\n",
      "Epoch [6/200], Step [4/6], decoder loss: 0.1369, encoder1_activation l2 loss: 0.0001, total loss: 0.1370\n",
      "Epoch [6/200], Step [5/6], decoder loss: 0.1365, encoder1_activation l2 loss: 0.0001, total loss: 0.1367\n",
      "Epoch [6/200], Step [6/6], decoder loss: 0.1330, encoder1_activation l2 loss: 0.0001, total loss: 0.1330\n",
      "Epoch [6/200] (avg 6 steps), decoder loss: 0.1381, total loss: 0.1382\n",
      "Epoch [7/200], Step [1/6], decoder loss: 0.1324, encoder1_activation l2 loss: 0.0001, total loss: 0.1325\n",
      "Epoch [7/200], Step [2/6], decoder loss: 0.1304, encoder1_activation l2 loss: 0.0001, total loss: 0.1304\n",
      "Epoch [7/200], Step [3/6], decoder loss: 0.1299, encoder1_activation l2 loss: 0.0001, total loss: 0.1300\n",
      "Epoch [7/200], Step [4/6], decoder loss: 0.1278, encoder1_activation l2 loss: 0.0001, total loss: 0.1279\n",
      "Epoch [7/200], Step [5/6], decoder loss: 0.1253, encoder1_activation l2 loss: 0.0001, total loss: 0.1253\n",
      "Epoch [7/200], Step [6/6], decoder loss: 0.1246, encoder1_activation l2 loss: 0.0000, total loss: 0.1246\n",
      "Epoch [7/200] (avg 6 steps), decoder loss: 0.1284, total loss: 0.1285\n",
      "Epoch [8/200], Step [1/6], decoder loss: 0.1239, encoder1_activation l2 loss: 0.0001, total loss: 0.1240\n",
      "Epoch [8/200], Step [2/6], decoder loss: 0.1211, encoder1_activation l2 loss: 0.0001, total loss: 0.1212\n",
      "Epoch [8/200], Step [3/6], decoder loss: 0.1195, encoder1_activation l2 loss: 0.0001, total loss: 0.1196\n",
      "Epoch [8/200], Step [4/6], decoder loss: 0.1171, encoder1_activation l2 loss: 0.0001, total loss: 0.1171\n",
      "Epoch [8/200], Step [5/6], decoder loss: 0.1171, encoder1_activation l2 loss: 0.0001, total loss: 0.1172\n",
      "Epoch [8/200], Step [6/6], decoder loss: 0.1155, encoder1_activation l2 loss: 0.0000, total loss: 0.1156\n",
      "Epoch [8/200] (avg 6 steps), decoder loss: 0.1190, total loss: 0.1191\n",
      "Epoch [9/200], Step [1/6], decoder loss: 0.1145, encoder1_activation l2 loss: 0.0001, total loss: 0.1146\n",
      "Epoch [9/200], Step [2/6], decoder loss: 0.1116, encoder1_activation l2 loss: 0.0000, total loss: 0.1117\n",
      "Epoch [9/200], Step [3/6], decoder loss: 0.1106, encoder1_activation l2 loss: 0.0001, total loss: 0.1107\n",
      "Epoch [9/200], Step [4/6], decoder loss: 0.1098, encoder1_activation l2 loss: 0.0000, total loss: 0.1098\n",
      "Epoch [9/200], Step [5/6], decoder loss: 0.1080, encoder1_activation l2 loss: 0.0000, total loss: 0.1081\n",
      "Epoch [9/200], Step [6/6], decoder loss: 0.1063, encoder1_activation l2 loss: 0.0000, total loss: 0.1063\n",
      "Epoch [9/200] (avg 6 steps), decoder loss: 0.1101, total loss: 0.1102\n",
      "Epoch [10/200], Step [1/6], decoder loss: 0.1049, encoder1_activation l2 loss: 0.0001, total loss: 0.1049\n",
      "Epoch [10/200], Step [2/6], decoder loss: 0.1036, encoder1_activation l2 loss: 0.0000, total loss: 0.1036\n",
      "Epoch [10/200], Step [3/6], decoder loss: 0.1024, encoder1_activation l2 loss: 0.0000, total loss: 0.1025\n",
      "Epoch [10/200], Step [4/6], decoder loss: 0.1009, encoder1_activation l2 loss: 0.0000, total loss: 0.1009\n",
      "Epoch [10/200], Step [5/6], decoder loss: 0.0996, encoder1_activation l2 loss: 0.0000, total loss: 0.0996\n",
      "Epoch [10/200], Step [6/6], decoder loss: 0.1003, encoder1_activation l2 loss: 0.0000, total loss: 0.1003\n",
      "Epoch [10/200] (avg 6 steps), decoder loss: 0.1019, total loss: 0.1020\n",
      "Epoch [11/200], Step [1/6], decoder loss: 0.0968, encoder1_activation l2 loss: 0.0000, total loss: 0.0969\n",
      "Epoch [11/200], Step [2/6], decoder loss: 0.0961, encoder1_activation l2 loss: 0.0000, total loss: 0.0962\n",
      "Epoch [11/200], Step [3/6], decoder loss: 0.0943, encoder1_activation l2 loss: 0.0000, total loss: 0.0943\n",
      "Epoch [11/200], Step [4/6], decoder loss: 0.0933, encoder1_activation l2 loss: 0.0000, total loss: 0.0933\n",
      "Epoch [11/200], Step [5/6], decoder loss: 0.0923, encoder1_activation l2 loss: 0.0000, total loss: 0.0923\n",
      "Epoch [11/200], Step [6/6], decoder loss: 0.0914, encoder1_activation l2 loss: 0.0000, total loss: 0.0914\n",
      "Epoch [11/200] (avg 6 steps), decoder loss: 0.0940, total loss: 0.0941\n",
      "Epoch [12/200], Step [1/6], decoder loss: 0.0892, encoder1_activation l2 loss: 0.0000, total loss: 0.0892\n",
      "Epoch [12/200], Step [2/6], decoder loss: 0.0892, encoder1_activation l2 loss: 0.0000, total loss: 0.0892\n",
      "Epoch [12/200], Step [3/6], decoder loss: 0.0868, encoder1_activation l2 loss: 0.0000, total loss: 0.0868\n",
      "Epoch [12/200], Step [4/6], decoder loss: 0.0866, encoder1_activation l2 loss: 0.0000, total loss: 0.0866\n",
      "Epoch [12/200], Step [5/6], decoder loss: 0.0833, encoder1_activation l2 loss: 0.0000, total loss: 0.0833\n",
      "Epoch [12/200], Step [6/6], decoder loss: 0.0860, encoder1_activation l2 loss: 0.0000, total loss: 0.0860\n",
      "Epoch [12/200] (avg 6 steps), decoder loss: 0.0868, total loss: 0.0868\n",
      "Epoch [13/200], Step [1/6], decoder loss: 0.0820, encoder1_activation l2 loss: 0.0000, total loss: 0.0820\n",
      "Epoch [13/200], Step [2/6], decoder loss: 0.0814, encoder1_activation l2 loss: 0.0000, total loss: 0.0814\n",
      "Epoch [13/200], Step [3/6], decoder loss: 0.0809, encoder1_activation l2 loss: 0.0000, total loss: 0.0809\n",
      "Epoch [13/200], Step [4/6], decoder loss: 0.0801, encoder1_activation l2 loss: 0.0000, total loss: 0.0802\n",
      "Epoch [13/200], Step [5/6], decoder loss: 0.0774, encoder1_activation l2 loss: 0.0000, total loss: 0.0774\n",
      "Epoch [13/200], Step [6/6], decoder loss: 0.0779, encoder1_activation l2 loss: 0.0000, total loss: 0.0779\n",
      "Epoch [13/200] (avg 6 steps), decoder loss: 0.0800, total loss: 0.0800\n",
      "Epoch [14/200], Step [1/6], decoder loss: 0.0762, encoder1_activation l2 loss: 0.0000, total loss: 0.0763\n",
      "Epoch [14/200], Step [2/6], decoder loss: 0.0764, encoder1_activation l2 loss: 0.0000, total loss: 0.0764\n",
      "Epoch [14/200], Step [3/6], decoder loss: 0.0733, encoder1_activation l2 loss: 0.0000, total loss: 0.0733\n",
      "Epoch [14/200], Step [4/6], decoder loss: 0.0730, encoder1_activation l2 loss: 0.0000, total loss: 0.0730\n",
      "Epoch [14/200], Step [5/6], decoder loss: 0.0720, encoder1_activation l2 loss: 0.0000, total loss: 0.0721\n",
      "Epoch [14/200], Step [6/6], decoder loss: 0.0709, encoder1_activation l2 loss: 0.0000, total loss: 0.0709\n",
      "Epoch [14/200] (avg 6 steps), decoder loss: 0.0736, total loss: 0.0737\n",
      "Epoch [15/200], Step [1/6], decoder loss: 0.0693, encoder1_activation l2 loss: 0.0000, total loss: 0.0694\n",
      "Epoch [15/200], Step [2/6], decoder loss: 0.0701, encoder1_activation l2 loss: 0.0000, total loss: 0.0701\n",
      "Epoch [15/200], Step [3/6], decoder loss: 0.0686, encoder1_activation l2 loss: 0.0000, total loss: 0.0686\n",
      "Epoch [15/200], Step [4/6], decoder loss: 0.0671, encoder1_activation l2 loss: 0.0000, total loss: 0.0672\n",
      "Epoch [15/200], Step [5/6], decoder loss: 0.0672, encoder1_activation l2 loss: 0.0000, total loss: 0.0672\n",
      "Epoch [15/200], Step [6/6], decoder loss: 0.0649, encoder1_activation l2 loss: 0.0000, total loss: 0.0650\n",
      "Epoch [15/200] (avg 6 steps), decoder loss: 0.0679, total loss: 0.0679\n",
      "Epoch [16/200], Step [1/6], decoder loss: 0.0645, encoder1_activation l2 loss: 0.0000, total loss: 0.0645\n",
      "Epoch [16/200], Step [2/6], decoder loss: 0.0633, encoder1_activation l2 loss: 0.0000, total loss: 0.0633\n",
      "Epoch [16/200], Step [3/6], decoder loss: 0.0629, encoder1_activation l2 loss: 0.0000, total loss: 0.0629\n",
      "Epoch [16/200], Step [4/6], decoder loss: 0.0638, encoder1_activation l2 loss: 0.0000, total loss: 0.0638\n",
      "Epoch [16/200], Step [5/6], decoder loss: 0.0615, encoder1_activation l2 loss: 0.0000, total loss: 0.0615\n",
      "Epoch [16/200], Step [6/6], decoder loss: 0.0598, encoder1_activation l2 loss: 0.0000, total loss: 0.0598\n",
      "Epoch [16/200] (avg 6 steps), decoder loss: 0.0626, total loss: 0.0627\n",
      "Epoch [17/200], Step [1/6], decoder loss: 0.0610, encoder1_activation l2 loss: 0.0000, total loss: 0.0610\n",
      "Epoch [17/200], Step [2/6], decoder loss: 0.0585, encoder1_activation l2 loss: 0.0000, total loss: 0.0585\n",
      "Epoch [17/200], Step [3/6], decoder loss: 0.0588, encoder1_activation l2 loss: 0.0000, total loss: 0.0588\n",
      "Epoch [17/200], Step [4/6], decoder loss: 0.0570, encoder1_activation l2 loss: 0.0000, total loss: 0.0570\n",
      "Epoch [17/200], Step [5/6], decoder loss: 0.0568, encoder1_activation l2 loss: 0.0000, total loss: 0.0568\n",
      "Epoch [17/200], Step [6/6], decoder loss: 0.0552, encoder1_activation l2 loss: 0.0000, total loss: 0.0552\n",
      "Epoch [17/200] (avg 6 steps), decoder loss: 0.0579, total loss: 0.0579\n",
      "Epoch [18/200], Step [1/6], decoder loss: 0.0549, encoder1_activation l2 loss: 0.0000, total loss: 0.0550\n",
      "Epoch [18/200], Step [2/6], decoder loss: 0.0534, encoder1_activation l2 loss: 0.0000, total loss: 0.0534\n",
      "Epoch [18/200], Step [3/6], decoder loss: 0.0537, encoder1_activation l2 loss: 0.0000, total loss: 0.0537\n",
      "Epoch [18/200], Step [4/6], decoder loss: 0.0536, encoder1_activation l2 loss: 0.0000, total loss: 0.0537\n",
      "Epoch [18/200], Step [5/6], decoder loss: 0.0536, encoder1_activation l2 loss: 0.0000, total loss: 0.0536\n",
      "Epoch [18/200], Step [6/6], decoder loss: 0.0532, encoder1_activation l2 loss: 0.0000, total loss: 0.0532\n",
      "Epoch [18/200] (avg 6 steps), decoder loss: 0.0537, total loss: 0.0537\n",
      "Epoch [19/200], Step [1/6], decoder loss: 0.0522, encoder1_activation l2 loss: 0.0000, total loss: 0.0522\n",
      "Epoch [19/200], Step [2/6], decoder loss: 0.0500, encoder1_activation l2 loss: 0.0000, total loss: 0.0500\n",
      "Epoch [19/200], Step [3/6], decoder loss: 0.0502, encoder1_activation l2 loss: 0.0000, total loss: 0.0503\n",
      "Epoch [19/200], Step [4/6], decoder loss: 0.0497, encoder1_activation l2 loss: 0.0000, total loss: 0.0497\n",
      "Epoch [19/200], Step [5/6], decoder loss: 0.0477, encoder1_activation l2 loss: 0.0000, total loss: 0.0477\n",
      "Epoch [19/200], Step [6/6], decoder loss: 0.0490, encoder1_activation l2 loss: 0.0000, total loss: 0.0490\n",
      "Epoch [19/200] (avg 6 steps), decoder loss: 0.0498, total loss: 0.0498\n",
      "Epoch [20/200], Step [1/6], decoder loss: 0.0478, encoder1_activation l2 loss: 0.0000, total loss: 0.0478\n",
      "Epoch [20/200], Step [2/6], decoder loss: 0.0469, encoder1_activation l2 loss: 0.0000, total loss: 0.0470\n",
      "Epoch [20/200], Step [3/6], decoder loss: 0.0456, encoder1_activation l2 loss: 0.0000, total loss: 0.0457\n",
      "Epoch [20/200], Step [4/6], decoder loss: 0.0462, encoder1_activation l2 loss: 0.0000, total loss: 0.0462\n",
      "Epoch [20/200], Step [5/6], decoder loss: 0.0466, encoder1_activation l2 loss: 0.0000, total loss: 0.0466\n",
      "Epoch [20/200], Step [6/6], decoder loss: 0.0440, encoder1_activation l2 loss: 0.0000, total loss: 0.0440\n",
      "Epoch [20/200] (avg 6 steps), decoder loss: 0.0462, total loss: 0.0462\n",
      "Epoch [21/200], Step [1/6], decoder loss: 0.0425, encoder1_activation l2 loss: 0.0000, total loss: 0.0425\n",
      "Epoch [21/200], Step [2/6], decoder loss: 0.0447, encoder1_activation l2 loss: 0.0000, total loss: 0.0447\n",
      "Epoch [21/200], Step [3/6], decoder loss: 0.0436, encoder1_activation l2 loss: 0.0000, total loss: 0.0437\n",
      "Epoch [21/200], Step [4/6], decoder loss: 0.0432, encoder1_activation l2 loss: 0.0000, total loss: 0.0432\n",
      "Epoch [21/200], Step [5/6], decoder loss: 0.0417, encoder1_activation l2 loss: 0.0000, total loss: 0.0417\n",
      "Epoch [21/200], Step [6/6], decoder loss: 0.0438, encoder1_activation l2 loss: 0.0000, total loss: 0.0438\n",
      "Epoch [21/200] (avg 6 steps), decoder loss: 0.0433, total loss: 0.0433\n",
      "Epoch [22/200], Step [1/6], decoder loss: 0.0399, encoder1_activation l2 loss: 0.0000, total loss: 0.0399\n",
      "Epoch [22/200], Step [2/6], decoder loss: 0.0410, encoder1_activation l2 loss: 0.0000, total loss: 0.0410\n",
      "Epoch [22/200], Step [3/6], decoder loss: 0.0401, encoder1_activation l2 loss: 0.0000, total loss: 0.0401\n",
      "Epoch [22/200], Step [4/6], decoder loss: 0.0417, encoder1_activation l2 loss: 0.0000, total loss: 0.0417\n",
      "Epoch [22/200], Step [5/6], decoder loss: 0.0395, encoder1_activation l2 loss: 0.0000, total loss: 0.0395\n",
      "Epoch [22/200], Step [6/6], decoder loss: 0.0402, encoder1_activation l2 loss: 0.0000, total loss: 0.0402\n",
      "Epoch [22/200] (avg 6 steps), decoder loss: 0.0404, total loss: 0.0404\n",
      "Epoch [23/200], Step [1/6], decoder loss: 0.0393, encoder1_activation l2 loss: 0.0000, total loss: 0.0393\n",
      "Epoch [23/200], Step [2/6], decoder loss: 0.0383, encoder1_activation l2 loss: 0.0000, total loss: 0.0383\n",
      "Epoch [23/200], Step [3/6], decoder loss: 0.0387, encoder1_activation l2 loss: 0.0000, total loss: 0.0387\n",
      "Epoch [23/200], Step [4/6], decoder loss: 0.0379, encoder1_activation l2 loss: 0.0000, total loss: 0.0379\n",
      "Epoch [23/200], Step [5/6], decoder loss: 0.0366, encoder1_activation l2 loss: 0.0000, total loss: 0.0366\n",
      "Epoch [23/200], Step [6/6], decoder loss: 0.0354, encoder1_activation l2 loss: 0.0000, total loss: 0.0354\n",
      "Epoch [23/200] (avg 6 steps), decoder loss: 0.0377, total loss: 0.0377\n",
      "Epoch [24/200], Step [1/6], decoder loss: 0.0375, encoder1_activation l2 loss: 0.0000, total loss: 0.0375\n",
      "Epoch [24/200], Step [2/6], decoder loss: 0.0362, encoder1_activation l2 loss: 0.0000, total loss: 0.0362\n",
      "Epoch [24/200], Step [3/6], decoder loss: 0.0348, encoder1_activation l2 loss: 0.0000, total loss: 0.0348\n",
      "Epoch [24/200], Step [4/6], decoder loss: 0.0346, encoder1_activation l2 loss: 0.0000, total loss: 0.0346\n",
      "Epoch [24/200], Step [5/6], decoder loss: 0.0352, encoder1_activation l2 loss: 0.0000, total loss: 0.0352\n",
      "Epoch [24/200], Step [6/6], decoder loss: 0.0352, encoder1_activation l2 loss: 0.0000, total loss: 0.0352\n",
      "Epoch [24/200] (avg 6 steps), decoder loss: 0.0356, total loss: 0.0356\n",
      "Epoch [25/200], Step [1/6], decoder loss: 0.0334, encoder1_activation l2 loss: 0.0000, total loss: 0.0334\n",
      "Epoch [25/200], Step [2/6], decoder loss: 0.0359, encoder1_activation l2 loss: 0.0000, total loss: 0.0359\n",
      "Epoch [25/200], Step [3/6], decoder loss: 0.0339, encoder1_activation l2 loss: 0.0000, total loss: 0.0339\n",
      "Epoch [25/200], Step [4/6], decoder loss: 0.0330, encoder1_activation l2 loss: 0.0000, total loss: 0.0330\n",
      "Epoch [25/200], Step [5/6], decoder loss: 0.0321, encoder1_activation l2 loss: 0.0000, total loss: 0.0321\n",
      "Epoch [25/200], Step [6/6], decoder loss: 0.0331, encoder1_activation l2 loss: 0.0000, total loss: 0.0331\n",
      "Epoch [25/200] (avg 6 steps), decoder loss: 0.0336, total loss: 0.0336\n",
      "Epoch [26/200], Step [1/6], decoder loss: 0.0318, encoder1_activation l2 loss: 0.0000, total loss: 0.0319\n",
      "Epoch [26/200], Step [2/6], decoder loss: 0.0330, encoder1_activation l2 loss: 0.0000, total loss: 0.0330\n",
      "Epoch [26/200], Step [3/6], decoder loss: 0.0309, encoder1_activation l2 loss: 0.0000, total loss: 0.0309\n",
      "Epoch [26/200], Step [4/6], decoder loss: 0.0306, encoder1_activation l2 loss: 0.0000, total loss: 0.0306\n",
      "Epoch [26/200], Step [5/6], decoder loss: 0.0321, encoder1_activation l2 loss: 0.0000, total loss: 0.0321\n",
      "Epoch [26/200], Step [6/6], decoder loss: 0.0328, encoder1_activation l2 loss: 0.0000, total loss: 0.0328\n",
      "Epoch [26/200] (avg 6 steps), decoder loss: 0.0319, total loss: 0.0319\n",
      "Epoch [27/200], Step [1/6], decoder loss: 0.0308, encoder1_activation l2 loss: 0.0000, total loss: 0.0308\n",
      "Epoch [27/200], Step [2/6], decoder loss: 0.0299, encoder1_activation l2 loss: 0.0000, total loss: 0.0299\n",
      "Epoch [27/200], Step [3/6], decoder loss: 0.0299, encoder1_activation l2 loss: 0.0000, total loss: 0.0299\n",
      "Epoch [27/200], Step [4/6], decoder loss: 0.0304, encoder1_activation l2 loss: 0.0000, total loss: 0.0305\n",
      "Epoch [27/200], Step [5/6], decoder loss: 0.0300, encoder1_activation l2 loss: 0.0000, total loss: 0.0300\n",
      "Epoch [27/200], Step [6/6], decoder loss: 0.0300, encoder1_activation l2 loss: 0.0000, total loss: 0.0300\n",
      "Epoch [27/200] (avg 6 steps), decoder loss: 0.0302, total loss: 0.0302\n",
      "Epoch [28/200], Step [1/6], decoder loss: 0.0285, encoder1_activation l2 loss: 0.0000, total loss: 0.0285\n",
      "Epoch [28/200], Step [2/6], decoder loss: 0.0297, encoder1_activation l2 loss: 0.0000, total loss: 0.0297\n",
      "Epoch [28/200], Step [3/6], decoder loss: 0.0290, encoder1_activation l2 loss: 0.0000, total loss: 0.0290\n",
      "Epoch [28/200], Step [4/6], decoder loss: 0.0279, encoder1_activation l2 loss: 0.0000, total loss: 0.0279\n",
      "Epoch [28/200], Step [5/6], decoder loss: 0.0282, encoder1_activation l2 loss: 0.0000, total loss: 0.0282\n",
      "Epoch [28/200], Step [6/6], decoder loss: 0.0293, encoder1_activation l2 loss: 0.0000, total loss: 0.0293\n",
      "Epoch [28/200] (avg 6 steps), decoder loss: 0.0288, total loss: 0.0288\n",
      "Epoch [29/200], Step [1/6], decoder loss: 0.0273, encoder1_activation l2 loss: 0.0000, total loss: 0.0273\n",
      "Epoch [29/200], Step [2/6], decoder loss: 0.0278, encoder1_activation l2 loss: 0.0000, total loss: 0.0278\n",
      "Epoch [29/200], Step [3/6], decoder loss: 0.0283, encoder1_activation l2 loss: 0.0000, total loss: 0.0283\n",
      "Epoch [29/200], Step [4/6], decoder loss: 0.0289, encoder1_activation l2 loss: 0.0000, total loss: 0.0289\n",
      "Epoch [29/200], Step [5/6], decoder loss: 0.0251, encoder1_activation l2 loss: 0.0000, total loss: 0.0251\n",
      "Epoch [29/200], Step [6/6], decoder loss: 0.0271, encoder1_activation l2 loss: 0.0000, total loss: 0.0271\n",
      "Epoch [29/200] (avg 6 steps), decoder loss: 0.0274, total loss: 0.0274\n",
      "Epoch [30/200], Step [1/6], decoder loss: 0.0268, encoder1_activation l2 loss: 0.0000, total loss: 0.0268\n",
      "Epoch [30/200], Step [2/6], decoder loss: 0.0265, encoder1_activation l2 loss: 0.0000, total loss: 0.0265\n",
      "Epoch [30/200], Step [3/6], decoder loss: 0.0265, encoder1_activation l2 loss: 0.0000, total loss: 0.0266\n",
      "Epoch [30/200], Step [4/6], decoder loss: 0.0274, encoder1_activation l2 loss: 0.0000, total loss: 0.0274\n",
      "Epoch [30/200], Step [5/6], decoder loss: 0.0257, encoder1_activation l2 loss: 0.0000, total loss: 0.0257\n",
      "Epoch [30/200], Step [6/6], decoder loss: 0.0235, encoder1_activation l2 loss: 0.0000, total loss: 0.0235\n",
      "Epoch [30/200] (avg 6 steps), decoder loss: 0.0261, total loss: 0.0261\n",
      "Epoch [31/200], Step [1/6], decoder loss: 0.0262, encoder1_activation l2 loss: 0.0000, total loss: 0.0262\n",
      "Epoch [31/200], Step [2/6], decoder loss: 0.0252, encoder1_activation l2 loss: 0.0000, total loss: 0.0252\n",
      "Epoch [31/200], Step [3/6], decoder loss: 0.0249, encoder1_activation l2 loss: 0.0000, total loss: 0.0249\n",
      "Epoch [31/200], Step [4/6], decoder loss: 0.0252, encoder1_activation l2 loss: 0.0000, total loss: 0.0252\n",
      "Epoch [31/200], Step [5/6], decoder loss: 0.0251, encoder1_activation l2 loss: 0.0000, total loss: 0.0251\n",
      "Epoch [31/200], Step [6/6], decoder loss: 0.0243, encoder1_activation l2 loss: 0.0000, total loss: 0.0243\n",
      "Epoch [31/200] (avg 6 steps), decoder loss: 0.0252, total loss: 0.0252\n",
      "Epoch [32/200], Step [1/6], decoder loss: 0.0248, encoder1_activation l2 loss: 0.0000, total loss: 0.0248\n",
      "Epoch [32/200], Step [2/6], decoder loss: 0.0247, encoder1_activation l2 loss: 0.0000, total loss: 0.0247\n",
      "Epoch [32/200], Step [3/6], decoder loss: 0.0231, encoder1_activation l2 loss: 0.0000, total loss: 0.0231\n",
      "Epoch [32/200], Step [4/6], decoder loss: 0.0240, encoder1_activation l2 loss: 0.0000, total loss: 0.0240\n",
      "Epoch [32/200], Step [5/6], decoder loss: 0.0241, encoder1_activation l2 loss: 0.0000, total loss: 0.0241\n",
      "Epoch [32/200], Step [6/6], decoder loss: 0.0254, encoder1_activation l2 loss: 0.0000, total loss: 0.0254\n",
      "Epoch [32/200] (avg 6 steps), decoder loss: 0.0243, total loss: 0.0243\n",
      "Epoch [33/200], Step [1/6], decoder loss: 0.0234, encoder1_activation l2 loss: 0.0000, total loss: 0.0234\n",
      "Epoch [33/200], Step [2/6], decoder loss: 0.0234, encoder1_activation l2 loss: 0.0000, total loss: 0.0234\n",
      "Epoch [33/200], Step [3/6], decoder loss: 0.0235, encoder1_activation l2 loss: 0.0000, total loss: 0.0235\n",
      "Epoch [33/200], Step [4/6], decoder loss: 0.0235, encoder1_activation l2 loss: 0.0000, total loss: 0.0235\n",
      "Epoch [33/200], Step [5/6], decoder loss: 0.0236, encoder1_activation l2 loss: 0.0000, total loss: 0.0236\n",
      "Epoch [33/200], Step [6/6], decoder loss: 0.0226, encoder1_activation l2 loss: 0.0000, total loss: 0.0226\n",
      "Epoch [33/200] (avg 6 steps), decoder loss: 0.0233, total loss: 0.0234\n",
      "Epoch [34/200], Step [1/6], decoder loss: 0.0221, encoder1_activation l2 loss: 0.0000, total loss: 0.0221\n",
      "Epoch [34/200], Step [2/6], decoder loss: 0.0217, encoder1_activation l2 loss: 0.0000, total loss: 0.0217\n",
      "Epoch [34/200], Step [3/6], decoder loss: 0.0232, encoder1_activation l2 loss: 0.0000, total loss: 0.0232\n",
      "Epoch [34/200], Step [4/6], decoder loss: 0.0228, encoder1_activation l2 loss: 0.0000, total loss: 0.0228\n",
      "Epoch [34/200], Step [5/6], decoder loss: 0.0221, encoder1_activation l2 loss: 0.0000, total loss: 0.0221\n",
      "Epoch [34/200], Step [6/6], decoder loss: 0.0249, encoder1_activation l2 loss: 0.0000, total loss: 0.0249\n",
      "Epoch [34/200] (avg 6 steps), decoder loss: 0.0228, total loss: 0.0228\n",
      "Epoch [35/200], Step [1/6], decoder loss: 0.0217, encoder1_activation l2 loss: 0.0000, total loss: 0.0217\n",
      "Epoch [35/200], Step [2/6], decoder loss: 0.0230, encoder1_activation l2 loss: 0.0000, total loss: 0.0230\n",
      "Epoch [35/200], Step [3/6], decoder loss: 0.0213, encoder1_activation l2 loss: 0.0000, total loss: 0.0213\n",
      "Epoch [35/200], Step [4/6], decoder loss: 0.0221, encoder1_activation l2 loss: 0.0000, total loss: 0.0221\n",
      "Epoch [35/200], Step [5/6], decoder loss: 0.0218, encoder1_activation l2 loss: 0.0000, total loss: 0.0218\n",
      "Epoch [35/200], Step [6/6], decoder loss: 0.0214, encoder1_activation l2 loss: 0.0000, total loss: 0.0214\n",
      "Epoch [35/200] (avg 6 steps), decoder loss: 0.0219, total loss: 0.0219\n",
      "Epoch [36/200], Step [1/6], decoder loss: 0.0219, encoder1_activation l2 loss: 0.0000, total loss: 0.0219\n",
      "Epoch [36/200], Step [2/6], decoder loss: 0.0209, encoder1_activation l2 loss: 0.0000, total loss: 0.0209\n",
      "Epoch [36/200], Step [3/6], decoder loss: 0.0217, encoder1_activation l2 loss: 0.0000, total loss: 0.0217\n",
      "Epoch [36/200], Step [4/6], decoder loss: 0.0215, encoder1_activation l2 loss: 0.0000, total loss: 0.0215\n",
      "Epoch [36/200], Step [5/6], decoder loss: 0.0214, encoder1_activation l2 loss: 0.0000, total loss: 0.0215\n",
      "Epoch [36/200], Step [6/6], decoder loss: 0.0196, encoder1_activation l2 loss: 0.0000, total loss: 0.0196\n",
      "Epoch [36/200] (avg 6 steps), decoder loss: 0.0212, total loss: 0.0212\n",
      "Epoch [37/200], Step [1/6], decoder loss: 0.0202, encoder1_activation l2 loss: 0.0000, total loss: 0.0202\n",
      "Epoch [37/200], Step [2/6], decoder loss: 0.0202, encoder1_activation l2 loss: 0.0000, total loss: 0.0202\n",
      "Epoch [37/200], Step [3/6], decoder loss: 0.0206, encoder1_activation l2 loss: 0.0000, total loss: 0.0206\n",
      "Epoch [37/200], Step [4/6], decoder loss: 0.0211, encoder1_activation l2 loss: 0.0000, total loss: 0.0211\n",
      "Epoch [37/200], Step [5/6], decoder loss: 0.0215, encoder1_activation l2 loss: 0.0000, total loss: 0.0215\n",
      "Epoch [37/200], Step [6/6], decoder loss: 0.0204, encoder1_activation l2 loss: 0.0000, total loss: 0.0204\n",
      "Epoch [37/200] (avg 6 steps), decoder loss: 0.0207, total loss: 0.0207\n",
      "Epoch [38/200], Step [1/6], decoder loss: 0.0200, encoder1_activation l2 loss: 0.0000, total loss: 0.0200\n",
      "Epoch [38/200], Step [2/6], decoder loss: 0.0198, encoder1_activation l2 loss: 0.0000, total loss: 0.0198\n",
      "Epoch [38/200], Step [3/6], decoder loss: 0.0212, encoder1_activation l2 loss: 0.0000, total loss: 0.0212\n",
      "Epoch [38/200], Step [4/6], decoder loss: 0.0206, encoder1_activation l2 loss: 0.0000, total loss: 0.0206\n",
      "Epoch [38/200], Step [5/6], decoder loss: 0.0198, encoder1_activation l2 loss: 0.0000, total loss: 0.0198\n",
      "Epoch [38/200], Step [6/6], decoder loss: 0.0190, encoder1_activation l2 loss: 0.0000, total loss: 0.0190\n",
      "Epoch [38/200] (avg 6 steps), decoder loss: 0.0201, total loss: 0.0201\n",
      "Epoch [39/200], Step [1/6], decoder loss: 0.0198, encoder1_activation l2 loss: 0.0000, total loss: 0.0198\n",
      "Epoch [39/200], Step [2/6], decoder loss: 0.0207, encoder1_activation l2 loss: 0.0000, total loss: 0.0207\n",
      "Epoch [39/200], Step [3/6], decoder loss: 0.0195, encoder1_activation l2 loss: 0.0000, total loss: 0.0195\n",
      "Epoch [39/200], Step [4/6], decoder loss: 0.0194, encoder1_activation l2 loss: 0.0000, total loss: 0.0194\n",
      "Epoch [39/200], Step [5/6], decoder loss: 0.0199, encoder1_activation l2 loss: 0.0000, total loss: 0.0199\n",
      "Epoch [39/200], Step [6/6], decoder loss: 0.0180, encoder1_activation l2 loss: 0.0000, total loss: 0.0180\n",
      "Epoch [39/200] (avg 6 steps), decoder loss: 0.0196, total loss: 0.0196\n",
      "Epoch [40/200], Step [1/6], decoder loss: 0.0193, encoder1_activation l2 loss: 0.0000, total loss: 0.0193\n",
      "Epoch [40/200], Step [2/6], decoder loss: 0.0187, encoder1_activation l2 loss: 0.0000, total loss: 0.0187\n",
      "Epoch [40/200], Step [3/6], decoder loss: 0.0181, encoder1_activation l2 loss: 0.0000, total loss: 0.0181\n",
      "Epoch [40/200], Step [4/6], decoder loss: 0.0193, encoder1_activation l2 loss: 0.0000, total loss: 0.0193\n",
      "Epoch [40/200], Step [5/6], decoder loss: 0.0199, encoder1_activation l2 loss: 0.0000, total loss: 0.0199\n",
      "Epoch [40/200], Step [6/6], decoder loss: 0.0206, encoder1_activation l2 loss: 0.0000, total loss: 0.0206\n",
      "Epoch [40/200] (avg 6 steps), decoder loss: 0.0193, total loss: 0.0193\n",
      "Epoch [41/200], Step [1/6], decoder loss: 0.0183, encoder1_activation l2 loss: 0.0000, total loss: 0.0183\n",
      "Epoch [41/200], Step [2/6], decoder loss: 0.0185, encoder1_activation l2 loss: 0.0000, total loss: 0.0186\n",
      "Epoch [41/200], Step [3/6], decoder loss: 0.0202, encoder1_activation l2 loss: 0.0000, total loss: 0.0202\n",
      "Epoch [41/200], Step [4/6], decoder loss: 0.0179, encoder1_activation l2 loss: 0.0000, total loss: 0.0179\n",
      "Epoch [41/200], Step [5/6], decoder loss: 0.0188, encoder1_activation l2 loss: 0.0000, total loss: 0.0188\n",
      "Epoch [41/200], Step [6/6], decoder loss: 0.0194, encoder1_activation l2 loss: 0.0000, total loss: 0.0194\n",
      "Epoch [41/200] (avg 6 steps), decoder loss: 0.0189, total loss: 0.0189\n",
      "Epoch [42/200], Step [1/6], decoder loss: 0.0181, encoder1_activation l2 loss: 0.0000, total loss: 0.0181\n",
      "Epoch [42/200], Step [2/6], decoder loss: 0.0182, encoder1_activation l2 loss: 0.0000, total loss: 0.0182\n",
      "Epoch [42/200], Step [3/6], decoder loss: 0.0200, encoder1_activation l2 loss: 0.0000, total loss: 0.0200\n",
      "Epoch [42/200], Step [4/6], decoder loss: 0.0183, encoder1_activation l2 loss: 0.0000, total loss: 0.0183\n",
      "Epoch [42/200], Step [5/6], decoder loss: 0.0181, encoder1_activation l2 loss: 0.0000, total loss: 0.0181\n",
      "Epoch [42/200], Step [6/6], decoder loss: 0.0175, encoder1_activation l2 loss: 0.0000, total loss: 0.0175\n",
      "Epoch [42/200] (avg 6 steps), decoder loss: 0.0184, total loss: 0.0184\n",
      "Epoch [43/200], Step [1/6], decoder loss: 0.0170, encoder1_activation l2 loss: 0.0000, total loss: 0.0170\n",
      "Epoch [43/200], Step [2/6], decoder loss: 0.0189, encoder1_activation l2 loss: 0.0000, total loss: 0.0189\n",
      "Epoch [43/200], Step [3/6], decoder loss: 0.0175, encoder1_activation l2 loss: 0.0000, total loss: 0.0175\n",
      "Epoch [43/200], Step [4/6], decoder loss: 0.0194, encoder1_activation l2 loss: 0.0000, total loss: 0.0194\n",
      "Epoch [43/200], Step [5/6], decoder loss: 0.0177, encoder1_activation l2 loss: 0.0000, total loss: 0.0177\n",
      "Epoch [43/200], Step [6/6], decoder loss: 0.0179, encoder1_activation l2 loss: 0.0000, total loss: 0.0179\n",
      "Epoch [43/200] (avg 6 steps), decoder loss: 0.0181, total loss: 0.0181\n",
      "Epoch [44/200], Step [1/6], decoder loss: 0.0167, encoder1_activation l2 loss: 0.0000, total loss: 0.0167\n",
      "Epoch [44/200], Step [2/6], decoder loss: 0.0207, encoder1_activation l2 loss: 0.0000, total loss: 0.0207\n",
      "Epoch [44/200], Step [3/6], decoder loss: 0.0165, encoder1_activation l2 loss: 0.0000, total loss: 0.0165\n",
      "Epoch [44/200], Step [4/6], decoder loss: 0.0173, encoder1_activation l2 loss: 0.0000, total loss: 0.0173\n",
      "Epoch [44/200], Step [5/6], decoder loss: 0.0173, encoder1_activation l2 loss: 0.0000, total loss: 0.0174\n",
      "Epoch [44/200], Step [6/6], decoder loss: 0.0180, encoder1_activation l2 loss: 0.0000, total loss: 0.0180\n",
      "Epoch [44/200] (avg 6 steps), decoder loss: 0.0178, total loss: 0.0178\n",
      "Epoch [45/200], Step [1/6], decoder loss: 0.0185, encoder1_activation l2 loss: 0.0000, total loss: 0.0185\n",
      "Epoch [45/200], Step [2/6], decoder loss: 0.0182, encoder1_activation l2 loss: 0.0000, total loss: 0.0182\n",
      "Epoch [45/200], Step [3/6], decoder loss: 0.0165, encoder1_activation l2 loss: 0.0000, total loss: 0.0165\n",
      "Epoch [45/200], Step [4/6], decoder loss: 0.0169, encoder1_activation l2 loss: 0.0000, total loss: 0.0169\n",
      "Epoch [45/200], Step [5/6], decoder loss: 0.0169, encoder1_activation l2 loss: 0.0000, total loss: 0.0169\n",
      "Epoch [45/200], Step [6/6], decoder loss: 0.0180, encoder1_activation l2 loss: 0.0000, total loss: 0.0180\n",
      "Epoch [45/200] (avg 6 steps), decoder loss: 0.0175, total loss: 0.0175\n",
      "Epoch [46/200], Step [1/6], decoder loss: 0.0189, encoder1_activation l2 loss: 0.0000, total loss: 0.0189\n",
      "Epoch [46/200], Step [2/6], decoder loss: 0.0175, encoder1_activation l2 loss: 0.0000, total loss: 0.0175\n",
      "Epoch [46/200], Step [3/6], decoder loss: 0.0153, encoder1_activation l2 loss: 0.0000, total loss: 0.0153\n",
      "Epoch [46/200], Step [4/6], decoder loss: 0.0191, encoder1_activation l2 loss: 0.0000, total loss: 0.0191\n",
      "Epoch [46/200], Step [5/6], decoder loss: 0.0159, encoder1_activation l2 loss: 0.0000, total loss: 0.0159\n",
      "Epoch [46/200], Step [6/6], decoder loss: 0.0156, encoder1_activation l2 loss: 0.0000, total loss: 0.0156\n",
      "Epoch [46/200] (avg 6 steps), decoder loss: 0.0171, total loss: 0.0171\n",
      "Epoch [47/200], Step [1/6], decoder loss: 0.0164, encoder1_activation l2 loss: 0.0000, total loss: 0.0164\n",
      "Epoch [47/200], Step [2/6], decoder loss: 0.0169, encoder1_activation l2 loss: 0.0000, total loss: 0.0169\n",
      "Epoch [47/200], Step [3/6], decoder loss: 0.0169, encoder1_activation l2 loss: 0.0000, total loss: 0.0169\n",
      "Epoch [47/200], Step [4/6], decoder loss: 0.0183, encoder1_activation l2 loss: 0.0000, total loss: 0.0183\n",
      "Epoch [47/200], Step [5/6], decoder loss: 0.0158, encoder1_activation l2 loss: 0.0000, total loss: 0.0158\n",
      "Epoch [47/200], Step [6/6], decoder loss: 0.0175, encoder1_activation l2 loss: 0.0000, total loss: 0.0175\n",
      "Epoch [47/200] (avg 6 steps), decoder loss: 0.0170, total loss: 0.0170\n",
      "Epoch [48/200], Step [1/6], decoder loss: 0.0161, encoder1_activation l2 loss: 0.0000, total loss: 0.0161\n",
      "Epoch [48/200], Step [2/6], decoder loss: 0.0172, encoder1_activation l2 loss: 0.0000, total loss: 0.0172\n",
      "Epoch [48/200], Step [3/6], decoder loss: 0.0158, encoder1_activation l2 loss: 0.0000, total loss: 0.0158\n",
      "Epoch [48/200], Step [4/6], decoder loss: 0.0164, encoder1_activation l2 loss: 0.0000, total loss: 0.0164\n",
      "Epoch [48/200], Step [5/6], decoder loss: 0.0169, encoder1_activation l2 loss: 0.0000, total loss: 0.0169\n",
      "Epoch [48/200], Step [6/6], decoder loss: 0.0181, encoder1_activation l2 loss: 0.0000, total loss: 0.0181\n",
      "Epoch [48/200] (avg 6 steps), decoder loss: 0.0168, total loss: 0.0168\n",
      "Epoch [49/200], Step [1/6], decoder loss: 0.0163, encoder1_activation l2 loss: 0.0000, total loss: 0.0164\n",
      "Epoch [49/200], Step [2/6], decoder loss: 0.0167, encoder1_activation l2 loss: 0.0000, total loss: 0.0167\n",
      "Epoch [49/200], Step [3/6], decoder loss: 0.0155, encoder1_activation l2 loss: 0.0000, total loss: 0.0155\n",
      "Epoch [49/200], Step [4/6], decoder loss: 0.0175, encoder1_activation l2 loss: 0.0000, total loss: 0.0175\n",
      "Epoch [49/200], Step [5/6], decoder loss: 0.0164, encoder1_activation l2 loss: 0.0000, total loss: 0.0164\n",
      "Epoch [49/200], Step [6/6], decoder loss: 0.0161, encoder1_activation l2 loss: 0.0000, total loss: 0.0161\n",
      "Epoch [49/200] (avg 6 steps), decoder loss: 0.0164, total loss: 0.0164\n",
      "Epoch [50/200], Step [1/6], decoder loss: 0.0160, encoder1_activation l2 loss: 0.0000, total loss: 0.0160\n",
      "Epoch [50/200], Step [2/6], decoder loss: 0.0170, encoder1_activation l2 loss: 0.0000, total loss: 0.0170\n",
      "Epoch [50/200], Step [3/6], decoder loss: 0.0150, encoder1_activation l2 loss: 0.0000, total loss: 0.0150\n",
      "Epoch [50/200], Step [4/6], decoder loss: 0.0167, encoder1_activation l2 loss: 0.0000, total loss: 0.0167\n",
      "Epoch [50/200], Step [5/6], decoder loss: 0.0168, encoder1_activation l2 loss: 0.0000, total loss: 0.0168\n",
      "Epoch [50/200], Step [6/6], decoder loss: 0.0159, encoder1_activation l2 loss: 0.0000, total loss: 0.0159\n",
      "Epoch [50/200] (avg 6 steps), decoder loss: 0.0162, total loss: 0.0162\n",
      "Epoch [51/200], Step [1/6], decoder loss: 0.0171, encoder1_activation l2 loss: 0.0000, total loss: 0.0171\n",
      "Epoch [51/200], Step [2/6], decoder loss: 0.0147, encoder1_activation l2 loss: 0.0000, total loss: 0.0147\n",
      "Epoch [51/200], Step [3/6], decoder loss: 0.0153, encoder1_activation l2 loss: 0.0000, total loss: 0.0153\n",
      "Epoch [51/200], Step [4/6], decoder loss: 0.0168, encoder1_activation l2 loss: 0.0000, total loss: 0.0168\n",
      "Epoch [51/200], Step [5/6], decoder loss: 0.0163, encoder1_activation l2 loss: 0.0000, total loss: 0.0163\n",
      "Epoch [51/200], Step [6/6], decoder loss: 0.0162, encoder1_activation l2 loss: 0.0000, total loss: 0.0162\n",
      "Epoch [51/200] (avg 6 steps), decoder loss: 0.0161, total loss: 0.0161\n",
      "Epoch [52/200], Step [1/6], decoder loss: 0.0155, encoder1_activation l2 loss: 0.0000, total loss: 0.0155\n",
      "Epoch [52/200], Step [2/6], decoder loss: 0.0164, encoder1_activation l2 loss: 0.0000, total loss: 0.0164\n",
      "Epoch [52/200], Step [3/6], decoder loss: 0.0156, encoder1_activation l2 loss: 0.0000, total loss: 0.0156\n",
      "Epoch [52/200], Step [4/6], decoder loss: 0.0155, encoder1_activation l2 loss: 0.0000, total loss: 0.0155\n",
      "Epoch [52/200], Step [5/6], decoder loss: 0.0165, encoder1_activation l2 loss: 0.0000, total loss: 0.0165\n",
      "Epoch [52/200], Step [6/6], decoder loss: 0.0155, encoder1_activation l2 loss: 0.0000, total loss: 0.0155\n",
      "Epoch [52/200] (avg 6 steps), decoder loss: 0.0158, total loss: 0.0158\n",
      "Epoch [53/200], Step [1/6], decoder loss: 0.0149, encoder1_activation l2 loss: 0.0000, total loss: 0.0149\n",
      "Epoch [53/200], Step [2/6], decoder loss: 0.0156, encoder1_activation l2 loss: 0.0000, total loss: 0.0156\n",
      "Epoch [53/200], Step [3/6], decoder loss: 0.0152, encoder1_activation l2 loss: 0.0000, total loss: 0.0152\n",
      "Epoch [53/200], Step [4/6], decoder loss: 0.0165, encoder1_activation l2 loss: 0.0000, total loss: 0.0165\n",
      "Epoch [53/200], Step [5/6], decoder loss: 0.0154, encoder1_activation l2 loss: 0.0000, total loss: 0.0154\n",
      "Epoch [53/200], Step [6/6], decoder loss: 0.0170, encoder1_activation l2 loss: 0.0000, total loss: 0.0170\n",
      "Epoch [53/200] (avg 6 steps), decoder loss: 0.0158, total loss: 0.0158\n",
      "Epoch [54/200], Step [1/6], decoder loss: 0.0130, encoder1_activation l2 loss: 0.0000, total loss: 0.0130\n",
      "Epoch [54/200], Step [2/6], decoder loss: 0.0163, encoder1_activation l2 loss: 0.0000, total loss: 0.0163\n",
      "Epoch [54/200], Step [3/6], decoder loss: 0.0157, encoder1_activation l2 loss: 0.0000, total loss: 0.0157\n",
      "Epoch [54/200], Step [4/6], decoder loss: 0.0163, encoder1_activation l2 loss: 0.0000, total loss: 0.0163\n",
      "Epoch [54/200], Step [5/6], decoder loss: 0.0163, encoder1_activation l2 loss: 0.0000, total loss: 0.0163\n",
      "Epoch [54/200], Step [6/6], decoder loss: 0.0156, encoder1_activation l2 loss: 0.0000, total loss: 0.0156\n",
      "Epoch [54/200] (avg 6 steps), decoder loss: 0.0155, total loss: 0.0155\n",
      "Epoch [55/200], Step [1/6], decoder loss: 0.0150, encoder1_activation l2 loss: 0.0000, total loss: 0.0150\n",
      "Epoch [55/200], Step [2/6], decoder loss: 0.0150, encoder1_activation l2 loss: 0.0000, total loss: 0.0150\n",
      "Epoch [55/200], Step [3/6], decoder loss: 0.0169, encoder1_activation l2 loss: 0.0000, total loss: 0.0169\n",
      "Epoch [55/200], Step [4/6], decoder loss: 0.0152, encoder1_activation l2 loss: 0.0000, total loss: 0.0152\n",
      "Epoch [55/200], Step [5/6], decoder loss: 0.0151, encoder1_activation l2 loss: 0.0000, total loss: 0.0151\n",
      "Epoch [55/200], Step [6/6], decoder loss: 0.0148, encoder1_activation l2 loss: 0.0000, total loss: 0.0148\n",
      "Epoch [55/200] (avg 6 steps), decoder loss: 0.0153, total loss: 0.0153\n",
      "Epoch [56/200], Step [1/6], decoder loss: 0.0148, encoder1_activation l2 loss: 0.0000, total loss: 0.0148\n",
      "Epoch [56/200], Step [2/6], decoder loss: 0.0144, encoder1_activation l2 loss: 0.0000, total loss: 0.0144\n",
      "Epoch [56/200], Step [3/6], decoder loss: 0.0147, encoder1_activation l2 loss: 0.0000, total loss: 0.0147\n",
      "Epoch [56/200], Step [4/6], decoder loss: 0.0157, encoder1_activation l2 loss: 0.0000, total loss: 0.0157\n",
      "Epoch [56/200], Step [5/6], decoder loss: 0.0158, encoder1_activation l2 loss: 0.0000, total loss: 0.0158\n",
      "Epoch [56/200], Step [6/6], decoder loss: 0.0165, encoder1_activation l2 loss: 0.0000, total loss: 0.0165\n",
      "Epoch [56/200] (avg 6 steps), decoder loss: 0.0153, total loss: 0.0153\n",
      "Epoch [57/200], Step [1/6], decoder loss: 0.0141, encoder1_activation l2 loss: 0.0000, total loss: 0.0141\n",
      "Epoch [57/200], Step [2/6], decoder loss: 0.0152, encoder1_activation l2 loss: 0.0000, total loss: 0.0152\n",
      "Epoch [57/200], Step [3/6], decoder loss: 0.0150, encoder1_activation l2 loss: 0.0000, total loss: 0.0150\n",
      "Epoch [57/200], Step [4/6], decoder loss: 0.0154, encoder1_activation l2 loss: 0.0000, total loss: 0.0154\n",
      "Epoch [57/200], Step [5/6], decoder loss: 0.0150, encoder1_activation l2 loss: 0.0000, total loss: 0.0150\n",
      "Epoch [57/200], Step [6/6], decoder loss: 0.0164, encoder1_activation l2 loss: 0.0000, total loss: 0.0164\n",
      "Epoch [57/200] (avg 6 steps), decoder loss: 0.0152, total loss: 0.0152\n",
      "Epoch [58/200], Step [1/6], decoder loss: 0.0156, encoder1_activation l2 loss: 0.0000, total loss: 0.0156\n",
      "Epoch [58/200], Step [2/6], decoder loss: 0.0138, encoder1_activation l2 loss: 0.0000, total loss: 0.0138\n",
      "Epoch [58/200], Step [3/6], decoder loss: 0.0146, encoder1_activation l2 loss: 0.0000, total loss: 0.0146\n",
      "Epoch [58/200], Step [4/6], decoder loss: 0.0153, encoder1_activation l2 loss: 0.0000, total loss: 0.0153\n",
      "Epoch [58/200], Step [5/6], decoder loss: 0.0154, encoder1_activation l2 loss: 0.0000, total loss: 0.0154\n",
      "Epoch [58/200], Step [6/6], decoder loss: 0.0151, encoder1_activation l2 loss: 0.0000, total loss: 0.0151\n",
      "Epoch [58/200] (avg 6 steps), decoder loss: 0.0150, total loss: 0.0150\n",
      "Epoch [59/200], Step [1/6], decoder loss: 0.0136, encoder1_activation l2 loss: 0.0000, total loss: 0.0136\n",
      "Epoch [59/200], Step [2/6], decoder loss: 0.0152, encoder1_activation l2 loss: 0.0000, total loss: 0.0152\n",
      "Epoch [59/200], Step [3/6], decoder loss: 0.0141, encoder1_activation l2 loss: 0.0000, total loss: 0.0141\n",
      "Epoch [59/200], Step [4/6], decoder loss: 0.0154, encoder1_activation l2 loss: 0.0000, total loss: 0.0154\n",
      "Epoch [59/200], Step [5/6], decoder loss: 0.0153, encoder1_activation l2 loss: 0.0000, total loss: 0.0153\n",
      "Epoch [59/200], Step [6/6], decoder loss: 0.0158, encoder1_activation l2 loss: 0.0000, total loss: 0.0158\n",
      "Epoch [59/200] (avg 6 steps), decoder loss: 0.0149, total loss: 0.0149\n",
      "Epoch [60/200], Step [1/6], decoder loss: 0.0148, encoder1_activation l2 loss: 0.0000, total loss: 0.0148\n",
      "Epoch [60/200], Step [2/6], decoder loss: 0.0160, encoder1_activation l2 loss: 0.0000, total loss: 0.0161\n",
      "Epoch [60/200], Step [3/6], decoder loss: 0.0138, encoder1_activation l2 loss: 0.0000, total loss: 0.0138\n",
      "Epoch [60/200], Step [4/6], decoder loss: 0.0144, encoder1_activation l2 loss: 0.0000, total loss: 0.0144\n",
      "Epoch [60/200], Step [5/6], decoder loss: 0.0149, encoder1_activation l2 loss: 0.0000, total loss: 0.0149\n",
      "Epoch [60/200], Step [6/6], decoder loss: 0.0143, encoder1_activation l2 loss: 0.0000, total loss: 0.0143\n",
      "Epoch [60/200] (avg 6 steps), decoder loss: 0.0147, total loss: 0.0147\n",
      "Epoch [61/200], Step [1/6], decoder loss: 0.0143, encoder1_activation l2 loss: 0.0000, total loss: 0.0143\n",
      "Epoch [61/200], Step [2/6], decoder loss: 0.0139, encoder1_activation l2 loss: 0.0000, total loss: 0.0139\n",
      "Epoch [61/200], Step [3/6], decoder loss: 0.0156, encoder1_activation l2 loss: 0.0000, total loss: 0.0156\n",
      "Epoch [61/200], Step [4/6], decoder loss: 0.0160, encoder1_activation l2 loss: 0.0000, total loss: 0.0160\n",
      "Epoch [61/200], Step [5/6], decoder loss: 0.0132, encoder1_activation l2 loss: 0.0000, total loss: 0.0132\n",
      "Epoch [61/200], Step [6/6], decoder loss: 0.0147, encoder1_activation l2 loss: 0.0000, total loss: 0.0147\n",
      "Epoch [61/200] (avg 6 steps), decoder loss: 0.0146, total loss: 0.0146\n",
      "Epoch [62/200], Step [1/6], decoder loss: 0.0143, encoder1_activation l2 loss: 0.0000, total loss: 0.0143\n",
      "Epoch [62/200], Step [2/6], decoder loss: 0.0159, encoder1_activation l2 loss: 0.0000, total loss: 0.0159\n",
      "Epoch [62/200], Step [3/6], decoder loss: 0.0136, encoder1_activation l2 loss: 0.0000, total loss: 0.0136\n",
      "Epoch [62/200], Step [4/6], decoder loss: 0.0139, encoder1_activation l2 loss: 0.0000, total loss: 0.0139\n",
      "Epoch [62/200], Step [5/6], decoder loss: 0.0149, encoder1_activation l2 loss: 0.0000, total loss: 0.0149\n",
      "Epoch [62/200], Step [6/6], decoder loss: 0.0145, encoder1_activation l2 loss: 0.0000, total loss: 0.0145\n",
      "Epoch [62/200] (avg 6 steps), decoder loss: 0.0145, total loss: 0.0145\n",
      "Epoch [63/200], Step [1/6], decoder loss: 0.0146, encoder1_activation l2 loss: 0.0000, total loss: 0.0146\n",
      "Epoch [63/200], Step [2/6], decoder loss: 0.0149, encoder1_activation l2 loss: 0.0000, total loss: 0.0149\n",
      "Epoch [63/200], Step [3/6], decoder loss: 0.0144, encoder1_activation l2 loss: 0.0000, total loss: 0.0144\n",
      "Epoch [63/200], Step [4/6], decoder loss: 0.0131, encoder1_activation l2 loss: 0.0000, total loss: 0.0131\n",
      "Epoch [63/200], Step [5/6], decoder loss: 0.0148, encoder1_activation l2 loss: 0.0000, total loss: 0.0148\n",
      "Epoch [63/200], Step [6/6], decoder loss: 0.0152, encoder1_activation l2 loss: 0.0000, total loss: 0.0152\n",
      "Epoch [63/200] (avg 6 steps), decoder loss: 0.0145, total loss: 0.0145\n",
      "Epoch [64/200], Step [1/6], decoder loss: 0.0137, encoder1_activation l2 loss: 0.0000, total loss: 0.0137\n",
      "Epoch [64/200], Step [2/6], decoder loss: 0.0158, encoder1_activation l2 loss: 0.0000, total loss: 0.0158\n",
      "Epoch [64/200], Step [3/6], decoder loss: 0.0149, encoder1_activation l2 loss: 0.0000, total loss: 0.0149\n",
      "Epoch [64/200], Step [4/6], decoder loss: 0.0128, encoder1_activation l2 loss: 0.0000, total loss: 0.0128\n",
      "Epoch [64/200], Step [5/6], decoder loss: 0.0154, encoder1_activation l2 loss: 0.0000, total loss: 0.0154\n",
      "Epoch [64/200], Step [6/6], decoder loss: 0.0128, encoder1_activation l2 loss: 0.0000, total loss: 0.0128\n",
      "Epoch [64/200] (avg 6 steps), decoder loss: 0.0142, total loss: 0.0142\n",
      "Epoch [65/200], Step [1/6], decoder loss: 0.0167, encoder1_activation l2 loss: 0.0000, total loss: 0.0167\n",
      "Epoch [65/200], Step [2/6], decoder loss: 0.0152, encoder1_activation l2 loss: 0.0000, total loss: 0.0152\n",
      "Epoch [65/200], Step [3/6], decoder loss: 0.0149, encoder1_activation l2 loss: 0.0000, total loss: 0.0149\n",
      "Epoch [65/200], Step [4/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [65/200], Step [5/6], decoder loss: 0.0129, encoder1_activation l2 loss: 0.0000, total loss: 0.0129\n",
      "Epoch [65/200], Step [6/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [65/200] (avg 6 steps), decoder loss: 0.0141, total loss: 0.0141\n",
      "Epoch [66/200], Step [1/6], decoder loss: 0.0144, encoder1_activation l2 loss: 0.0000, total loss: 0.0144\n",
      "Epoch [66/200], Step [2/6], decoder loss: 0.0147, encoder1_activation l2 loss: 0.0000, total loss: 0.0147\n",
      "Epoch [66/200], Step [3/6], decoder loss: 0.0133, encoder1_activation l2 loss: 0.0000, total loss: 0.0133\n",
      "Epoch [66/200], Step [4/6], decoder loss: 0.0146, encoder1_activation l2 loss: 0.0000, total loss: 0.0146\n",
      "Epoch [66/200], Step [5/6], decoder loss: 0.0126, encoder1_activation l2 loss: 0.0000, total loss: 0.0126\n",
      "Epoch [66/200], Step [6/6], decoder loss: 0.0163, encoder1_activation l2 loss: 0.0000, total loss: 0.0163\n",
      "Epoch [66/200] (avg 6 steps), decoder loss: 0.0143, total loss: 0.0143\n",
      "Epoch [67/200], Step [1/6], decoder loss: 0.0137, encoder1_activation l2 loss: 0.0000, total loss: 0.0137\n",
      "Epoch [67/200], Step [2/6], decoder loss: 0.0142, encoder1_activation l2 loss: 0.0000, total loss: 0.0142\n",
      "Epoch [67/200], Step [3/6], decoder loss: 0.0135, encoder1_activation l2 loss: 0.0000, total loss: 0.0135\n",
      "Epoch [67/200], Step [4/6], decoder loss: 0.0141, encoder1_activation l2 loss: 0.0000, total loss: 0.0141\n",
      "Epoch [67/200], Step [5/6], decoder loss: 0.0153, encoder1_activation l2 loss: 0.0000, total loss: 0.0153\n",
      "Epoch [67/200], Step [6/6], decoder loss: 0.0135, encoder1_activation l2 loss: 0.0000, total loss: 0.0135\n",
      "Epoch [67/200] (avg 6 steps), decoder loss: 0.0140, total loss: 0.0140\n",
      "Epoch [68/200], Step [1/6], decoder loss: 0.0138, encoder1_activation l2 loss: 0.0000, total loss: 0.0138\n",
      "Epoch [68/200], Step [2/6], decoder loss: 0.0146, encoder1_activation l2 loss: 0.0000, total loss: 0.0146\n",
      "Epoch [68/200], Step [3/6], decoder loss: 0.0136, encoder1_activation l2 loss: 0.0000, total loss: 0.0136\n",
      "Epoch [68/200], Step [4/6], decoder loss: 0.0148, encoder1_activation l2 loss: 0.0000, total loss: 0.0148\n",
      "Epoch [68/200], Step [5/6], decoder loss: 0.0133, encoder1_activation l2 loss: 0.0000, total loss: 0.0133\n",
      "Epoch [68/200], Step [6/6], decoder loss: 0.0140, encoder1_activation l2 loss: 0.0000, total loss: 0.0140\n",
      "Epoch [68/200] (avg 6 steps), decoder loss: 0.0140, total loss: 0.0140\n",
      "Epoch [69/200], Step [1/6], decoder loss: 0.0144, encoder1_activation l2 loss: 0.0000, total loss: 0.0144\n",
      "Epoch [69/200], Step [2/6], decoder loss: 0.0151, encoder1_activation l2 loss: 0.0000, total loss: 0.0151\n",
      "Epoch [69/200], Step [3/6], decoder loss: 0.0136, encoder1_activation l2 loss: 0.0000, total loss: 0.0136\n",
      "Epoch [69/200], Step [4/6], decoder loss: 0.0140, encoder1_activation l2 loss: 0.0000, total loss: 0.0140\n",
      "Epoch [69/200], Step [5/6], decoder loss: 0.0128, encoder1_activation l2 loss: 0.0000, total loss: 0.0128\n",
      "Epoch [69/200], Step [6/6], decoder loss: 0.0135, encoder1_activation l2 loss: 0.0000, total loss: 0.0135\n",
      "Epoch [69/200] (avg 6 steps), decoder loss: 0.0139, total loss: 0.0139\n",
      "Epoch [70/200], Step [1/6], decoder loss: 0.0132, encoder1_activation l2 loss: 0.0000, total loss: 0.0132\n",
      "Epoch [70/200], Step [2/6], decoder loss: 0.0141, encoder1_activation l2 loss: 0.0000, total loss: 0.0141\n",
      "Epoch [70/200], Step [3/6], decoder loss: 0.0141, encoder1_activation l2 loss: 0.0000, total loss: 0.0141\n",
      "Epoch [70/200], Step [4/6], decoder loss: 0.0140, encoder1_activation l2 loss: 0.0000, total loss: 0.0140\n",
      "Epoch [70/200], Step [5/6], decoder loss: 0.0150, encoder1_activation l2 loss: 0.0000, total loss: 0.0150\n",
      "Epoch [70/200], Step [6/6], decoder loss: 0.0119, encoder1_activation l2 loss: 0.0000, total loss: 0.0119\n",
      "Epoch [70/200] (avg 6 steps), decoder loss: 0.0137, total loss: 0.0137\n",
      "Epoch [71/200], Step [1/6], decoder loss: 0.0130, encoder1_activation l2 loss: 0.0000, total loss: 0.0130\n",
      "Epoch [71/200], Step [2/6], decoder loss: 0.0164, encoder1_activation l2 loss: 0.0000, total loss: 0.0164\n",
      "Epoch [71/200], Step [3/6], decoder loss: 0.0138, encoder1_activation l2 loss: 0.0000, total loss: 0.0138\n",
      "Epoch [71/200], Step [4/6], decoder loss: 0.0130, encoder1_activation l2 loss: 0.0000, total loss: 0.0130\n",
      "Epoch [71/200], Step [5/6], decoder loss: 0.0130, encoder1_activation l2 loss: 0.0000, total loss: 0.0130\n",
      "Epoch [71/200], Step [6/6], decoder loss: 0.0134, encoder1_activation l2 loss: 0.0000, total loss: 0.0134\n",
      "Epoch [71/200] (avg 6 steps), decoder loss: 0.0138, total loss: 0.0138\n",
      "Epoch [72/200], Step [1/6], decoder loss: 0.0154, encoder1_activation l2 loss: 0.0000, total loss: 0.0154\n",
      "Epoch [72/200], Step [2/6], decoder loss: 0.0142, encoder1_activation l2 loss: 0.0000, total loss: 0.0142\n",
      "Epoch [72/200], Step [3/6], decoder loss: 0.0137, encoder1_activation l2 loss: 0.0000, total loss: 0.0137\n",
      "Epoch [72/200], Step [4/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [72/200], Step [5/6], decoder loss: 0.0130, encoder1_activation l2 loss: 0.0000, total loss: 0.0130\n",
      "Epoch [72/200], Step [6/6], decoder loss: 0.0132, encoder1_activation l2 loss: 0.0000, total loss: 0.0132\n",
      "Epoch [72/200] (avg 6 steps), decoder loss: 0.0137, total loss: 0.0137\n",
      "Epoch [73/200], Step [1/6], decoder loss: 0.0140, encoder1_activation l2 loss: 0.0000, total loss: 0.0140\n",
      "Epoch [73/200], Step [2/6], decoder loss: 0.0135, encoder1_activation l2 loss: 0.0000, total loss: 0.0135\n",
      "Epoch [73/200], Step [3/6], decoder loss: 0.0136, encoder1_activation l2 loss: 0.0000, total loss: 0.0136\n",
      "Epoch [73/200], Step [4/6], decoder loss: 0.0156, encoder1_activation l2 loss: 0.0000, total loss: 0.0156\n",
      "Epoch [73/200], Step [5/6], decoder loss: 0.0128, encoder1_activation l2 loss: 0.0000, total loss: 0.0128\n",
      "Epoch [73/200], Step [6/6], decoder loss: 0.0117, encoder1_activation l2 loss: 0.0000, total loss: 0.0117\n",
      "Epoch [73/200] (avg 6 steps), decoder loss: 0.0135, total loss: 0.0136\n",
      "Epoch [74/200], Step [1/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [74/200], Step [2/6], decoder loss: 0.0143, encoder1_activation l2 loss: 0.0000, total loss: 0.0143\n",
      "Epoch [74/200], Step [3/6], decoder loss: 0.0130, encoder1_activation l2 loss: 0.0000, total loss: 0.0130\n",
      "Epoch [74/200], Step [4/6], decoder loss: 0.0129, encoder1_activation l2 loss: 0.0000, total loss: 0.0129\n",
      "Epoch [74/200], Step [5/6], decoder loss: 0.0152, encoder1_activation l2 loss: 0.0000, total loss: 0.0152\n",
      "Epoch [74/200], Step [6/6], decoder loss: 0.0141, encoder1_activation l2 loss: 0.0000, total loss: 0.0141\n",
      "Epoch [74/200] (avg 6 steps), decoder loss: 0.0137, total loss: 0.0137\n",
      "Epoch [75/200], Step [1/6], decoder loss: 0.0132, encoder1_activation l2 loss: 0.0000, total loss: 0.0132\n",
      "Epoch [75/200], Step [2/6], decoder loss: 0.0131, encoder1_activation l2 loss: 0.0000, total loss: 0.0131\n",
      "Epoch [75/200], Step [3/6], decoder loss: 0.0132, encoder1_activation l2 loss: 0.0000, total loss: 0.0132\n",
      "Epoch [75/200], Step [4/6], decoder loss: 0.0151, encoder1_activation l2 loss: 0.0000, total loss: 0.0151\n",
      "Epoch [75/200], Step [5/6], decoder loss: 0.0136, encoder1_activation l2 loss: 0.0000, total loss: 0.0136\n",
      "Epoch [75/200], Step [6/6], decoder loss: 0.0131, encoder1_activation l2 loss: 0.0000, total loss: 0.0131\n",
      "Epoch [75/200] (avg 6 steps), decoder loss: 0.0135, total loss: 0.0135\n",
      "Epoch [76/200], Step [1/6], decoder loss: 0.0130, encoder1_activation l2 loss: 0.0000, total loss: 0.0130\n",
      "Epoch [76/200], Step [2/6], decoder loss: 0.0133, encoder1_activation l2 loss: 0.0000, total loss: 0.0133\n",
      "Epoch [76/200], Step [3/6], decoder loss: 0.0145, encoder1_activation l2 loss: 0.0000, total loss: 0.0145\n",
      "Epoch [76/200], Step [4/6], decoder loss: 0.0132, encoder1_activation l2 loss: 0.0000, total loss: 0.0132\n",
      "Epoch [76/200], Step [5/6], decoder loss: 0.0139, encoder1_activation l2 loss: 0.0000, total loss: 0.0139\n",
      "Epoch [76/200], Step [6/6], decoder loss: 0.0130, encoder1_activation l2 loss: 0.0000, total loss: 0.0130\n",
      "Epoch [76/200] (avg 6 steps), decoder loss: 0.0135, total loss: 0.0135\n",
      "Epoch [77/200], Step [1/6], decoder loss: 0.0146, encoder1_activation l2 loss: 0.0000, total loss: 0.0146\n",
      "Epoch [77/200], Step [2/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [77/200], Step [3/6], decoder loss: 0.0148, encoder1_activation l2 loss: 0.0000, total loss: 0.0148\n",
      "Epoch [77/200], Step [4/6], decoder loss: 0.0134, encoder1_activation l2 loss: 0.0000, total loss: 0.0134\n",
      "Epoch [77/200], Step [5/6], decoder loss: 0.0134, encoder1_activation l2 loss: 0.0000, total loss: 0.0134\n",
      "Epoch [77/200], Step [6/6], decoder loss: 0.0111, encoder1_activation l2 loss: 0.0000, total loss: 0.0111\n",
      "Epoch [77/200] (avg 6 steps), decoder loss: 0.0133, total loss: 0.0133\n",
      "Epoch [78/200], Step [1/6], decoder loss: 0.0134, encoder1_activation l2 loss: 0.0000, total loss: 0.0134\n",
      "Epoch [78/200], Step [2/6], decoder loss: 0.0147, encoder1_activation l2 loss: 0.0000, total loss: 0.0147\n",
      "Epoch [78/200], Step [3/6], decoder loss: 0.0138, encoder1_activation l2 loss: 0.0000, total loss: 0.0138\n",
      "Epoch [78/200], Step [4/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [78/200], Step [5/6], decoder loss: 0.0130, encoder1_activation l2 loss: 0.0000, total loss: 0.0130\n",
      "Epoch [78/200], Step [6/6], decoder loss: 0.0133, encoder1_activation l2 loss: 0.0000, total loss: 0.0133\n",
      "Epoch [78/200] (avg 6 steps), decoder loss: 0.0134, total loss: 0.0134\n",
      "Epoch [79/200], Step [1/6], decoder loss: 0.0137, encoder1_activation l2 loss: 0.0000, total loss: 0.0137\n",
      "Epoch [79/200], Step [2/6], decoder loss: 0.0116, encoder1_activation l2 loss: 0.0000, total loss: 0.0116\n",
      "Epoch [79/200], Step [3/6], decoder loss: 0.0140, encoder1_activation l2 loss: 0.0000, total loss: 0.0140\n",
      "Epoch [79/200], Step [4/6], decoder loss: 0.0135, encoder1_activation l2 loss: 0.0000, total loss: 0.0135\n",
      "Epoch [79/200], Step [5/6], decoder loss: 0.0135, encoder1_activation l2 loss: 0.0000, total loss: 0.0135\n",
      "Epoch [79/200], Step [6/6], decoder loss: 0.0144, encoder1_activation l2 loss: 0.0000, total loss: 0.0144\n",
      "Epoch [79/200] (avg 6 steps), decoder loss: 0.0134, total loss: 0.0134\n",
      "Epoch [80/200], Step [1/6], decoder loss: 0.0139, encoder1_activation l2 loss: 0.0000, total loss: 0.0139\n",
      "Epoch [80/200], Step [2/6], decoder loss: 0.0128, encoder1_activation l2 loss: 0.0000, total loss: 0.0128\n",
      "Epoch [80/200], Step [3/6], decoder loss: 0.0135, encoder1_activation l2 loss: 0.0000, total loss: 0.0135\n",
      "Epoch [80/200], Step [4/6], decoder loss: 0.0118, encoder1_activation l2 loss: 0.0000, total loss: 0.0118\n",
      "Epoch [80/200], Step [5/6], decoder loss: 0.0147, encoder1_activation l2 loss: 0.0000, total loss: 0.0147\n",
      "Epoch [80/200], Step [6/6], decoder loss: 0.0134, encoder1_activation l2 loss: 0.0000, total loss: 0.0134\n",
      "Epoch [80/200] (avg 6 steps), decoder loss: 0.0133, total loss: 0.0133\n",
      "Epoch [81/200], Step [1/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [81/200], Step [2/6], decoder loss: 0.0136, encoder1_activation l2 loss: 0.0000, total loss: 0.0136\n",
      "Epoch [81/200], Step [3/6], decoder loss: 0.0133, encoder1_activation l2 loss: 0.0000, total loss: 0.0133\n",
      "Epoch [81/200], Step [4/6], decoder loss: 0.0138, encoder1_activation l2 loss: 0.0000, total loss: 0.0138\n",
      "Epoch [81/200], Step [5/6], decoder loss: 0.0136, encoder1_activation l2 loss: 0.0000, total loss: 0.0136\n",
      "Epoch [81/200], Step [6/6], decoder loss: 0.0129, encoder1_activation l2 loss: 0.0000, total loss: 0.0129\n",
      "Epoch [81/200] (avg 6 steps), decoder loss: 0.0133, total loss: 0.0133\n",
      "Epoch [82/200], Step [1/6], decoder loss: 0.0132, encoder1_activation l2 loss: 0.0000, total loss: 0.0132\n",
      "Epoch [82/200], Step [2/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [82/200], Step [3/6], decoder loss: 0.0131, encoder1_activation l2 loss: 0.0000, total loss: 0.0131\n",
      "Epoch [82/200], Step [4/6], decoder loss: 0.0136, encoder1_activation l2 loss: 0.0000, total loss: 0.0136\n",
      "Epoch [82/200], Step [5/6], decoder loss: 0.0137, encoder1_activation l2 loss: 0.0000, total loss: 0.0137\n",
      "Epoch [82/200], Step [6/6], decoder loss: 0.0136, encoder1_activation l2 loss: 0.0000, total loss: 0.0136\n",
      "Epoch [82/200] (avg 6 steps), decoder loss: 0.0133, total loss: 0.0133\n",
      "Epoch [83/200], Step [1/6], decoder loss: 0.0150, encoder1_activation l2 loss: 0.0000, total loss: 0.0150\n",
      "Epoch [83/200], Step [2/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [83/200], Step [3/6], decoder loss: 0.0131, encoder1_activation l2 loss: 0.0000, total loss: 0.0131\n",
      "Epoch [83/200], Step [4/6], decoder loss: 0.0136, encoder1_activation l2 loss: 0.0000, total loss: 0.0136\n",
      "Epoch [83/200], Step [5/6], decoder loss: 0.0135, encoder1_activation l2 loss: 0.0000, total loss: 0.0135\n",
      "Epoch [83/200], Step [6/6], decoder loss: 0.0106, encoder1_activation l2 loss: 0.0000, total loss: 0.0106\n",
      "Epoch [83/200] (avg 6 steps), decoder loss: 0.0130, total loss: 0.0130\n",
      "Epoch [84/200], Step [1/6], decoder loss: 0.0147, encoder1_activation l2 loss: 0.0000, total loss: 0.0147\n",
      "Epoch [84/200], Step [2/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [84/200], Step [3/6], decoder loss: 0.0132, encoder1_activation l2 loss: 0.0000, total loss: 0.0132\n",
      "Epoch [84/200], Step [4/6], decoder loss: 0.0129, encoder1_activation l2 loss: 0.0000, total loss: 0.0129\n",
      "Epoch [84/200], Step [5/6], decoder loss: 0.0133, encoder1_activation l2 loss: 0.0000, total loss: 0.0133\n",
      "Epoch [84/200], Step [6/6], decoder loss: 0.0120, encoder1_activation l2 loss: 0.0000, total loss: 0.0120\n",
      "Epoch [84/200] (avg 6 steps), decoder loss: 0.0131, total loss: 0.0131\n",
      "Epoch [85/200], Step [1/6], decoder loss: 0.0120, encoder1_activation l2 loss: 0.0000, total loss: 0.0120\n",
      "Epoch [85/200], Step [2/6], decoder loss: 0.0143, encoder1_activation l2 loss: 0.0000, total loss: 0.0143\n",
      "Epoch [85/200], Step [3/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [85/200], Step [4/6], decoder loss: 0.0129, encoder1_activation l2 loss: 0.0000, total loss: 0.0129\n",
      "Epoch [85/200], Step [5/6], decoder loss: 0.0140, encoder1_activation l2 loss: 0.0000, total loss: 0.0140\n",
      "Epoch [85/200], Step [6/6], decoder loss: 0.0133, encoder1_activation l2 loss: 0.0000, total loss: 0.0133\n",
      "Epoch [85/200] (avg 6 steps), decoder loss: 0.0131, total loss: 0.0131\n",
      "Epoch [86/200], Step [1/6], decoder loss: 0.0129, encoder1_activation l2 loss: 0.0000, total loss: 0.0129\n",
      "Epoch [86/200], Step [2/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [86/200], Step [3/6], decoder loss: 0.0134, encoder1_activation l2 loss: 0.0000, total loss: 0.0134\n",
      "Epoch [86/200], Step [4/6], decoder loss: 0.0156, encoder1_activation l2 loss: 0.0000, total loss: 0.0156\n",
      "Epoch [86/200], Step [5/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [86/200], Step [6/6], decoder loss: 0.0116, encoder1_activation l2 loss: 0.0000, total loss: 0.0116\n",
      "Epoch [86/200] (avg 6 steps), decoder loss: 0.0130, total loss: 0.0130\n",
      "Epoch [87/200], Step [1/6], decoder loss: 0.0119, encoder1_activation l2 loss: 0.0000, total loss: 0.0119\n",
      "Epoch [87/200], Step [2/6], decoder loss: 0.0151, encoder1_activation l2 loss: 0.0000, total loss: 0.0151\n",
      "Epoch [87/200], Step [3/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [87/200], Step [4/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [87/200], Step [5/6], decoder loss: 0.0144, encoder1_activation l2 loss: 0.0000, total loss: 0.0144\n",
      "Epoch [87/200], Step [6/6], decoder loss: 0.0117, encoder1_activation l2 loss: 0.0000, total loss: 0.0117\n",
      "Epoch [87/200] (avg 6 steps), decoder loss: 0.0130, total loss: 0.0130\n",
      "Epoch [88/200], Step [1/6], decoder loss: 0.0146, encoder1_activation l2 loss: 0.0000, total loss: 0.0146\n",
      "Epoch [88/200], Step [2/6], decoder loss: 0.0117, encoder1_activation l2 loss: 0.0000, total loss: 0.0117\n",
      "Epoch [88/200], Step [3/6], decoder loss: 0.0138, encoder1_activation l2 loss: 0.0000, total loss: 0.0138\n",
      "Epoch [88/200], Step [4/6], decoder loss: 0.0135, encoder1_activation l2 loss: 0.0000, total loss: 0.0135\n",
      "Epoch [88/200], Step [5/6], decoder loss: 0.0121, encoder1_activation l2 loss: 0.0000, total loss: 0.0121\n",
      "Epoch [88/200], Step [6/6], decoder loss: 0.0121, encoder1_activation l2 loss: 0.0000, total loss: 0.0121\n",
      "Epoch [88/200] (avg 6 steps), decoder loss: 0.0130, total loss: 0.0130\n",
      "Epoch [89/200], Step [1/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [89/200], Step [2/6], decoder loss: 0.0134, encoder1_activation l2 loss: 0.0000, total loss: 0.0134\n",
      "Epoch [89/200], Step [3/6], decoder loss: 0.0115, encoder1_activation l2 loss: 0.0000, total loss: 0.0115\n",
      "Epoch [89/200], Step [4/6], decoder loss: 0.0145, encoder1_activation l2 loss: 0.0000, total loss: 0.0145\n",
      "Epoch [89/200], Step [5/6], decoder loss: 0.0138, encoder1_activation l2 loss: 0.0000, total loss: 0.0138\n",
      "Epoch [89/200], Step [6/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [89/200] (avg 6 steps), decoder loss: 0.0130, total loss: 0.0130\n",
      "Epoch [90/200], Step [1/6], decoder loss: 0.0137, encoder1_activation l2 loss: 0.0000, total loss: 0.0137\n",
      "Epoch [90/200], Step [2/6], decoder loss: 0.0120, encoder1_activation l2 loss: 0.0000, total loss: 0.0120\n",
      "Epoch [90/200], Step [3/6], decoder loss: 0.0132, encoder1_activation l2 loss: 0.0000, total loss: 0.0132\n",
      "Epoch [90/200], Step [4/6], decoder loss: 0.0132, encoder1_activation l2 loss: 0.0000, total loss: 0.0132\n",
      "Epoch [90/200], Step [5/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [90/200], Step [6/6], decoder loss: 0.0133, encoder1_activation l2 loss: 0.0000, total loss: 0.0133\n",
      "Epoch [90/200] (avg 6 steps), decoder loss: 0.0130, total loss: 0.0130\n",
      "Epoch [91/200], Step [1/6], decoder loss: 0.0129, encoder1_activation l2 loss: 0.0000, total loss: 0.0129\n",
      "Epoch [91/200], Step [2/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [91/200], Step [3/6], decoder loss: 0.0133, encoder1_activation l2 loss: 0.0000, total loss: 0.0133\n",
      "Epoch [91/200], Step [4/6], decoder loss: 0.0142, encoder1_activation l2 loss: 0.0000, total loss: 0.0142\n",
      "Epoch [91/200], Step [5/6], decoder loss: 0.0117, encoder1_activation l2 loss: 0.0000, total loss: 0.0117\n",
      "Epoch [91/200], Step [6/6], decoder loss: 0.0135, encoder1_activation l2 loss: 0.0000, total loss: 0.0135\n",
      "Epoch [91/200] (avg 6 steps), decoder loss: 0.0130, total loss: 0.0130\n",
      "Epoch [92/200], Step [1/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [92/200], Step [2/6], decoder loss: 0.0130, encoder1_activation l2 loss: 0.0000, total loss: 0.0130\n",
      "Epoch [92/200], Step [3/6], decoder loss: 0.0130, encoder1_activation l2 loss: 0.0000, total loss: 0.0130\n",
      "Epoch [92/200], Step [4/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0126\n",
      "Epoch [92/200], Step [5/6], decoder loss: 0.0132, encoder1_activation l2 loss: 0.0000, total loss: 0.0132\n",
      "Epoch [92/200], Step [6/6], decoder loss: 0.0138, encoder1_activation l2 loss: 0.0000, total loss: 0.0138\n",
      "Epoch [92/200] (avg 6 steps), decoder loss: 0.0130, total loss: 0.0130\n",
      "Epoch [93/200], Step [1/6], decoder loss: 0.0136, encoder1_activation l2 loss: 0.0000, total loss: 0.0136\n",
      "Epoch [93/200], Step [2/6], decoder loss: 0.0132, encoder1_activation l2 loss: 0.0000, total loss: 0.0132\n",
      "Epoch [93/200], Step [3/6], decoder loss: 0.0119, encoder1_activation l2 loss: 0.0000, total loss: 0.0119\n",
      "Epoch [93/200], Step [4/6], decoder loss: 0.0141, encoder1_activation l2 loss: 0.0000, total loss: 0.0141\n",
      "Epoch [93/200], Step [5/6], decoder loss: 0.0129, encoder1_activation l2 loss: 0.0000, total loss: 0.0129\n",
      "Epoch [93/200], Step [6/6], decoder loss: 0.0106, encoder1_activation l2 loss: 0.0000, total loss: 0.0106\n",
      "Epoch [93/200] (avg 6 steps), decoder loss: 0.0127, total loss: 0.0127\n",
      "Epoch [94/200], Step [1/6], decoder loss: 0.0146, encoder1_activation l2 loss: 0.0000, total loss: 0.0146\n",
      "Epoch [94/200], Step [2/6], decoder loss: 0.0111, encoder1_activation l2 loss: 0.0000, total loss: 0.0111\n",
      "Epoch [94/200], Step [3/6], decoder loss: 0.0121, encoder1_activation l2 loss: 0.0000, total loss: 0.0121\n",
      "Epoch [94/200], Step [4/6], decoder loss: 0.0136, encoder1_activation l2 loss: 0.0000, total loss: 0.0136\n",
      "Epoch [94/200], Step [5/6], decoder loss: 0.0131, encoder1_activation l2 loss: 0.0000, total loss: 0.0131\n",
      "Epoch [94/200], Step [6/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [94/200] (avg 6 steps), decoder loss: 0.0128, total loss: 0.0128\n",
      "Epoch [95/200], Step [1/6], decoder loss: 0.0137, encoder1_activation l2 loss: 0.0000, total loss: 0.0137\n",
      "Epoch [95/200], Step [2/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [95/200], Step [3/6], decoder loss: 0.0134, encoder1_activation l2 loss: 0.0000, total loss: 0.0134\n",
      "Epoch [95/200], Step [4/6], decoder loss: 0.0138, encoder1_activation l2 loss: 0.0000, total loss: 0.0138\n",
      "Epoch [95/200], Step [5/6], decoder loss: 0.0114, encoder1_activation l2 loss: 0.0000, total loss: 0.0114\n",
      "Epoch [95/200], Step [6/6], decoder loss: 0.0121, encoder1_activation l2 loss: 0.0000, total loss: 0.0121\n",
      "Epoch [95/200] (avg 6 steps), decoder loss: 0.0128, total loss: 0.0128\n",
      "Epoch [96/200], Step [1/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [96/200], Step [2/6], decoder loss: 0.0133, encoder1_activation l2 loss: 0.0000, total loss: 0.0133\n",
      "Epoch [96/200], Step [3/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [96/200], Step [4/6], decoder loss: 0.0113, encoder1_activation l2 loss: 0.0000, total loss: 0.0113\n",
      "Epoch [96/200], Step [5/6], decoder loss: 0.0138, encoder1_activation l2 loss: 0.0000, total loss: 0.0138\n",
      "Epoch [96/200], Step [6/6], decoder loss: 0.0139, encoder1_activation l2 loss: 0.0000, total loss: 0.0139\n",
      "Epoch [96/200] (avg 6 steps), decoder loss: 0.0129, total loss: 0.0129\n",
      "Epoch [97/200], Step [1/6], decoder loss: 0.0108, encoder1_activation l2 loss: 0.0000, total loss: 0.0108\n",
      "Epoch [97/200], Step [2/6], decoder loss: 0.0133, encoder1_activation l2 loss: 0.0000, total loss: 0.0133\n",
      "Epoch [97/200], Step [3/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [97/200], Step [4/6], decoder loss: 0.0139, encoder1_activation l2 loss: 0.0000, total loss: 0.0139\n",
      "Epoch [97/200], Step [5/6], decoder loss: 0.0119, encoder1_activation l2 loss: 0.0000, total loss: 0.0119\n",
      "Epoch [97/200], Step [6/6], decoder loss: 0.0158, encoder1_activation l2 loss: 0.0000, total loss: 0.0158\n",
      "Epoch [97/200] (avg 6 steps), decoder loss: 0.0130, total loss: 0.0130\n",
      "Epoch [98/200], Step [1/6], decoder loss: 0.0153, encoder1_activation l2 loss: 0.0000, total loss: 0.0153\n",
      "Epoch [98/200], Step [2/6], decoder loss: 0.0126, encoder1_activation l2 loss: 0.0000, total loss: 0.0126\n",
      "Epoch [98/200], Step [3/6], decoder loss: 0.0120, encoder1_activation l2 loss: 0.0000, total loss: 0.0120\n",
      "Epoch [98/200], Step [4/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [98/200], Step [5/6], decoder loss: 0.0115, encoder1_activation l2 loss: 0.0000, total loss: 0.0115\n",
      "Epoch [98/200], Step [6/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [98/200] (avg 6 steps), decoder loss: 0.0127, total loss: 0.0127\n",
      "Epoch [99/200], Step [1/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [99/200], Step [2/6], decoder loss: 0.0116, encoder1_activation l2 loss: 0.0000, total loss: 0.0116\n",
      "Epoch [99/200], Step [3/6], decoder loss: 0.0134, encoder1_activation l2 loss: 0.0000, total loss: 0.0134\n",
      "Epoch [99/200], Step [4/6], decoder loss: 0.0131, encoder1_activation l2 loss: 0.0000, total loss: 0.0131\n",
      "Epoch [99/200], Step [5/6], decoder loss: 0.0131, encoder1_activation l2 loss: 0.0000, total loss: 0.0131\n",
      "Epoch [99/200], Step [6/6], decoder loss: 0.0129, encoder1_activation l2 loss: 0.0000, total loss: 0.0129\n",
      "Epoch [99/200] (avg 6 steps), decoder loss: 0.0127, total loss: 0.0127\n",
      "Epoch [100/200], Step [1/6], decoder loss: 0.0120, encoder1_activation l2 loss: 0.0000, total loss: 0.0120\n",
      "Epoch [100/200], Step [2/6], decoder loss: 0.0115, encoder1_activation l2 loss: 0.0000, total loss: 0.0115\n",
      "Epoch [100/200], Step [3/6], decoder loss: 0.0119, encoder1_activation l2 loss: 0.0000, total loss: 0.0119\n",
      "Epoch [100/200], Step [4/6], decoder loss: 0.0147, encoder1_activation l2 loss: 0.0000, total loss: 0.0147\n",
      "Epoch [100/200], Step [5/6], decoder loss: 0.0142, encoder1_activation l2 loss: 0.0000, total loss: 0.0142\n",
      "Epoch [100/200], Step [6/6], decoder loss: 0.0116, encoder1_activation l2 loss: 0.0000, total loss: 0.0116\n",
      "Epoch [100/200] (avg 6 steps), decoder loss: 0.0126, total loss: 0.0126\n",
      "Epoch [101/200], Step [1/6], decoder loss: 0.0128, encoder1_activation l2 loss: 0.0000, total loss: 0.0128\n",
      "Epoch [101/200], Step [2/6], decoder loss: 0.0121, encoder1_activation l2 loss: 0.0000, total loss: 0.0121\n",
      "Epoch [101/200], Step [3/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [101/200], Step [4/6], decoder loss: 0.0136, encoder1_activation l2 loss: 0.0000, total loss: 0.0136\n",
      "Epoch [101/200], Step [5/6], decoder loss: 0.0130, encoder1_activation l2 loss: 0.0000, total loss: 0.0130\n",
      "Epoch [101/200], Step [6/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [101/200] (avg 6 steps), decoder loss: 0.0127, total loss: 0.0127\n",
      "Epoch [102/200], Step [1/6], decoder loss: 0.0106, encoder1_activation l2 loss: 0.0000, total loss: 0.0106\n",
      "Epoch [102/200], Step [2/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [102/200], Step [3/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [102/200], Step [4/6], decoder loss: 0.0132, encoder1_activation l2 loss: 0.0000, total loss: 0.0132\n",
      "Epoch [102/200], Step [5/6], decoder loss: 0.0148, encoder1_activation l2 loss: 0.0000, total loss: 0.0148\n",
      "Epoch [102/200], Step [6/6], decoder loss: 0.0119, encoder1_activation l2 loss: 0.0000, total loss: 0.0119\n",
      "Epoch [102/200] (avg 6 steps), decoder loss: 0.0126, total loss: 0.0126\n",
      "Epoch [103/200], Step [1/6], decoder loss: 0.0119, encoder1_activation l2 loss: 0.0000, total loss: 0.0119\n",
      "Epoch [103/200], Step [2/6], decoder loss: 0.0133, encoder1_activation l2 loss: 0.0000, total loss: 0.0133\n",
      "Epoch [103/200], Step [3/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [103/200], Step [4/6], decoder loss: 0.0133, encoder1_activation l2 loss: 0.0000, total loss: 0.0133\n",
      "Epoch [103/200], Step [5/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [103/200], Step [6/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [103/200] (avg 6 steps), decoder loss: 0.0126, total loss: 0.0126\n",
      "Epoch [104/200], Step [1/6], decoder loss: 0.0131, encoder1_activation l2 loss: 0.0000, total loss: 0.0131\n",
      "Epoch [104/200], Step [2/6], decoder loss: 0.0118, encoder1_activation l2 loss: 0.0000, total loss: 0.0118\n",
      "Epoch [104/200], Step [3/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [104/200], Step [4/6], decoder loss: 0.0128, encoder1_activation l2 loss: 0.0000, total loss: 0.0128\n",
      "Epoch [104/200], Step [5/6], decoder loss: 0.0131, encoder1_activation l2 loss: 0.0000, total loss: 0.0131\n",
      "Epoch [104/200], Step [6/6], decoder loss: 0.0121, encoder1_activation l2 loss: 0.0000, total loss: 0.0121\n",
      "Epoch [104/200] (avg 6 steps), decoder loss: 0.0126, total loss: 0.0126\n",
      "Epoch [105/200], Step [1/6], decoder loss: 0.0117, encoder1_activation l2 loss: 0.0000, total loss: 0.0117\n",
      "Epoch [105/200], Step [2/6], decoder loss: 0.0132, encoder1_activation l2 loss: 0.0000, total loss: 0.0132\n",
      "Epoch [105/200], Step [3/6], decoder loss: 0.0112, encoder1_activation l2 loss: 0.0000, total loss: 0.0112\n",
      "Epoch [105/200], Step [4/6], decoder loss: 0.0138, encoder1_activation l2 loss: 0.0000, total loss: 0.0138\n",
      "Epoch [105/200], Step [5/6], decoder loss: 0.0126, encoder1_activation l2 loss: 0.0000, total loss: 0.0126\n",
      "Epoch [105/200], Step [6/6], decoder loss: 0.0136, encoder1_activation l2 loss: 0.0000, total loss: 0.0136\n",
      "Epoch [105/200] (avg 6 steps), decoder loss: 0.0127, total loss: 0.0127\n",
      "Epoch [106/200], Step [1/6], decoder loss: 0.0119, encoder1_activation l2 loss: 0.0000, total loss: 0.0119\n",
      "Epoch [106/200], Step [2/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [106/200], Step [3/6], decoder loss: 0.0133, encoder1_activation l2 loss: 0.0000, total loss: 0.0133\n",
      "Epoch [106/200], Step [4/6], decoder loss: 0.0138, encoder1_activation l2 loss: 0.0000, total loss: 0.0138\n",
      "Epoch [106/200], Step [5/6], decoder loss: 0.0112, encoder1_activation l2 loss: 0.0000, total loss: 0.0112\n",
      "Epoch [106/200], Step [6/6], decoder loss: 0.0131, encoder1_activation l2 loss: 0.0000, total loss: 0.0131\n",
      "Epoch [106/200] (avg 6 steps), decoder loss: 0.0126, total loss: 0.0126\n",
      "Epoch [107/200], Step [1/6], decoder loss: 0.0141, encoder1_activation l2 loss: 0.0000, total loss: 0.0141\n",
      "Epoch [107/200], Step [2/6], decoder loss: 0.0119, encoder1_activation l2 loss: 0.0000, total loss: 0.0119\n",
      "Epoch [107/200], Step [3/6], decoder loss: 0.0117, encoder1_activation l2 loss: 0.0000, total loss: 0.0117\n",
      "Epoch [107/200], Step [4/6], decoder loss: 0.0144, encoder1_activation l2 loss: 0.0000, total loss: 0.0144\n",
      "Epoch [107/200], Step [5/6], decoder loss: 0.0112, encoder1_activation l2 loss: 0.0000, total loss: 0.0112\n",
      "Epoch [107/200], Step [6/6], decoder loss: 0.0120, encoder1_activation l2 loss: 0.0000, total loss: 0.0120\n",
      "Epoch [107/200] (avg 6 steps), decoder loss: 0.0125, total loss: 0.0125\n",
      "Epoch [108/200], Step [1/6], decoder loss: 0.0121, encoder1_activation l2 loss: 0.0000, total loss: 0.0121\n",
      "Epoch [108/200], Step [2/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [108/200], Step [3/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [108/200], Step [4/6], decoder loss: 0.0128, encoder1_activation l2 loss: 0.0000, total loss: 0.0128\n",
      "Epoch [108/200], Step [5/6], decoder loss: 0.0131, encoder1_activation l2 loss: 0.0000, total loss: 0.0131\n",
      "Epoch [108/200], Step [6/6], decoder loss: 0.0129, encoder1_activation l2 loss: 0.0000, total loss: 0.0129\n",
      "Epoch [108/200] (avg 6 steps), decoder loss: 0.0126, total loss: 0.0126\n",
      "Epoch [109/200], Step [1/6], decoder loss: 0.0113, encoder1_activation l2 loss: 0.0000, total loss: 0.0113\n",
      "Epoch [109/200], Step [2/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [109/200], Step [3/6], decoder loss: 0.0139, encoder1_activation l2 loss: 0.0000, total loss: 0.0139\n",
      "Epoch [109/200], Step [4/6], decoder loss: 0.0121, encoder1_activation l2 loss: 0.0000, total loss: 0.0121\n",
      "Epoch [109/200], Step [5/6], decoder loss: 0.0118, encoder1_activation l2 loss: 0.0000, total loss: 0.0118\n",
      "Epoch [109/200], Step [6/6], decoder loss: 0.0141, encoder1_activation l2 loss: 0.0000, total loss: 0.0141\n",
      "Epoch [109/200] (avg 6 steps), decoder loss: 0.0127, total loss: 0.0127\n",
      "Epoch [110/200], Step [1/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [110/200], Step [2/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [110/200], Step [3/6], decoder loss: 0.0130, encoder1_activation l2 loss: 0.0000, total loss: 0.0130\n",
      "Epoch [110/200], Step [4/6], decoder loss: 0.0121, encoder1_activation l2 loss: 0.0000, total loss: 0.0121\n",
      "Epoch [110/200], Step [5/6], decoder loss: 0.0129, encoder1_activation l2 loss: 0.0000, total loss: 0.0129\n",
      "Epoch [110/200], Step [6/6], decoder loss: 0.0121, encoder1_activation l2 loss: 0.0000, total loss: 0.0121\n",
      "Epoch [110/200] (avg 6 steps), decoder loss: 0.0125, total loss: 0.0125\n",
      "Epoch [111/200], Step [1/6], decoder loss: 0.0118, encoder1_activation l2 loss: 0.0000, total loss: 0.0118\n",
      "Epoch [111/200], Step [2/6], decoder loss: 0.0148, encoder1_activation l2 loss: 0.0000, total loss: 0.0148\n",
      "Epoch [111/200], Step [3/6], decoder loss: 0.0140, encoder1_activation l2 loss: 0.0000, total loss: 0.0140\n",
      "Epoch [111/200], Step [4/6], decoder loss: 0.0113, encoder1_activation l2 loss: 0.0000, total loss: 0.0113\n",
      "Epoch [111/200], Step [5/6], decoder loss: 0.0118, encoder1_activation l2 loss: 0.0000, total loss: 0.0118\n",
      "Epoch [111/200], Step [6/6], decoder loss: 0.0106, encoder1_activation l2 loss: 0.0000, total loss: 0.0106\n",
      "Epoch [111/200] (avg 6 steps), decoder loss: 0.0124, total loss: 0.0124\n",
      "Epoch [112/200], Step [1/6], decoder loss: 0.0121, encoder1_activation l2 loss: 0.0000, total loss: 0.0121\n",
      "Epoch [112/200], Step [2/6], decoder loss: 0.0117, encoder1_activation l2 loss: 0.0000, total loss: 0.0117\n",
      "Epoch [112/200], Step [3/6], decoder loss: 0.0116, encoder1_activation l2 loss: 0.0000, total loss: 0.0116\n",
      "Epoch [112/200], Step [4/6], decoder loss: 0.0139, encoder1_activation l2 loss: 0.0000, total loss: 0.0139\n",
      "Epoch [112/200], Step [5/6], decoder loss: 0.0128, encoder1_activation l2 loss: 0.0000, total loss: 0.0128\n",
      "Epoch [112/200], Step [6/6], decoder loss: 0.0131, encoder1_activation l2 loss: 0.0000, total loss: 0.0131\n",
      "Epoch [112/200] (avg 6 steps), decoder loss: 0.0125, total loss: 0.0125\n",
      "Epoch [113/200], Step [1/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [113/200], Step [2/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [113/200], Step [3/6], decoder loss: 0.0133, encoder1_activation l2 loss: 0.0000, total loss: 0.0133\n",
      "Epoch [113/200], Step [4/6], decoder loss: 0.0131, encoder1_activation l2 loss: 0.0000, total loss: 0.0131\n",
      "Epoch [113/200], Step [5/6], decoder loss: 0.0111, encoder1_activation l2 loss: 0.0000, total loss: 0.0111\n",
      "Epoch [113/200], Step [6/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [113/200] (avg 6 steps), decoder loss: 0.0125, total loss: 0.0125\n",
      "Epoch [114/200], Step [1/6], decoder loss: 0.0128, encoder1_activation l2 loss: 0.0000, total loss: 0.0128\n",
      "Epoch [114/200], Step [2/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [114/200], Step [3/6], decoder loss: 0.0130, encoder1_activation l2 loss: 0.0000, total loss: 0.0130\n",
      "Epoch [114/200], Step [4/6], decoder loss: 0.0113, encoder1_activation l2 loss: 0.0000, total loss: 0.0113\n",
      "Epoch [114/200], Step [5/6], decoder loss: 0.0126, encoder1_activation l2 loss: 0.0000, total loss: 0.0126\n",
      "Epoch [114/200], Step [6/6], decoder loss: 0.0131, encoder1_activation l2 loss: 0.0000, total loss: 0.0131\n",
      "Epoch [114/200] (avg 6 steps), decoder loss: 0.0125, total loss: 0.0125\n",
      "Epoch [115/200], Step [1/6], decoder loss: 0.0113, encoder1_activation l2 loss: 0.0000, total loss: 0.0113\n",
      "Epoch [115/200], Step [2/6], decoder loss: 0.0114, encoder1_activation l2 loss: 0.0000, total loss: 0.0114\n",
      "Epoch [115/200], Step [3/6], decoder loss: 0.0138, encoder1_activation l2 loss: 0.0000, total loss: 0.0138\n",
      "Epoch [115/200], Step [4/6], decoder loss: 0.0128, encoder1_activation l2 loss: 0.0000, total loss: 0.0128\n",
      "Epoch [115/200], Step [5/6], decoder loss: 0.0130, encoder1_activation l2 loss: 0.0000, total loss: 0.0130\n",
      "Epoch [115/200], Step [6/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [115/200] (avg 6 steps), decoder loss: 0.0125, total loss: 0.0125\n",
      "Epoch [116/200], Step [1/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [116/200], Step [2/6], decoder loss: 0.0142, encoder1_activation l2 loss: 0.0000, total loss: 0.0142\n",
      "Epoch [116/200], Step [3/6], decoder loss: 0.0118, encoder1_activation l2 loss: 0.0000, total loss: 0.0118\n",
      "Epoch [116/200], Step [4/6], decoder loss: 0.0117, encoder1_activation l2 loss: 0.0000, total loss: 0.0117\n",
      "Epoch [116/200], Step [5/6], decoder loss: 0.0115, encoder1_activation l2 loss: 0.0000, total loss: 0.0115\n",
      "Epoch [116/200], Step [6/6], decoder loss: 0.0134, encoder1_activation l2 loss: 0.0000, total loss: 0.0134\n",
      "Epoch [116/200] (avg 6 steps), decoder loss: 0.0125, total loss: 0.0125\n",
      "Epoch [117/200], Step [1/6], decoder loss: 0.0138, encoder1_activation l2 loss: 0.0000, total loss: 0.0138\n",
      "Epoch [117/200], Step [2/6], decoder loss: 0.0132, encoder1_activation l2 loss: 0.0000, total loss: 0.0132\n",
      "Epoch [117/200], Step [3/6], decoder loss: 0.0115, encoder1_activation l2 loss: 0.0000, total loss: 0.0115\n",
      "Epoch [117/200], Step [4/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [117/200], Step [5/6], decoder loss: 0.0113, encoder1_activation l2 loss: 0.0000, total loss: 0.0113\n",
      "Epoch [117/200], Step [6/6], decoder loss: 0.0118, encoder1_activation l2 loss: 0.0000, total loss: 0.0118\n",
      "Epoch [117/200] (avg 6 steps), decoder loss: 0.0124, total loss: 0.0124\n",
      "Epoch [118/200], Step [1/6], decoder loss: 0.0119, encoder1_activation l2 loss: 0.0000, total loss: 0.0119\n",
      "Epoch [118/200], Step [2/6], decoder loss: 0.0117, encoder1_activation l2 loss: 0.0000, total loss: 0.0117\n",
      "Epoch [118/200], Step [3/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [118/200], Step [4/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [118/200], Step [5/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [118/200], Step [6/6], decoder loss: 0.0146, encoder1_activation l2 loss: 0.0000, total loss: 0.0146\n",
      "Epoch [118/200] (avg 6 steps), decoder loss: 0.0126, total loss: 0.0126\n",
      "Epoch [119/200], Step [1/6], decoder loss: 0.0138, encoder1_activation l2 loss: 0.0000, total loss: 0.0138\n",
      "Epoch [119/200], Step [2/6], decoder loss: 0.0117, encoder1_activation l2 loss: 0.0000, total loss: 0.0117\n",
      "Epoch [119/200], Step [3/6], decoder loss: 0.0118, encoder1_activation l2 loss: 0.0000, total loss: 0.0118\n",
      "Epoch [119/200], Step [4/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [119/200], Step [5/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [119/200], Step [6/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [119/200] (avg 6 steps), decoder loss: 0.0124, total loss: 0.0124\n",
      "Epoch [120/200], Step [1/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [120/200], Step [2/6], decoder loss: 0.0133, encoder1_activation l2 loss: 0.0000, total loss: 0.0133\n",
      "Epoch [120/200], Step [3/6], decoder loss: 0.0126, encoder1_activation l2 loss: 0.0000, total loss: 0.0126\n",
      "Epoch [120/200], Step [4/6], decoder loss: 0.0105, encoder1_activation l2 loss: 0.0000, total loss: 0.0105\n",
      "Epoch [120/200], Step [5/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [120/200], Step [6/6], decoder loss: 0.0130, encoder1_activation l2 loss: 0.0000, total loss: 0.0130\n",
      "Epoch [120/200] (avg 6 steps), decoder loss: 0.0124, total loss: 0.0124\n",
      "Epoch [121/200], Step [1/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [121/200], Step [2/6], decoder loss: 0.0119, encoder1_activation l2 loss: 0.0000, total loss: 0.0119\n",
      "Epoch [121/200], Step [3/6], decoder loss: 0.0129, encoder1_activation l2 loss: 0.0000, total loss: 0.0129\n",
      "Epoch [121/200], Step [4/6], decoder loss: 0.0120, encoder1_activation l2 loss: 0.0000, total loss: 0.0120\n",
      "Epoch [121/200], Step [5/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [121/200], Step [6/6], decoder loss: 0.0135, encoder1_activation l2 loss: 0.0000, total loss: 0.0135\n",
      "Epoch [121/200] (avg 6 steps), decoder loss: 0.0125, total loss: 0.0125\n",
      "Epoch [122/200], Step [1/6], decoder loss: 0.0134, encoder1_activation l2 loss: 0.0000, total loss: 0.0134\n",
      "Epoch [122/200], Step [2/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [122/200], Step [3/6], decoder loss: 0.0115, encoder1_activation l2 loss: 0.0000, total loss: 0.0115\n",
      "Epoch [122/200], Step [4/6], decoder loss: 0.0111, encoder1_activation l2 loss: 0.0000, total loss: 0.0111\n",
      "Epoch [122/200], Step [5/6], decoder loss: 0.0137, encoder1_activation l2 loss: 0.0000, total loss: 0.0137\n",
      "Epoch [122/200], Step [6/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [122/200] (avg 6 steps), decoder loss: 0.0124, total loss: 0.0124\n",
      "Epoch [123/200], Step [1/6], decoder loss: 0.0132, encoder1_activation l2 loss: 0.0000, total loss: 0.0132\n",
      "Epoch [123/200], Step [2/6], decoder loss: 0.0133, encoder1_activation l2 loss: 0.0000, total loss: 0.0133\n",
      "Epoch [123/200], Step [3/6], decoder loss: 0.0118, encoder1_activation l2 loss: 0.0000, total loss: 0.0118\n",
      "Epoch [123/200], Step [4/6], decoder loss: 0.0112, encoder1_activation l2 loss: 0.0000, total loss: 0.0112\n",
      "Epoch [123/200], Step [5/6], decoder loss: 0.0119, encoder1_activation l2 loss: 0.0000, total loss: 0.0119\n",
      "Epoch [123/200], Step [6/6], decoder loss: 0.0131, encoder1_activation l2 loss: 0.0000, total loss: 0.0131\n",
      "Epoch [123/200] (avg 6 steps), decoder loss: 0.0124, total loss: 0.0124\n",
      "Epoch [124/200], Step [1/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [124/200], Step [2/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [124/200], Step [3/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [124/200], Step [4/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [124/200], Step [5/6], decoder loss: 0.0128, encoder1_activation l2 loss: 0.0000, total loss: 0.0128\n",
      "Epoch [124/200], Step [6/6], decoder loss: 0.0103, encoder1_activation l2 loss: 0.0000, total loss: 0.0103\n",
      "Epoch [124/200] (avg 6 steps), decoder loss: 0.0122, total loss: 0.0122\n",
      "Epoch [125/200], Step [1/6], decoder loss: 0.0128, encoder1_activation l2 loss: 0.0000, total loss: 0.0128\n",
      "Epoch [125/200], Step [2/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [125/200], Step [3/6], decoder loss: 0.0138, encoder1_activation l2 loss: 0.0000, total loss: 0.0138\n",
      "Epoch [125/200], Step [4/6], decoder loss: 0.0119, encoder1_activation l2 loss: 0.0000, total loss: 0.0119\n",
      "Epoch [125/200], Step [5/6], decoder loss: 0.0114, encoder1_activation l2 loss: 0.0000, total loss: 0.0114\n",
      "Epoch [125/200], Step [6/6], decoder loss: 0.0116, encoder1_activation l2 loss: 0.0000, total loss: 0.0116\n",
      "Epoch [125/200] (avg 6 steps), decoder loss: 0.0123, total loss: 0.0123\n",
      "Epoch [126/200], Step [1/6], decoder loss: 0.0121, encoder1_activation l2 loss: 0.0000, total loss: 0.0121\n",
      "Epoch [126/200], Step [2/6], decoder loss: 0.0119, encoder1_activation l2 loss: 0.0000, total loss: 0.0119\n",
      "Epoch [126/200], Step [3/6], decoder loss: 0.0120, encoder1_activation l2 loss: 0.0000, total loss: 0.0120\n",
      "Epoch [126/200], Step [4/6], decoder loss: 0.0137, encoder1_activation l2 loss: 0.0000, total loss: 0.0138\n",
      "Epoch [126/200], Step [5/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [126/200], Step [6/6], decoder loss: 0.0116, encoder1_activation l2 loss: 0.0000, total loss: 0.0116\n",
      "Epoch [126/200] (avg 6 steps), decoder loss: 0.0123, total loss: 0.0123\n",
      "Epoch [127/200], Step [1/6], decoder loss: 0.0136, encoder1_activation l2 loss: 0.0000, total loss: 0.0136\n",
      "Epoch [127/200], Step [2/6], decoder loss: 0.0111, encoder1_activation l2 loss: 0.0000, total loss: 0.0111\n",
      "Epoch [127/200], Step [3/6], decoder loss: 0.0119, encoder1_activation l2 loss: 0.0000, total loss: 0.0119\n",
      "Epoch [127/200], Step [4/6], decoder loss: 0.0121, encoder1_activation l2 loss: 0.0000, total loss: 0.0121\n",
      "Epoch [127/200], Step [5/6], decoder loss: 0.0135, encoder1_activation l2 loss: 0.0000, total loss: 0.0135\n",
      "Epoch [127/200], Step [6/6], decoder loss: 0.0113, encoder1_activation l2 loss: 0.0000, total loss: 0.0113\n",
      "Epoch [127/200] (avg 6 steps), decoder loss: 0.0122, total loss: 0.0122\n",
      "Epoch [128/200], Step [1/6], decoder loss: 0.0102, encoder1_activation l2 loss: 0.0000, total loss: 0.0102\n",
      "Epoch [128/200], Step [2/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [128/200], Step [3/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [128/200], Step [4/6], decoder loss: 0.0133, encoder1_activation l2 loss: 0.0000, total loss: 0.0133\n",
      "Epoch [128/200], Step [5/6], decoder loss: 0.0133, encoder1_activation l2 loss: 0.0000, total loss: 0.0133\n",
      "Epoch [128/200], Step [6/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [128/200] (avg 6 steps), decoder loss: 0.0123, total loss: 0.0123\n",
      "Epoch [129/200], Step [1/6], decoder loss: 0.0110, encoder1_activation l2 loss: 0.0000, total loss: 0.0110\n",
      "Epoch [129/200], Step [2/6], decoder loss: 0.0160, encoder1_activation l2 loss: 0.0000, total loss: 0.0160\n",
      "Epoch [129/200], Step [3/6], decoder loss: 0.0117, encoder1_activation l2 loss: 0.0000, total loss: 0.0117\n",
      "Epoch [129/200], Step [4/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [129/200], Step [5/6], decoder loss: 0.0115, encoder1_activation l2 loss: 0.0000, total loss: 0.0115\n",
      "Epoch [129/200], Step [6/6], decoder loss: 0.0108, encoder1_activation l2 loss: 0.0000, total loss: 0.0108\n",
      "Epoch [129/200] (avg 6 steps), decoder loss: 0.0122, total loss: 0.0122\n",
      "Epoch [130/200], Step [1/6], decoder loss: 0.0116, encoder1_activation l2 loss: 0.0000, total loss: 0.0116\n",
      "Epoch [130/200], Step [2/6], decoder loss: 0.0118, encoder1_activation l2 loss: 0.0000, total loss: 0.0118\n",
      "Epoch [130/200], Step [3/6], decoder loss: 0.0117, encoder1_activation l2 loss: 0.0000, total loss: 0.0117\n",
      "Epoch [130/200], Step [4/6], decoder loss: 0.0137, encoder1_activation l2 loss: 0.0000, total loss: 0.0137\n",
      "Epoch [130/200], Step [5/6], decoder loss: 0.0133, encoder1_activation l2 loss: 0.0000, total loss: 0.0133\n",
      "Epoch [130/200], Step [6/6], decoder loss: 0.0111, encoder1_activation l2 loss: 0.0000, total loss: 0.0111\n",
      "Epoch [130/200] (avg 6 steps), decoder loss: 0.0122, total loss: 0.0122\n",
      "Epoch [131/200], Step [1/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [131/200], Step [2/6], decoder loss: 0.0130, encoder1_activation l2 loss: 0.0000, total loss: 0.0130\n",
      "Epoch [131/200], Step [3/6], decoder loss: 0.0135, encoder1_activation l2 loss: 0.0000, total loss: 0.0135\n",
      "Epoch [131/200], Step [4/6], decoder loss: 0.0109, encoder1_activation l2 loss: 0.0000, total loss: 0.0109\n",
      "Epoch [131/200], Step [5/6], decoder loss: 0.0118, encoder1_activation l2 loss: 0.0000, total loss: 0.0118\n",
      "Epoch [131/200], Step [6/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [131/200] (avg 6 steps), decoder loss: 0.0123, total loss: 0.0123\n",
      "Epoch [132/200], Step [1/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [132/200], Step [2/6], decoder loss: 0.0120, encoder1_activation l2 loss: 0.0000, total loss: 0.0120\n",
      "Epoch [132/200], Step [3/6], decoder loss: 0.0111, encoder1_activation l2 loss: 0.0000, total loss: 0.0111\n",
      "Epoch [132/200], Step [4/6], decoder loss: 0.0120, encoder1_activation l2 loss: 0.0000, total loss: 0.0120\n",
      "Epoch [132/200], Step [5/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [132/200], Step [6/6], decoder loss: 0.0142, encoder1_activation l2 loss: 0.0000, total loss: 0.0142\n",
      "Epoch [132/200] (avg 6 steps), decoder loss: 0.0124, total loss: 0.0124\n",
      "Epoch [133/200], Step [1/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [133/200], Step [2/6], decoder loss: 0.0135, encoder1_activation l2 loss: 0.0000, total loss: 0.0135\n",
      "Epoch [133/200], Step [3/6], decoder loss: 0.0121, encoder1_activation l2 loss: 0.0000, total loss: 0.0121\n",
      "Epoch [133/200], Step [4/6], decoder loss: 0.0121, encoder1_activation l2 loss: 0.0000, total loss: 0.0121\n",
      "Epoch [133/200], Step [5/6], decoder loss: 0.0115, encoder1_activation l2 loss: 0.0000, total loss: 0.0115\n",
      "Epoch [133/200], Step [6/6], decoder loss: 0.0120, encoder1_activation l2 loss: 0.0000, total loss: 0.0120\n",
      "Epoch [133/200] (avg 6 steps), decoder loss: 0.0122, total loss: 0.0122\n",
      "Epoch [134/200], Step [1/6], decoder loss: 0.0129, encoder1_activation l2 loss: 0.0000, total loss: 0.0129\n",
      "Epoch [134/200], Step [2/6], decoder loss: 0.0114, encoder1_activation l2 loss: 0.0000, total loss: 0.0114\n",
      "Epoch [134/200], Step [3/6], decoder loss: 0.0114, encoder1_activation l2 loss: 0.0000, total loss: 0.0114\n",
      "Epoch [134/200], Step [4/6], decoder loss: 0.0120, encoder1_activation l2 loss: 0.0000, total loss: 0.0120\n",
      "Epoch [134/200], Step [5/6], decoder loss: 0.0137, encoder1_activation l2 loss: 0.0000, total loss: 0.0137\n",
      "Epoch [134/200], Step [6/6], decoder loss: 0.0119, encoder1_activation l2 loss: 0.0000, total loss: 0.0119\n",
      "Epoch [134/200] (avg 6 steps), decoder loss: 0.0122, total loss: 0.0122\n",
      "Epoch [135/200], Step [1/6], decoder loss: 0.0131, encoder1_activation l2 loss: 0.0000, total loss: 0.0131\n",
      "Epoch [135/200], Step [2/6], decoder loss: 0.0116, encoder1_activation l2 loss: 0.0000, total loss: 0.0116\n",
      "Epoch [135/200], Step [3/6], decoder loss: 0.0126, encoder1_activation l2 loss: 0.0000, total loss: 0.0126\n",
      "Epoch [135/200], Step [4/6], decoder loss: 0.0118, encoder1_activation l2 loss: 0.0000, total loss: 0.0118\n",
      "Epoch [135/200], Step [5/6], decoder loss: 0.0114, encoder1_activation l2 loss: 0.0000, total loss: 0.0114\n",
      "Epoch [135/200], Step [6/6], decoder loss: 0.0134, encoder1_activation l2 loss: 0.0000, total loss: 0.0134\n",
      "Epoch [135/200] (avg 6 steps), decoder loss: 0.0123, total loss: 0.0123\n",
      "Epoch [136/200], Step [1/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [136/200], Step [2/6], decoder loss: 0.0114, encoder1_activation l2 loss: 0.0000, total loss: 0.0114\n",
      "Epoch [136/200], Step [3/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [136/200], Step [4/6], decoder loss: 0.0118, encoder1_activation l2 loss: 0.0000, total loss: 0.0118\n",
      "Epoch [136/200], Step [5/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [136/200], Step [6/6], decoder loss: 0.0130, encoder1_activation l2 loss: 0.0000, total loss: 0.0130\n",
      "Epoch [136/200] (avg 6 steps), decoder loss: 0.0123, total loss: 0.0123\n",
      "Epoch [137/200], Step [1/6], decoder loss: 0.0117, encoder1_activation l2 loss: 0.0000, total loss: 0.0117\n",
      "Epoch [137/200], Step [2/6], decoder loss: 0.0128, encoder1_activation l2 loss: 0.0000, total loss: 0.0128\n",
      "Epoch [137/200], Step [3/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [137/200], Step [4/6], decoder loss: 0.0116, encoder1_activation l2 loss: 0.0000, total loss: 0.0116\n",
      "Epoch [137/200], Step [5/6], decoder loss: 0.0126, encoder1_activation l2 loss: 0.0000, total loss: 0.0126\n",
      "Epoch [137/200], Step [6/6], decoder loss: 0.0119, encoder1_activation l2 loss: 0.0000, total loss: 0.0119\n",
      "Epoch [137/200] (avg 6 steps), decoder loss: 0.0122, total loss: 0.0122\n",
      "Epoch [138/200], Step [1/6], decoder loss: 0.0131, encoder1_activation l2 loss: 0.0000, total loss: 0.0131\n",
      "Epoch [138/200], Step [2/6], decoder loss: 0.0137, encoder1_activation l2 loss: 0.0000, total loss: 0.0137\n",
      "Epoch [138/200], Step [3/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [138/200], Step [4/6], decoder loss: 0.0116, encoder1_activation l2 loss: 0.0000, total loss: 0.0116\n",
      "Epoch [138/200], Step [5/6], decoder loss: 0.0119, encoder1_activation l2 loss: 0.0000, total loss: 0.0119\n",
      "Epoch [138/200], Step [6/6], decoder loss: 0.0091, encoder1_activation l2 loss: 0.0000, total loss: 0.0091\n",
      "Epoch [138/200] (avg 6 steps), decoder loss: 0.0120, total loss: 0.0120\n",
      "Epoch [139/200], Step [1/6], decoder loss: 0.0115, encoder1_activation l2 loss: 0.0000, total loss: 0.0115\n",
      "Epoch [139/200], Step [2/6], decoder loss: 0.0113, encoder1_activation l2 loss: 0.0000, total loss: 0.0113\n",
      "Epoch [139/200], Step [3/6], decoder loss: 0.0132, encoder1_activation l2 loss: 0.0000, total loss: 0.0132\n",
      "Epoch [139/200], Step [4/6], decoder loss: 0.0129, encoder1_activation l2 loss: 0.0000, total loss: 0.0129\n",
      "Epoch [139/200], Step [5/6], decoder loss: 0.0129, encoder1_activation l2 loss: 0.0000, total loss: 0.0129\n",
      "Epoch [139/200], Step [6/6], decoder loss: 0.0110, encoder1_activation l2 loss: 0.0000, total loss: 0.0110\n",
      "Epoch [139/200] (avg 6 steps), decoder loss: 0.0121, total loss: 0.0121\n",
      "Epoch [140/200], Step [1/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [140/200], Step [2/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [140/200], Step [3/6], decoder loss: 0.0121, encoder1_activation l2 loss: 0.0000, total loss: 0.0121\n",
      "Epoch [140/200], Step [4/6], decoder loss: 0.0138, encoder1_activation l2 loss: 0.0000, total loss: 0.0138\n",
      "Epoch [140/200], Step [5/6], decoder loss: 0.0105, encoder1_activation l2 loss: 0.0000, total loss: 0.0105\n",
      "Epoch [140/200], Step [6/6], decoder loss: 0.0119, encoder1_activation l2 loss: 0.0000, total loss: 0.0119\n",
      "Epoch [140/200] (avg 6 steps), decoder loss: 0.0122, total loss: 0.0122\n",
      "Epoch [141/200], Step [1/6], decoder loss: 0.0111, encoder1_activation l2 loss: 0.0000, total loss: 0.0111\n",
      "Epoch [141/200], Step [2/6], decoder loss: 0.0116, encoder1_activation l2 loss: 0.0000, total loss: 0.0116\n",
      "Epoch [141/200], Step [3/6], decoder loss: 0.0121, encoder1_activation l2 loss: 0.0000, total loss: 0.0121\n",
      "Epoch [141/200], Step [4/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [141/200], Step [5/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [141/200], Step [6/6], decoder loss: 0.0134, encoder1_activation l2 loss: 0.0000, total loss: 0.0134\n",
      "Epoch [141/200] (avg 6 steps), decoder loss: 0.0123, total loss: 0.0123\n",
      "Epoch [142/200], Step [1/6], decoder loss: 0.0139, encoder1_activation l2 loss: 0.0000, total loss: 0.0139\n",
      "Epoch [142/200], Step [2/6], decoder loss: 0.0100, encoder1_activation l2 loss: 0.0000, total loss: 0.0100\n",
      "Epoch [142/200], Step [3/6], decoder loss: 0.0115, encoder1_activation l2 loss: 0.0000, total loss: 0.0115\n",
      "Epoch [142/200], Step [4/6], decoder loss: 0.0128, encoder1_activation l2 loss: 0.0000, total loss: 0.0128\n",
      "Epoch [142/200], Step [5/6], decoder loss: 0.0138, encoder1_activation l2 loss: 0.0000, total loss: 0.0138\n",
      "Epoch [142/200], Step [6/6], decoder loss: 0.0103, encoder1_activation l2 loss: 0.0000, total loss: 0.0103\n",
      "Epoch [142/200] (avg 6 steps), decoder loss: 0.0121, total loss: 0.0121\n",
      "Epoch [143/200], Step [1/6], decoder loss: 0.0112, encoder1_activation l2 loss: 0.0000, total loss: 0.0112\n",
      "Epoch [143/200], Step [2/6], decoder loss: 0.0132, encoder1_activation l2 loss: 0.0000, total loss: 0.0132\n",
      "Epoch [143/200], Step [3/6], decoder loss: 0.0128, encoder1_activation l2 loss: 0.0000, total loss: 0.0128\n",
      "Epoch [143/200], Step [4/6], decoder loss: 0.0111, encoder1_activation l2 loss: 0.0000, total loss: 0.0111\n",
      "Epoch [143/200], Step [5/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [143/200], Step [6/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [143/200] (avg 6 steps), decoder loss: 0.0122, total loss: 0.0122\n",
      "Epoch [144/200], Step [1/6], decoder loss: 0.0114, encoder1_activation l2 loss: 0.0000, total loss: 0.0114\n",
      "Epoch [144/200], Step [2/6], decoder loss: 0.0130, encoder1_activation l2 loss: 0.0000, total loss: 0.0130\n",
      "Epoch [144/200], Step [3/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [144/200], Step [4/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [144/200], Step [5/6], decoder loss: 0.0118, encoder1_activation l2 loss: 0.0000, total loss: 0.0118\n",
      "Epoch [144/200], Step [6/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [144/200] (avg 6 steps), decoder loss: 0.0122, total loss: 0.0122\n",
      "Epoch [145/200], Step [1/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [145/200], Step [2/6], decoder loss: 0.0111, encoder1_activation l2 loss: 0.0000, total loss: 0.0111\n",
      "Epoch [145/200], Step [3/6], decoder loss: 0.0121, encoder1_activation l2 loss: 0.0000, total loss: 0.0121\n",
      "Epoch [145/200], Step [4/6], decoder loss: 0.0107, encoder1_activation l2 loss: 0.0000, total loss: 0.0107\n",
      "Epoch [145/200], Step [5/6], decoder loss: 0.0130, encoder1_activation l2 loss: 0.0000, total loss: 0.0130\n",
      "Epoch [145/200], Step [6/6], decoder loss: 0.0148, encoder1_activation l2 loss: 0.0000, total loss: 0.0148\n",
      "Epoch [145/200] (avg 6 steps), decoder loss: 0.0123, total loss: 0.0123\n",
      "Epoch [146/200], Step [1/6], decoder loss: 0.0120, encoder1_activation l2 loss: 0.0000, total loss: 0.0120\n",
      "Epoch [146/200], Step [2/6], decoder loss: 0.0116, encoder1_activation l2 loss: 0.0000, total loss: 0.0116\n",
      "Epoch [146/200], Step [3/6], decoder loss: 0.0130, encoder1_activation l2 loss: 0.0000, total loss: 0.0130\n",
      "Epoch [146/200], Step [4/6], decoder loss: 0.0119, encoder1_activation l2 loss: 0.0000, total loss: 0.0119\n",
      "Epoch [146/200], Step [5/6], decoder loss: 0.0119, encoder1_activation l2 loss: 0.0000, total loss: 0.0119\n",
      "Epoch [146/200], Step [6/6], decoder loss: 0.0131, encoder1_activation l2 loss: 0.0000, total loss: 0.0131\n",
      "Epoch [146/200] (avg 6 steps), decoder loss: 0.0122, total loss: 0.0122\n",
      "Epoch [147/200], Step [1/6], decoder loss: 0.0128, encoder1_activation l2 loss: 0.0000, total loss: 0.0128\n",
      "Epoch [147/200], Step [2/6], decoder loss: 0.0115, encoder1_activation l2 loss: 0.0000, total loss: 0.0115\n",
      "Epoch [147/200], Step [3/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [147/200], Step [4/6], decoder loss: 0.0118, encoder1_activation l2 loss: 0.0000, total loss: 0.0118\n",
      "Epoch [147/200], Step [5/6], decoder loss: 0.0128, encoder1_activation l2 loss: 0.0000, total loss: 0.0128\n",
      "Epoch [147/200], Step [6/6], decoder loss: 0.0111, encoder1_activation l2 loss: 0.0000, total loss: 0.0111\n",
      "Epoch [147/200] (avg 6 steps), decoder loss: 0.0121, total loss: 0.0121\n",
      "Epoch [148/200], Step [1/6], decoder loss: 0.0115, encoder1_activation l2 loss: 0.0000, total loss: 0.0115\n",
      "Epoch [148/200], Step [2/6], decoder loss: 0.0126, encoder1_activation l2 loss: 0.0000, total loss: 0.0126\n",
      "Epoch [148/200], Step [3/6], decoder loss: 0.0117, encoder1_activation l2 loss: 0.0000, total loss: 0.0117\n",
      "Epoch [148/200], Step [4/6], decoder loss: 0.0118, encoder1_activation l2 loss: 0.0000, total loss: 0.0118\n",
      "Epoch [148/200], Step [5/6], decoder loss: 0.0129, encoder1_activation l2 loss: 0.0000, total loss: 0.0129\n",
      "Epoch [148/200], Step [6/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [148/200] (avg 6 steps), decoder loss: 0.0122, total loss: 0.0122\n",
      "Epoch [149/200], Step [1/6], decoder loss: 0.0117, encoder1_activation l2 loss: 0.0000, total loss: 0.0117\n",
      "Epoch [149/200], Step [2/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [149/200], Step [3/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [149/200], Step [4/6], decoder loss: 0.0128, encoder1_activation l2 loss: 0.0000, total loss: 0.0128\n",
      "Epoch [149/200], Step [5/6], decoder loss: 0.0112, encoder1_activation l2 loss: 0.0000, total loss: 0.0112\n",
      "Epoch [149/200], Step [6/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [149/200] (avg 6 steps), decoder loss: 0.0121, total loss: 0.0121\n",
      "Epoch [150/200], Step [1/6], decoder loss: 0.0113, encoder1_activation l2 loss: 0.0000, total loss: 0.0113\n",
      "Epoch [150/200], Step [2/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [150/200], Step [3/6], decoder loss: 0.0100, encoder1_activation l2 loss: 0.0000, total loss: 0.0100\n",
      "Epoch [150/200], Step [4/6], decoder loss: 0.0128, encoder1_activation l2 loss: 0.0000, total loss: 0.0128\n",
      "Epoch [150/200], Step [5/6], decoder loss: 0.0129, encoder1_activation l2 loss: 0.0000, total loss: 0.0129\n",
      "Epoch [150/200], Step [6/6], decoder loss: 0.0147, encoder1_activation l2 loss: 0.0000, total loss: 0.0147\n",
      "Epoch [150/200] (avg 6 steps), decoder loss: 0.0123, total loss: 0.0123\n",
      "Epoch [151/200], Step [1/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [151/200], Step [2/6], decoder loss: 0.0112, encoder1_activation l2 loss: 0.0000, total loss: 0.0112\n",
      "Epoch [151/200], Step [3/6], decoder loss: 0.0119, encoder1_activation l2 loss: 0.0000, total loss: 0.0119\n",
      "Epoch [151/200], Step [4/6], decoder loss: 0.0138, encoder1_activation l2 loss: 0.0000, total loss: 0.0138\n",
      "Epoch [151/200], Step [5/6], decoder loss: 0.0120, encoder1_activation l2 loss: 0.0000, total loss: 0.0120\n",
      "Epoch [151/200], Step [6/6], decoder loss: 0.0112, encoder1_activation l2 loss: 0.0000, total loss: 0.0112\n",
      "Epoch [151/200] (avg 6 steps), decoder loss: 0.0121, total loss: 0.0121\n",
      "Epoch [152/200], Step [1/6], decoder loss: 0.0129, encoder1_activation l2 loss: 0.0000, total loss: 0.0129\n",
      "Epoch [152/200], Step [2/6], decoder loss: 0.0134, encoder1_activation l2 loss: 0.0000, total loss: 0.0134\n",
      "Epoch [152/200], Step [3/6], decoder loss: 0.0100, encoder1_activation l2 loss: 0.0000, total loss: 0.0100\n",
      "Epoch [152/200], Step [4/6], decoder loss: 0.0128, encoder1_activation l2 loss: 0.0000, total loss: 0.0128\n",
      "Epoch [152/200], Step [5/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [152/200], Step [6/6], decoder loss: 0.0101, encoder1_activation l2 loss: 0.0000, total loss: 0.0101\n",
      "Epoch [152/200] (avg 6 steps), decoder loss: 0.0120, total loss: 0.0120\n",
      "Epoch [153/200], Step [1/6], decoder loss: 0.0131, encoder1_activation l2 loss: 0.0000, total loss: 0.0131\n",
      "Epoch [153/200], Step [2/6], decoder loss: 0.0108, encoder1_activation l2 loss: 0.0000, total loss: 0.0108\n",
      "Epoch [153/200], Step [3/6], decoder loss: 0.0150, encoder1_activation l2 loss: 0.0000, total loss: 0.0150\n",
      "Epoch [153/200], Step [4/6], decoder loss: 0.0103, encoder1_activation l2 loss: 0.0000, total loss: 0.0103\n",
      "Epoch [153/200], Step [5/6], decoder loss: 0.0111, encoder1_activation l2 loss: 0.0000, total loss: 0.0111\n",
      "Epoch [153/200], Step [6/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [153/200] (avg 6 steps), decoder loss: 0.0121, total loss: 0.0121\n",
      "Epoch [154/200], Step [1/6], decoder loss: 0.0128, encoder1_activation l2 loss: 0.0000, total loss: 0.0128\n",
      "Epoch [154/200], Step [2/6], decoder loss: 0.0112, encoder1_activation l2 loss: 0.0000, total loss: 0.0112\n",
      "Epoch [154/200], Step [3/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [154/200], Step [4/6], decoder loss: 0.0126, encoder1_activation l2 loss: 0.0000, total loss: 0.0126\n",
      "Epoch [154/200], Step [5/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [154/200], Step [6/6], decoder loss: 0.0103, encoder1_activation l2 loss: 0.0000, total loss: 0.0103\n",
      "Epoch [154/200] (avg 6 steps), decoder loss: 0.0120, total loss: 0.0120\n",
      "Epoch [155/200], Step [1/6], decoder loss: 0.0114, encoder1_activation l2 loss: 0.0000, total loss: 0.0114\n",
      "Epoch [155/200], Step [2/6], decoder loss: 0.0120, encoder1_activation l2 loss: 0.0000, total loss: 0.0120\n",
      "Epoch [155/200], Step [3/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [155/200], Step [4/6], decoder loss: 0.0120, encoder1_activation l2 loss: 0.0000, total loss: 0.0120\n",
      "Epoch [155/200], Step [5/6], decoder loss: 0.0135, encoder1_activation l2 loss: 0.0000, total loss: 0.0135\n",
      "Epoch [155/200], Step [6/6], decoder loss: 0.0106, encoder1_activation l2 loss: 0.0000, total loss: 0.0106\n",
      "Epoch [155/200] (avg 6 steps), decoder loss: 0.0120, total loss: 0.0120\n",
      "Epoch [156/200], Step [1/6], decoder loss: 0.0119, encoder1_activation l2 loss: 0.0000, total loss: 0.0119\n",
      "Epoch [156/200], Step [2/6], decoder loss: 0.0117, encoder1_activation l2 loss: 0.0000, total loss: 0.0117\n",
      "Epoch [156/200], Step [3/6], decoder loss: 0.0134, encoder1_activation l2 loss: 0.0000, total loss: 0.0134\n",
      "Epoch [156/200], Step [4/6], decoder loss: 0.0119, encoder1_activation l2 loss: 0.0000, total loss: 0.0119\n",
      "Epoch [156/200], Step [5/6], decoder loss: 0.0115, encoder1_activation l2 loss: 0.0000, total loss: 0.0115\n",
      "Epoch [156/200], Step [6/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [156/200] (avg 6 steps), decoder loss: 0.0121, total loss: 0.0121\n",
      "Epoch [157/200], Step [1/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [157/200], Step [2/6], decoder loss: 0.0128, encoder1_activation l2 loss: 0.0000, total loss: 0.0128\n",
      "Epoch [157/200], Step [3/6], decoder loss: 0.0126, encoder1_activation l2 loss: 0.0000, total loss: 0.0126\n",
      "Epoch [157/200], Step [4/6], decoder loss: 0.0118, encoder1_activation l2 loss: 0.0000, total loss: 0.0118\n",
      "Epoch [157/200], Step [5/6], decoder loss: 0.0104, encoder1_activation l2 loss: 0.0000, total loss: 0.0104\n",
      "Epoch [157/200], Step [6/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [157/200] (avg 6 steps), decoder loss: 0.0121, total loss: 0.0121\n",
      "Epoch [158/200], Step [1/6], decoder loss: 0.0121, encoder1_activation l2 loss: 0.0000, total loss: 0.0121\n",
      "Epoch [158/200], Step [2/6], decoder loss: 0.0121, encoder1_activation l2 loss: 0.0000, total loss: 0.0121\n",
      "Epoch [158/200], Step [3/6], decoder loss: 0.0117, encoder1_activation l2 loss: 0.0000, total loss: 0.0117\n",
      "Epoch [158/200], Step [4/6], decoder loss: 0.0128, encoder1_activation l2 loss: 0.0000, total loss: 0.0128\n",
      "Epoch [158/200], Step [5/6], decoder loss: 0.0120, encoder1_activation l2 loss: 0.0000, total loss: 0.0120\n",
      "Epoch [158/200], Step [6/6], decoder loss: 0.0115, encoder1_activation l2 loss: 0.0000, total loss: 0.0115\n",
      "Epoch [158/200] (avg 6 steps), decoder loss: 0.0120, total loss: 0.0120\n",
      "Epoch [159/200], Step [1/6], decoder loss: 0.0116, encoder1_activation l2 loss: 0.0000, total loss: 0.0116\n",
      "Epoch [159/200], Step [2/6], decoder loss: 0.0115, encoder1_activation l2 loss: 0.0000, total loss: 0.0115\n",
      "Epoch [159/200], Step [3/6], decoder loss: 0.0126, encoder1_activation l2 loss: 0.0000, total loss: 0.0126\n",
      "Epoch [159/200], Step [4/6], decoder loss: 0.0128, encoder1_activation l2 loss: 0.0000, total loss: 0.0128\n",
      "Epoch [159/200], Step [5/6], decoder loss: 0.0111, encoder1_activation l2 loss: 0.0000, total loss: 0.0111\n",
      "Epoch [159/200], Step [6/6], decoder loss: 0.0136, encoder1_activation l2 loss: 0.0000, total loss: 0.0136\n",
      "Epoch [159/200] (avg 6 steps), decoder loss: 0.0122, total loss: 0.0122\n",
      "Epoch [160/200], Step [1/6], decoder loss: 0.0111, encoder1_activation l2 loss: 0.0000, total loss: 0.0111\n",
      "Epoch [160/200], Step [2/6], decoder loss: 0.0121, encoder1_activation l2 loss: 0.0000, total loss: 0.0121\n",
      "Epoch [160/200], Step [3/6], decoder loss: 0.0135, encoder1_activation l2 loss: 0.0000, total loss: 0.0135\n",
      "Epoch [160/200], Step [4/6], decoder loss: 0.0118, encoder1_activation l2 loss: 0.0000, total loss: 0.0118\n",
      "Epoch [160/200], Step [5/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [160/200], Step [6/6], decoder loss: 0.0108, encoder1_activation l2 loss: 0.0000, total loss: 0.0108\n",
      "Epoch [160/200] (avg 6 steps), decoder loss: 0.0120, total loss: 0.0120\n",
      "Epoch [161/200], Step [1/6], decoder loss: 0.0137, encoder1_activation l2 loss: 0.0000, total loss: 0.0137\n",
      "Epoch [161/200], Step [2/6], decoder loss: 0.0120, encoder1_activation l2 loss: 0.0000, total loss: 0.0120\n",
      "Epoch [161/200], Step [3/6], decoder loss: 0.0100, encoder1_activation l2 loss: 0.0000, total loss: 0.0100\n",
      "Epoch [161/200], Step [4/6], decoder loss: 0.0112, encoder1_activation l2 loss: 0.0000, total loss: 0.0112\n",
      "Epoch [161/200], Step [5/6], decoder loss: 0.0129, encoder1_activation l2 loss: 0.0000, total loss: 0.0129\n",
      "Epoch [161/200], Step [6/6], decoder loss: 0.0129, encoder1_activation l2 loss: 0.0000, total loss: 0.0129\n",
      "Epoch [161/200] (avg 6 steps), decoder loss: 0.0121, total loss: 0.0121\n",
      "Epoch [162/200], Step [1/6], decoder loss: 0.0109, encoder1_activation l2 loss: 0.0000, total loss: 0.0109\n",
      "Epoch [162/200], Step [2/6], decoder loss: 0.0140, encoder1_activation l2 loss: 0.0000, total loss: 0.0140\n",
      "Epoch [162/200], Step [3/6], decoder loss: 0.0128, encoder1_activation l2 loss: 0.0000, total loss: 0.0128\n",
      "Epoch [162/200], Step [4/6], decoder loss: 0.0111, encoder1_activation l2 loss: 0.0000, total loss: 0.0111\n",
      "Epoch [162/200], Step [5/6], decoder loss: 0.0121, encoder1_activation l2 loss: 0.0000, total loss: 0.0121\n",
      "Epoch [162/200], Step [6/6], decoder loss: 0.0111, encoder1_activation l2 loss: 0.0000, total loss: 0.0111\n",
      "Epoch [162/200] (avg 6 steps), decoder loss: 0.0120, total loss: 0.0120\n",
      "Epoch [163/200], Step [1/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [163/200], Step [2/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [163/200], Step [3/6], decoder loss: 0.0118, encoder1_activation l2 loss: 0.0000, total loss: 0.0118\n",
      "Epoch [163/200], Step [4/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [163/200], Step [5/6], decoder loss: 0.0115, encoder1_activation l2 loss: 0.0000, total loss: 0.0115\n",
      "Epoch [163/200], Step [6/6], decoder loss: 0.0111, encoder1_activation l2 loss: 0.0000, total loss: 0.0111\n",
      "Epoch [163/200] (avg 6 steps), decoder loss: 0.0120, total loss: 0.0120\n",
      "Epoch [164/200], Step [1/6], decoder loss: 0.0120, encoder1_activation l2 loss: 0.0000, total loss: 0.0120\n",
      "Epoch [164/200], Step [2/6], decoder loss: 0.0121, encoder1_activation l2 loss: 0.0000, total loss: 0.0121\n",
      "Epoch [164/200], Step [3/6], decoder loss: 0.0107, encoder1_activation l2 loss: 0.0000, total loss: 0.0107\n",
      "Epoch [164/200], Step [4/6], decoder loss: 0.0126, encoder1_activation l2 loss: 0.0000, total loss: 0.0126\n",
      "Epoch [164/200], Step [5/6], decoder loss: 0.0133, encoder1_activation l2 loss: 0.0000, total loss: 0.0133\n",
      "Epoch [164/200], Step [6/6], decoder loss: 0.0115, encoder1_activation l2 loss: 0.0000, total loss: 0.0115\n",
      "Epoch [164/200] (avg 6 steps), decoder loss: 0.0120, total loss: 0.0120\n",
      "Epoch [165/200], Step [1/6], decoder loss: 0.0136, encoder1_activation l2 loss: 0.0000, total loss: 0.0136\n",
      "Epoch [165/200], Step [2/6], decoder loss: 0.0117, encoder1_activation l2 loss: 0.0000, total loss: 0.0117\n",
      "Epoch [165/200], Step [3/6], decoder loss: 0.0128, encoder1_activation l2 loss: 0.0000, total loss: 0.0128\n",
      "Epoch [165/200], Step [4/6], decoder loss: 0.0113, encoder1_activation l2 loss: 0.0000, total loss: 0.0113\n",
      "Epoch [165/200], Step [5/6], decoder loss: 0.0121, encoder1_activation l2 loss: 0.0000, total loss: 0.0121\n",
      "Epoch [165/200], Step [6/6], decoder loss: 0.0101, encoder1_activation l2 loss: 0.0000, total loss: 0.0101\n",
      "Epoch [165/200] (avg 6 steps), decoder loss: 0.0119, total loss: 0.0119\n",
      "Epoch [166/200], Step [1/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [166/200], Step [2/6], decoder loss: 0.0120, encoder1_activation l2 loss: 0.0000, total loss: 0.0120\n",
      "Epoch [166/200], Step [3/6], decoder loss: 0.0113, encoder1_activation l2 loss: 0.0000, total loss: 0.0113\n",
      "Epoch [166/200], Step [4/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [166/200], Step [5/6], decoder loss: 0.0111, encoder1_activation l2 loss: 0.0000, total loss: 0.0111\n",
      "Epoch [166/200], Step [6/6], decoder loss: 0.0131, encoder1_activation l2 loss: 0.0000, total loss: 0.0131\n",
      "Epoch [166/200] (avg 6 steps), decoder loss: 0.0121, total loss: 0.0121\n",
      "Epoch [167/200], Step [1/6], decoder loss: 0.0120, encoder1_activation l2 loss: 0.0000, total loss: 0.0120\n",
      "Epoch [167/200], Step [2/6], decoder loss: 0.0106, encoder1_activation l2 loss: 0.0000, total loss: 0.0106\n",
      "Epoch [167/200], Step [3/6], decoder loss: 0.0121, encoder1_activation l2 loss: 0.0000, total loss: 0.0121\n",
      "Epoch [167/200], Step [4/6], decoder loss: 0.0135, encoder1_activation l2 loss: 0.0000, total loss: 0.0135\n",
      "Epoch [167/200], Step [5/6], decoder loss: 0.0128, encoder1_activation l2 loss: 0.0000, total loss: 0.0128\n",
      "Epoch [167/200], Step [6/6], decoder loss: 0.0105, encoder1_activation l2 loss: 0.0000, total loss: 0.0105\n",
      "Epoch [167/200] (avg 6 steps), decoder loss: 0.0119, total loss: 0.0119\n",
      "Epoch [168/200], Step [1/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [168/200], Step [2/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [168/200], Step [3/6], decoder loss: 0.0110, encoder1_activation l2 loss: 0.0000, total loss: 0.0110\n",
      "Epoch [168/200], Step [4/6], decoder loss: 0.0113, encoder1_activation l2 loss: 0.0000, total loss: 0.0113\n",
      "Epoch [168/200], Step [5/6], decoder loss: 0.0120, encoder1_activation l2 loss: 0.0000, total loss: 0.0120\n",
      "Epoch [168/200], Step [6/6], decoder loss: 0.0132, encoder1_activation l2 loss: 0.0000, total loss: 0.0132\n",
      "Epoch [168/200] (avg 6 steps), decoder loss: 0.0121, total loss: 0.0121\n",
      "Epoch [169/200], Step [1/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [169/200], Step [2/6], decoder loss: 0.0116, encoder1_activation l2 loss: 0.0000, total loss: 0.0116\n",
      "Epoch [169/200], Step [3/6], decoder loss: 0.0118, encoder1_activation l2 loss: 0.0000, total loss: 0.0118\n",
      "Epoch [169/200], Step [4/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [169/200], Step [5/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [169/200], Step [6/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [169/200] (avg 6 steps), decoder loss: 0.0120, total loss: 0.0120\n",
      "Epoch [170/200], Step [1/6], decoder loss: 0.0113, encoder1_activation l2 loss: 0.0000, total loss: 0.0113\n",
      "Epoch [170/200], Step [2/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [170/200], Step [3/6], decoder loss: 0.0112, encoder1_activation l2 loss: 0.0000, total loss: 0.0112\n",
      "Epoch [170/200], Step [4/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [170/200], Step [5/6], decoder loss: 0.0129, encoder1_activation l2 loss: 0.0000, total loss: 0.0129\n",
      "Epoch [170/200], Step [6/6], decoder loss: 0.0120, encoder1_activation l2 loss: 0.0000, total loss: 0.0120\n",
      "Epoch [170/200] (avg 6 steps), decoder loss: 0.0120, total loss: 0.0120\n",
      "Epoch [171/200], Step [1/6], decoder loss: 0.0102, encoder1_activation l2 loss: 0.0000, total loss: 0.0102\n",
      "Epoch [171/200], Step [2/6], decoder loss: 0.0148, encoder1_activation l2 loss: 0.0000, total loss: 0.0148\n",
      "Epoch [171/200], Step [3/6], decoder loss: 0.0132, encoder1_activation l2 loss: 0.0000, total loss: 0.0132\n",
      "Epoch [171/200], Step [4/6], decoder loss: 0.0103, encoder1_activation l2 loss: 0.0000, total loss: 0.0103\n",
      "Epoch [171/200], Step [5/6], decoder loss: 0.0132, encoder1_activation l2 loss: 0.0000, total loss: 0.0132\n",
      "Epoch [171/200], Step [6/6], decoder loss: 0.0094, encoder1_activation l2 loss: 0.0000, total loss: 0.0094\n",
      "Epoch [171/200] (avg 6 steps), decoder loss: 0.0118, total loss: 0.0118\n",
      "Epoch [172/200], Step [1/6], decoder loss: 0.0120, encoder1_activation l2 loss: 0.0000, total loss: 0.0120\n",
      "Epoch [172/200], Step [2/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [172/200], Step [3/6], decoder loss: 0.0116, encoder1_activation l2 loss: 0.0000, total loss: 0.0116\n",
      "Epoch [172/200], Step [4/6], decoder loss: 0.0115, encoder1_activation l2 loss: 0.0000, total loss: 0.0115\n",
      "Epoch [172/200], Step [5/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [172/200], Step [6/6], decoder loss: 0.0120, encoder1_activation l2 loss: 0.0000, total loss: 0.0120\n",
      "Epoch [172/200] (avg 6 steps), decoder loss: 0.0120, total loss: 0.0120\n",
      "Epoch [173/200], Step [1/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [173/200], Step [2/6], decoder loss: 0.0115, encoder1_activation l2 loss: 0.0000, total loss: 0.0115\n",
      "Epoch [173/200], Step [3/6], decoder loss: 0.0121, encoder1_activation l2 loss: 0.0000, total loss: 0.0121\n",
      "Epoch [173/200], Step [4/6], decoder loss: 0.0128, encoder1_activation l2 loss: 0.0000, total loss: 0.0128\n",
      "Epoch [173/200], Step [5/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [173/200], Step [6/6], decoder loss: 0.0106, encoder1_activation l2 loss: 0.0000, total loss: 0.0106\n",
      "Epoch [173/200] (avg 6 steps), decoder loss: 0.0119, total loss: 0.0119\n",
      "Epoch [174/200], Step [1/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [174/200], Step [2/6], decoder loss: 0.0115, encoder1_activation l2 loss: 0.0000, total loss: 0.0115\n",
      "Epoch [174/200], Step [3/6], decoder loss: 0.0119, encoder1_activation l2 loss: 0.0000, total loss: 0.0119\n",
      "Epoch [174/200], Step [4/6], decoder loss: 0.0129, encoder1_activation l2 loss: 0.0000, total loss: 0.0129\n",
      "Epoch [174/200], Step [5/6], decoder loss: 0.0111, encoder1_activation l2 loss: 0.0000, total loss: 0.0111\n",
      "Epoch [174/200], Step [6/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [174/200] (avg 6 steps), decoder loss: 0.0120, total loss: 0.0120\n",
      "Epoch [175/200], Step [1/6], decoder loss: 0.0102, encoder1_activation l2 loss: 0.0000, total loss: 0.0102\n",
      "Epoch [175/200], Step [2/6], decoder loss: 0.0129, encoder1_activation l2 loss: 0.0000, total loss: 0.0129\n",
      "Epoch [175/200], Step [3/6], decoder loss: 0.0116, encoder1_activation l2 loss: 0.0000, total loss: 0.0116\n",
      "Epoch [175/200], Step [4/6], decoder loss: 0.0130, encoder1_activation l2 loss: 0.0000, total loss: 0.0130\n",
      "Epoch [175/200], Step [5/6], decoder loss: 0.0119, encoder1_activation l2 loss: 0.0000, total loss: 0.0119\n",
      "Epoch [175/200], Step [6/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [175/200] (avg 6 steps), decoder loss: 0.0121, total loss: 0.0121\n",
      "Epoch [176/200], Step [1/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [176/200], Step [2/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [176/200], Step [3/6], decoder loss: 0.0117, encoder1_activation l2 loss: 0.0000, total loss: 0.0117\n",
      "Epoch [176/200], Step [4/6], decoder loss: 0.0114, encoder1_activation l2 loss: 0.0000, total loss: 0.0114\n",
      "Epoch [176/200], Step [5/6], decoder loss: 0.0128, encoder1_activation l2 loss: 0.0000, total loss: 0.0128\n",
      "Epoch [176/200], Step [6/6], decoder loss: 0.0110, encoder1_activation l2 loss: 0.0000, total loss: 0.0110\n",
      "Epoch [176/200] (avg 6 steps), decoder loss: 0.0119, total loss: 0.0119\n",
      "Epoch [177/200], Step [1/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [177/200], Step [2/6], decoder loss: 0.0119, encoder1_activation l2 loss: 0.0000, total loss: 0.0119\n",
      "Epoch [177/200], Step [3/6], decoder loss: 0.0128, encoder1_activation l2 loss: 0.0000, total loss: 0.0128\n",
      "Epoch [177/200], Step [4/6], decoder loss: 0.0121, encoder1_activation l2 loss: 0.0000, total loss: 0.0121\n",
      "Epoch [177/200], Step [5/6], decoder loss: 0.0115, encoder1_activation l2 loss: 0.0000, total loss: 0.0115\n",
      "Epoch [177/200], Step [6/6], decoder loss: 0.0111, encoder1_activation l2 loss: 0.0000, total loss: 0.0111\n",
      "Epoch [177/200] (avg 6 steps), decoder loss: 0.0119, total loss: 0.0119\n",
      "Epoch [178/200], Step [1/6], decoder loss: 0.0129, encoder1_activation l2 loss: 0.0000, total loss: 0.0129\n",
      "Epoch [178/200], Step [2/6], decoder loss: 0.0113, encoder1_activation l2 loss: 0.0000, total loss: 0.0113\n",
      "Epoch [178/200], Step [3/6], decoder loss: 0.0115, encoder1_activation l2 loss: 0.0000, total loss: 0.0115\n",
      "Epoch [178/200], Step [4/6], decoder loss: 0.0116, encoder1_activation l2 loss: 0.0000, total loss: 0.0116\n",
      "Epoch [178/200], Step [5/6], decoder loss: 0.0118, encoder1_activation l2 loss: 0.0000, total loss: 0.0118\n",
      "Epoch [178/200], Step [6/6], decoder loss: 0.0135, encoder1_activation l2 loss: 0.0000, total loss: 0.0135\n",
      "Epoch [178/200] (avg 6 steps), decoder loss: 0.0121, total loss: 0.0121\n",
      "Epoch [179/200], Step [1/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [179/200], Step [2/6], decoder loss: 0.0110, encoder1_activation l2 loss: 0.0000, total loss: 0.0110\n",
      "Epoch [179/200], Step [3/6], decoder loss: 0.0126, encoder1_activation l2 loss: 0.0000, total loss: 0.0126\n",
      "Epoch [179/200], Step [4/6], decoder loss: 0.0116, encoder1_activation l2 loss: 0.0000, total loss: 0.0116\n",
      "Epoch [179/200], Step [5/6], decoder loss: 0.0130, encoder1_activation l2 loss: 0.0000, total loss: 0.0130\n",
      "Epoch [179/200], Step [6/6], decoder loss: 0.0108, encoder1_activation l2 loss: 0.0000, total loss: 0.0108\n",
      "Epoch [179/200] (avg 6 steps), decoder loss: 0.0119, total loss: 0.0119\n",
      "Epoch [180/200], Step [1/6], decoder loss: 0.0115, encoder1_activation l2 loss: 0.0000, total loss: 0.0115\n",
      "Epoch [180/200], Step [2/6], decoder loss: 0.0110, encoder1_activation l2 loss: 0.0000, total loss: 0.0110\n",
      "Epoch [180/200], Step [3/6], decoder loss: 0.0129, encoder1_activation l2 loss: 0.0000, total loss: 0.0129\n",
      "Epoch [180/200], Step [4/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [180/200], Step [5/6], decoder loss: 0.0134, encoder1_activation l2 loss: 0.0000, total loss: 0.0134\n",
      "Epoch [180/200], Step [6/6], decoder loss: 0.0101, encoder1_activation l2 loss: 0.0000, total loss: 0.0101\n",
      "Epoch [180/200] (avg 6 steps), decoder loss: 0.0119, total loss: 0.0119\n",
      "Epoch [181/200], Step [1/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [181/200], Step [2/6], decoder loss: 0.0113, encoder1_activation l2 loss: 0.0000, total loss: 0.0113\n",
      "Epoch [181/200], Step [3/6], decoder loss: 0.0115, encoder1_activation l2 loss: 0.0000, total loss: 0.0115\n",
      "Epoch [181/200], Step [4/6], decoder loss: 0.0119, encoder1_activation l2 loss: 0.0000, total loss: 0.0119\n",
      "Epoch [181/200], Step [5/6], decoder loss: 0.0129, encoder1_activation l2 loss: 0.0000, total loss: 0.0129\n",
      "Epoch [181/200], Step [6/6], decoder loss: 0.0117, encoder1_activation l2 loss: 0.0000, total loss: 0.0117\n",
      "Epoch [181/200] (avg 6 steps), decoder loss: 0.0120, total loss: 0.0120\n",
      "Epoch [182/200], Step [1/6], decoder loss: 0.0115, encoder1_activation l2 loss: 0.0000, total loss: 0.0115\n",
      "Epoch [182/200], Step [2/6], decoder loss: 0.0119, encoder1_activation l2 loss: 0.0000, total loss: 0.0119\n",
      "Epoch [182/200], Step [3/6], decoder loss: 0.0111, encoder1_activation l2 loss: 0.0000, total loss: 0.0111\n",
      "Epoch [182/200], Step [4/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [182/200], Step [5/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [182/200], Step [6/6], decoder loss: 0.0136, encoder1_activation l2 loss: 0.0000, total loss: 0.0136\n",
      "Epoch [182/200] (avg 6 steps), decoder loss: 0.0121, total loss: 0.0121\n",
      "Epoch [183/200], Step [1/6], decoder loss: 0.0115, encoder1_activation l2 loss: 0.0000, total loss: 0.0115\n",
      "Epoch [183/200], Step [2/6], decoder loss: 0.0118, encoder1_activation l2 loss: 0.0000, total loss: 0.0118\n",
      "Epoch [183/200], Step [3/6], decoder loss: 0.0118, encoder1_activation l2 loss: 0.0000, total loss: 0.0118\n",
      "Epoch [183/200], Step [4/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [183/200], Step [5/6], decoder loss: 0.0111, encoder1_activation l2 loss: 0.0000, total loss: 0.0111\n",
      "Epoch [183/200], Step [6/6], decoder loss: 0.0145, encoder1_activation l2 loss: 0.0000, total loss: 0.0145\n",
      "Epoch [183/200] (avg 6 steps), decoder loss: 0.0122, total loss: 0.0122\n",
      "Epoch [184/200], Step [1/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [184/200], Step [2/6], decoder loss: 0.0128, encoder1_activation l2 loss: 0.0000, total loss: 0.0128\n",
      "Epoch [184/200], Step [3/6], decoder loss: 0.0121, encoder1_activation l2 loss: 0.0000, total loss: 0.0121\n",
      "Epoch [184/200], Step [4/6], decoder loss: 0.0112, encoder1_activation l2 loss: 0.0000, total loss: 0.0112\n",
      "Epoch [184/200], Step [5/6], decoder loss: 0.0110, encoder1_activation l2 loss: 0.0000, total loss: 0.0110\n",
      "Epoch [184/200], Step [6/6], decoder loss: 0.0129, encoder1_activation l2 loss: 0.0000, total loss: 0.0129\n",
      "Epoch [184/200] (avg 6 steps), decoder loss: 0.0120, total loss: 0.0120\n",
      "Epoch [185/200], Step [1/6], decoder loss: 0.0107, encoder1_activation l2 loss: 0.0000, total loss: 0.0107\n",
      "Epoch [185/200], Step [2/6], decoder loss: 0.0106, encoder1_activation l2 loss: 0.0000, total loss: 0.0106\n",
      "Epoch [185/200], Step [3/6], decoder loss: 0.0121, encoder1_activation l2 loss: 0.0000, total loss: 0.0121\n",
      "Epoch [185/200], Step [4/6], decoder loss: 0.0132, encoder1_activation l2 loss: 0.0000, total loss: 0.0132\n",
      "Epoch [185/200], Step [5/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [185/200], Step [6/6], decoder loss: 0.0138, encoder1_activation l2 loss: 0.0000, total loss: 0.0138\n",
      "Epoch [185/200] (avg 6 steps), decoder loss: 0.0121, total loss: 0.0121\n",
      "Epoch [186/200], Step [1/6], decoder loss: 0.0118, encoder1_activation l2 loss: 0.0000, total loss: 0.0118\n",
      "Epoch [186/200], Step [2/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [186/200], Step [3/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [186/200], Step [4/6], decoder loss: 0.0111, encoder1_activation l2 loss: 0.0000, total loss: 0.0111\n",
      "Epoch [186/200], Step [5/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [186/200], Step [6/6], decoder loss: 0.0115, encoder1_activation l2 loss: 0.0000, total loss: 0.0115\n",
      "Epoch [186/200] (avg 6 steps), decoder loss: 0.0119, total loss: 0.0119\n",
      "Epoch [187/200], Step [1/6], decoder loss: 0.0117, encoder1_activation l2 loss: 0.0000, total loss: 0.0117\n",
      "Epoch [187/200], Step [2/6], decoder loss: 0.0116, encoder1_activation l2 loss: 0.0000, total loss: 0.0116\n",
      "Epoch [187/200], Step [3/6], decoder loss: 0.0112, encoder1_activation l2 loss: 0.0000, total loss: 0.0112\n",
      "Epoch [187/200], Step [4/6], decoder loss: 0.0113, encoder1_activation l2 loss: 0.0000, total loss: 0.0113\n",
      "Epoch [187/200], Step [5/6], decoder loss: 0.0130, encoder1_activation l2 loss: 0.0000, total loss: 0.0130\n",
      "Epoch [187/200], Step [6/6], decoder loss: 0.0137, encoder1_activation l2 loss: 0.0000, total loss: 0.0137\n",
      "Epoch [187/200] (avg 6 steps), decoder loss: 0.0121, total loss: 0.0121\n",
      "Epoch [188/200], Step [1/6], decoder loss: 0.0111, encoder1_activation l2 loss: 0.0000, total loss: 0.0111\n",
      "Epoch [188/200], Step [2/6], decoder loss: 0.0110, encoder1_activation l2 loss: 0.0000, total loss: 0.0110\n",
      "Epoch [188/200], Step [3/6], decoder loss: 0.0144, encoder1_activation l2 loss: 0.0000, total loss: 0.0144\n",
      "Epoch [188/200], Step [4/6], decoder loss: 0.0121, encoder1_activation l2 loss: 0.0000, total loss: 0.0121\n",
      "Epoch [188/200], Step [5/6], decoder loss: 0.0113, encoder1_activation l2 loss: 0.0000, total loss: 0.0113\n",
      "Epoch [188/200], Step [6/6], decoder loss: 0.0119, encoder1_activation l2 loss: 0.0000, total loss: 0.0119\n",
      "Epoch [188/200] (avg 6 steps), decoder loss: 0.0120, total loss: 0.0120\n",
      "Epoch [189/200], Step [1/6], decoder loss: 0.0162, encoder1_activation l2 loss: 0.0000, total loss: 0.0162\n",
      "Epoch [189/200], Step [2/6], decoder loss: 0.0101, encoder1_activation l2 loss: 0.0000, total loss: 0.0101\n",
      "Epoch [189/200], Step [3/6], decoder loss: 0.0105, encoder1_activation l2 loss: 0.0000, total loss: 0.0105\n",
      "Epoch [189/200], Step [4/6], decoder loss: 0.0112, encoder1_activation l2 loss: 0.0000, total loss: 0.0112\n",
      "Epoch [189/200], Step [5/6], decoder loss: 0.0116, encoder1_activation l2 loss: 0.0000, total loss: 0.0116\n",
      "Epoch [189/200], Step [6/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [189/200] (avg 6 steps), decoder loss: 0.0120, total loss: 0.0120\n",
      "Epoch [190/200], Step [1/6], decoder loss: 0.0107, encoder1_activation l2 loss: 0.0000, total loss: 0.0107\n",
      "Epoch [190/200], Step [2/6], decoder loss: 0.0112, encoder1_activation l2 loss: 0.0000, total loss: 0.0112\n",
      "Epoch [190/200], Step [3/6], decoder loss: 0.0111, encoder1_activation l2 loss: 0.0000, total loss: 0.0111\n",
      "Epoch [190/200], Step [4/6], decoder loss: 0.0136, encoder1_activation l2 loss: 0.0000, total loss: 0.0136\n",
      "Epoch [190/200], Step [5/6], decoder loss: 0.0134, encoder1_activation l2 loss: 0.0000, total loss: 0.0134\n",
      "Epoch [190/200], Step [6/6], decoder loss: 0.0118, encoder1_activation l2 loss: 0.0000, total loss: 0.0118\n",
      "Epoch [190/200] (avg 6 steps), decoder loss: 0.0119, total loss: 0.0119\n",
      "Epoch [191/200], Step [1/6], decoder loss: 0.0111, encoder1_activation l2 loss: 0.0000, total loss: 0.0111\n",
      "Epoch [191/200], Step [2/6], decoder loss: 0.0113, encoder1_activation l2 loss: 0.0000, total loss: 0.0113\n",
      "Epoch [191/200], Step [3/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [191/200], Step [4/6], decoder loss: 0.0113, encoder1_activation l2 loss: 0.0000, total loss: 0.0113\n",
      "Epoch [191/200], Step [5/6], decoder loss: 0.0145, encoder1_activation l2 loss: 0.0000, total loss: 0.0145\n",
      "Epoch [191/200], Step [6/6], decoder loss: 0.0107, encoder1_activation l2 loss: 0.0000, total loss: 0.0107\n",
      "Epoch [191/200] (avg 6 steps), decoder loss: 0.0119, total loss: 0.0119\n",
      "Epoch [192/200], Step [1/6], decoder loss: 0.0108, encoder1_activation l2 loss: 0.0000, total loss: 0.0108\n",
      "Epoch [192/200], Step [2/6], decoder loss: 0.0113, encoder1_activation l2 loss: 0.0000, total loss: 0.0113\n",
      "Epoch [192/200], Step [3/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [192/200], Step [4/6], decoder loss: 0.0138, encoder1_activation l2 loss: 0.0000, total loss: 0.0138\n",
      "Epoch [192/200], Step [5/6], decoder loss: 0.0112, encoder1_activation l2 loss: 0.0000, total loss: 0.0112\n",
      "Epoch [192/200], Step [6/6], decoder loss: 0.0121, encoder1_activation l2 loss: 0.0000, total loss: 0.0121\n",
      "Epoch [192/200] (avg 6 steps), decoder loss: 0.0120, total loss: 0.0120\n",
      "Epoch [193/200], Step [1/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [193/200], Step [2/6], decoder loss: 0.0114, encoder1_activation l2 loss: 0.0000, total loss: 0.0114\n",
      "Epoch [193/200], Step [3/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [193/200], Step [4/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [193/200], Step [5/6], decoder loss: 0.0113, encoder1_activation l2 loss: 0.0000, total loss: 0.0113\n",
      "Epoch [193/200], Step [6/6], decoder loss: 0.0111, encoder1_activation l2 loss: 0.0000, total loss: 0.0111\n",
      "Epoch [193/200] (avg 6 steps), decoder loss: 0.0119, total loss: 0.0119\n",
      "Epoch [194/200], Step [1/6], decoder loss: 0.0126, encoder1_activation l2 loss: 0.0000, total loss: 0.0126\n",
      "Epoch [194/200], Step [2/6], decoder loss: 0.0115, encoder1_activation l2 loss: 0.0000, total loss: 0.0115\n",
      "Epoch [194/200], Step [3/6], decoder loss: 0.0120, encoder1_activation l2 loss: 0.0000, total loss: 0.0120\n",
      "Epoch [194/200], Step [4/6], decoder loss: 0.0115, encoder1_activation l2 loss: 0.0000, total loss: 0.0115\n",
      "Epoch [194/200], Step [5/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [194/200], Step [6/6], decoder loss: 0.0113, encoder1_activation l2 loss: 0.0000, total loss: 0.0113\n",
      "Epoch [194/200] (avg 6 steps), decoder loss: 0.0119, total loss: 0.0119\n",
      "Epoch [195/200], Step [1/6], decoder loss: 0.0108, encoder1_activation l2 loss: 0.0000, total loss: 0.0108\n",
      "Epoch [195/200], Step [2/6], decoder loss: 0.0132, encoder1_activation l2 loss: 0.0000, total loss: 0.0132\n",
      "Epoch [195/200], Step [3/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [195/200], Step [4/6], decoder loss: 0.0130, encoder1_activation l2 loss: 0.0000, total loss: 0.0130\n",
      "Epoch [195/200], Step [5/6], decoder loss: 0.0113, encoder1_activation l2 loss: 0.0000, total loss: 0.0113\n",
      "Epoch [195/200], Step [6/6], decoder loss: 0.0106, encoder1_activation l2 loss: 0.0000, total loss: 0.0106\n",
      "Epoch [195/200] (avg 6 steps), decoder loss: 0.0118, total loss: 0.0118\n",
      "Epoch [196/200], Step [1/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [196/200], Step [2/6], decoder loss: 0.0130, encoder1_activation l2 loss: 0.0000, total loss: 0.0130\n",
      "Epoch [196/200], Step [3/6], decoder loss: 0.0121, encoder1_activation l2 loss: 0.0000, total loss: 0.0121\n",
      "Epoch [196/200], Step [4/6], decoder loss: 0.0116, encoder1_activation l2 loss: 0.0000, total loss: 0.0116\n",
      "Epoch [196/200], Step [5/6], decoder loss: 0.0104, encoder1_activation l2 loss: 0.0000, total loss: 0.0104\n",
      "Epoch [196/200], Step [6/6], decoder loss: 0.0117, encoder1_activation l2 loss: 0.0000, total loss: 0.0117\n",
      "Epoch [196/200] (avg 6 steps), decoder loss: 0.0119, total loss: 0.0119\n",
      "Epoch [197/200], Step [1/6], decoder loss: 0.0120, encoder1_activation l2 loss: 0.0000, total loss: 0.0120\n",
      "Epoch [197/200], Step [2/6], decoder loss: 0.0112, encoder1_activation l2 loss: 0.0000, total loss: 0.0112\n",
      "Epoch [197/200], Step [3/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [197/200], Step [4/6], decoder loss: 0.0121, encoder1_activation l2 loss: 0.0000, total loss: 0.0121\n",
      "Epoch [197/200], Step [5/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [197/200], Step [6/6], decoder loss: 0.0103, encoder1_activation l2 loss: 0.0000, total loss: 0.0103\n",
      "Epoch [197/200] (avg 6 steps), decoder loss: 0.0118, total loss: 0.0118\n",
      "Epoch [198/200], Step [1/6], decoder loss: 0.0113, encoder1_activation l2 loss: 0.0000, total loss: 0.0113\n",
      "Epoch [198/200], Step [2/6], decoder loss: 0.0128, encoder1_activation l2 loss: 0.0000, total loss: 0.0128\n",
      "Epoch [198/200], Step [3/6], decoder loss: 0.0134, encoder1_activation l2 loss: 0.0000, total loss: 0.0134\n",
      "Epoch [198/200], Step [4/6], decoder loss: 0.0113, encoder1_activation l2 loss: 0.0000, total loss: 0.0113\n",
      "Epoch [198/200], Step [5/6], decoder loss: 0.0113, encoder1_activation l2 loss: 0.0000, total loss: 0.0113\n",
      "Epoch [198/200], Step [6/6], decoder loss: 0.0113, encoder1_activation l2 loss: 0.0000, total loss: 0.0113\n",
      "Epoch [198/200] (avg 6 steps), decoder loss: 0.0119, total loss: 0.0119\n",
      "Epoch [199/200], Step [1/6], decoder loss: 0.0126, encoder1_activation l2 loss: 0.0000, total loss: 0.0126\n",
      "Epoch [199/200], Step [2/6], decoder loss: 0.0112, encoder1_activation l2 loss: 0.0000, total loss: 0.0112\n",
      "Epoch [199/200], Step [3/6], decoder loss: 0.0126, encoder1_activation l2 loss: 0.0000, total loss: 0.0126\n",
      "Epoch [199/200], Step [4/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [199/200], Step [5/6], decoder loss: 0.0121, encoder1_activation l2 loss: 0.0000, total loss: 0.0121\n",
      "Epoch [199/200], Step [6/6], decoder loss: 0.0095, encoder1_activation l2 loss: 0.0000, total loss: 0.0095\n",
      "Epoch [199/200] (avg 6 steps), decoder loss: 0.0118, total loss: 0.0118\n",
      "Epoch [200/200], Step [1/6], decoder loss: 0.0129, encoder1_activation l2 loss: 0.0000, total loss: 0.0129\n",
      "Epoch [200/200], Step [2/6], decoder loss: 0.0113, encoder1_activation l2 loss: 0.0000, total loss: 0.0113\n",
      "Epoch [200/200], Step [3/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [200/200], Step [4/6], decoder loss: 0.0101, encoder1_activation l2 loss: 0.0000, total loss: 0.0101\n",
      "Epoch [200/200], Step [5/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [200/200], Step [6/6], decoder loss: 0.0135, encoder1_activation l2 loss: 0.0000, total loss: 0.0135\n",
      "Epoch [200/200] (avg 6 steps), decoder loss: 0.0120, total loss: 0.0120\n"
     ]
    }
   ],
   "source": [
    "training_loss = ae.fit(train_data, 128, 200, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_loss, label=\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.7756, 0.7027, 0.5600,  ..., 0.0223, 0.0233, 0.0230],\n",
       "         [0.7756, 0.7027, 0.5600,  ..., 0.0223, 0.0233, 0.0230],\n",
       "         [0.7756, 0.7027, 0.5600,  ..., 0.0223, 0.0233, 0.0230],\n",
       "         ...,\n",
       "         [0.7756, 0.7027, 0.5600,  ..., 0.0223, 0.0233, 0.0230],\n",
       "         [0.7756, 0.7027, 0.5600,  ..., 0.0223, 0.0233, 0.0230],\n",
       "         [0.7756, 0.7027, 0.5600,  ..., 0.0223, 0.0233, 0.0230]],\n",
       "        grad_fn=<SigmoidBackward0>),\n",
       " tensor([0.0263, 0.0125, 0.0148, 0.0101, 0.0064, 0.0454, 0.0052, 0.0080, 0.0120,\n",
       "         0.0247, 0.0137, 0.0080, 0.0300, 0.0063, 0.0035, 0.0131, 0.0389, 0.0486,\n",
       "         0.0067, 0.0134, 0.0132, 0.0095, 0.0153, 0.0066, 0.0074, 0.0109, 0.0115,\n",
       "         0.0080, 0.0242, 0.0062, 0.0063, 0.0056, 0.0133, 0.0080, 0.0113, 0.0060,\n",
       "         0.0224, 0.0114, 0.0119, 0.0057, 0.0238, 0.0098, 0.0297, 0.0084, 0.0069,\n",
       "         0.0278, 0.0128, 0.0137, 0.0118, 0.0181, 0.0080, 0.0164, 0.0028, 0.0150,\n",
       "         0.0250, 0.0312, 0.0072, 0.0114, 0.0269, 0.0062, 0.0088, 0.0291, 0.0062,\n",
       "         0.0249, 0.0071, 0.0114, 0.0066, 0.0097, 0.0086, 0.0086, 0.0101, 0.0083,\n",
       "         0.0154, 0.0187, 0.0166, 0.0097, 0.0141, 0.0320, 0.0103, 0.0328, 0.0152,\n",
       "         0.0091, 0.0111, 0.0238, 0.0377, 0.0088, 0.0133, 0.0119, 0.0303, 0.0108,\n",
       "         0.0085, 0.0093, 0.0332, 0.0230, 0.0062, 0.0067, 0.0081, 0.0066, 0.0263,\n",
       "         0.0286, 0.0240, 0.0130, 0.0064, 0.0067, 0.0081, 0.0079, 0.0104, 0.0122,\n",
       "         0.0095, 0.0100, 0.0059, 0.0075, 0.0057, 0.0105, 0.0106, 0.0189, 0.0104,\n",
       "         0.0058, 0.0134, 0.0074, 0.0425, 0.0154, 0.0143, 0.0229, 0.0076, 0.0107,\n",
       "         0.0048, 0.1198, 0.0120, 0.0104, 0.0378, 0.0430, 0.0139, 0.0269, 0.0058,\n",
       "         0.0046, 0.0151, 0.0061, 0.0150, 0.0064, 0.0059, 0.0278, 0.0081, 0.0042,\n",
       "         0.0060, 0.0070, 0.0141, 0.0117, 0.0088, 0.0058, 0.0047, 0.0057, 0.0114,\n",
       "         0.0165, 0.0115, 0.0068, 0.0049, 0.0110, 0.0094, 0.0153, 0.0170, 0.0042,\n",
       "         0.0098, 0.0101, 0.0047, 0.0033, 0.0139, 0.0077, 0.0143, 0.0056, 0.0061,\n",
       "         0.0108, 0.0101, 0.0057, 0.0260, 0.0064, 0.0281, 0.0085, 0.0066, 0.0133,\n",
       "         0.0051, 0.0065, 0.0110, 0.0040, 0.0073, 0.0026, 0.0078, 0.0055, 0.0118,\n",
       "         0.0033, 0.0129, 0.0088, 0.0056, 0.0247, 0.0467, 0.0061, 0.0071, 0.0069,\n",
       "         0.0087, 0.0079, 0.0120, 0.0044, 0.0216, 0.0104, 0.0096, 0.0071, 0.0104,\n",
       "         0.0058, 0.0065, 0.0044, 0.0255, 0.0045, 0.0321, 0.0137, 0.0316, 0.0223,\n",
       "         0.0032, 0.0072, 0.0042, 0.0040, 0.0036, 0.0059, 0.0305, 0.0046, 0.0104,\n",
       "         0.0077, 0.0049, 0.0044, 0.0052, 0.0073, 0.0104, 0.0022, 0.0068, 0.0269,\n",
       "         0.0132, 0.0038, 0.0041, 0.0072, 0.0028, 0.0440, 0.0233, 0.0028, 0.0053,\n",
       "         0.0036, 0.0071, 0.0104, 0.0081, 0.0088, 0.0036, 0.0243, 0.0220, 0.0043,\n",
       "         0.0036, 0.0034, 0.0258, 0.0053, 0.0079, 0.0241, 0.0024, 0.0036, 0.0084,\n",
       "         0.0030, 0.0091, 0.0095, 0.0055, 0.0032, 0.0032, 0.0057, 0.0036, 0.0050,\n",
       "         0.0054, 0.0063, 0.0041, 0.0028, 0.0075, 0.0028, 0.0111, 0.0097, 0.0030,\n",
       "         0.0042, 0.0043, 0.0084, 0.0027, 0.0023, 0.0060, 0.0021, 0.0094, 0.0053,\n",
       "         0.0027, 0.0088, 0.0095, 0.0260, 0.0033, 0.0058, 0.0073, 0.0067, 0.0094,\n",
       "         0.0055, 0.0114, 0.0041, 0.0082, 0.0039, 0.0016, 0.0061, 0.0049, 0.0109,\n",
       "         0.0249, 0.0027, 0.0080, 0.0030, 0.0227, 0.0213, 0.0014, 0.0067, 0.0025,\n",
       "         0.0052, 0.0023, 0.0029, 0.0030, 0.0114, 0.0146, 0.0030, 0.0063, 0.0068,\n",
       "         0.0116, 0.0082, 0.0091, 0.0034, 0.0022, 0.0027, 0.0062, 0.0200, 0.0034,\n",
       "         0.0026, 0.0085, 0.0050, 0.0025, 0.0020, 0.0039, 0.0208, 0.0206, 0.0200,\n",
       "         0.0026, 0.0055, 0.0027, 0.0058, 0.0062, 0.0038, 0.0066, 0.0077, 0.0266,\n",
       "         0.0031, 0.0047, 0.0127, 0.0097, 0.0241, 0.0067, 0.0068, 0.0060, 0.0033,\n",
       "         0.0024, 0.0038, 0.0046, 0.0030, 0.0098, 0.0159, 0.0066, 0.0213, 0.0049,\n",
       "         0.0396, 0.0055, 0.0068, 0.0077, 0.0031, 0.0513, 0.0202, 0.0074, 0.0017,\n",
       "         0.0097, 0.0103, 0.0123, 0.0079, 0.0236, 0.0088, 0.0078, 0.0080, 0.0072,\n",
       "         0.0127, 0.0259, 0.0229, 0.0081, 0.0076, 0.0035, 0.0087, 0.0050, 0.0100,\n",
       "         0.0040, 0.0030, 0.0061, 0.0059, 0.0091, 0.0040, 0.0047, 0.0032, 0.0086,\n",
       "         0.0042, 0.0043, 0.0026, 0.0052, 0.0099, 0.0028, 0.0075, 0.0204, 0.0207,\n",
       "         0.0038, 0.0186, 0.0038, 0.0030, 0.0301, 0.0138, 0.0041, 0.0268, 0.0025,\n",
       "         0.0220, 0.0034, 0.0202, 0.0078, 0.0025, 0.0017, 0.0092, 0.0024, 0.0081,\n",
       "         0.0051, 0.0027, 0.0030, 0.0040, 0.0077, 0.0099, 0.0068, 0.0111, 0.0041,\n",
       "         0.0055, 0.0060, 0.0240, 0.0196, 0.0078, 0.0124, 0.0039, 0.0312, 0.0087,\n",
       "         0.0130, 0.0030, 0.0071, 0.0071, 0.0070, 0.0059, 0.0269, 0.0030, 0.0026,\n",
       "         0.0023, 0.0062, 0.0022, 0.0025, 0.0083, 0.0232, 0.0050, 0.0113, 0.0043,\n",
       "         0.0025, 0.0018, 0.0084, 0.0271, 0.0075, 0.0040, 0.0070, 0.0032, 0.0039,\n",
       "         0.0076, 0.0043, 0.0045, 0.0195, 0.0118, 0.0083, 0.0227, 0.0079, 0.0217,\n",
       "         0.0076, 0.0254, 0.0018, 0.0080, 0.0037, 0.0245, 0.0084, 0.0058, 0.0044,\n",
       "         0.0209, 0.0092, 0.0046, 0.0307, 0.0095, 0.0032, 0.0120, 0.0253, 0.0082,\n",
       "         0.0034, 0.0084, 0.0078, 0.0107, 0.0027, 0.0110, 0.0078, 0.0064, 0.0078,\n",
       "         0.0217, 0.0078, 0.0037, 0.0179, 0.0297, 0.0024, 0.0034, 0.0076, 0.0048,\n",
       "         0.0053, 0.0083, 0.0028, 0.0226, 0.0050, 0.0082, 0.0043, 0.0103, 0.0095,\n",
       "         0.0067, 0.0114, 0.0043, 0.0067, 0.0042, 0.0043, 0.0033, 0.0070, 0.0046,\n",
       "         0.0062, 0.0400, 0.0080, 0.0024, 0.0143, 0.0086, 0.0077, 0.0023, 0.0035,\n",
       "         0.0060, 0.0053, 0.0239, 0.0045, 0.0028, 0.0049, 0.0065, 0.0240, 0.0067,\n",
       "         0.0037, 0.0075, 0.0052, 0.0030, 0.0092, 0.0251, 0.0082, 0.0043, 0.0048,\n",
       "         0.0202, 0.0069, 0.0080, 0.0112, 0.0067, 0.0073, 0.0064, 0.0070, 0.0048,\n",
       "         0.0047, 0.0095, 0.0049, 0.0062, 0.0263, 0.0034, 0.0059, 0.0053, 0.0139,\n",
       "         0.0099, 0.0032, 0.0040, 0.0045, 0.0035, 0.0226, 0.0074, 0.0181, 0.0048,\n",
       "         0.0090, 0.0308, 0.0080, 0.0072, 0.0033, 0.0257, 0.0051, 0.0047, 0.0060,\n",
       "         0.0120, 0.0078, 0.0024, 0.0045, 0.0031, 0.0157, 0.0063, 0.0050, 0.0100,\n",
       "         0.0111, 0.0051, 0.0093, 0.0055, 0.0073, 0.0080, 0.0087, 0.0069, 0.0098,\n",
       "         0.0085, 0.0235, 0.0040, 0.0044, 0.0062, 0.0127, 0.0040, 0.0109, 0.0214,\n",
       "         0.0214, 0.0043, 0.0095, 0.0291, 0.0036, 0.0061, 0.0074, 0.0169, 0.0461,\n",
       "         0.0603, 0.0581, 0.0471, 0.0532, 0.0418, 0.0440, 0.0593, 0.0484, 0.0358,\n",
       "         0.0405, 0.0369, 0.0684, 0.0505, 0.0538, 0.0426, 0.0433, 0.0566, 0.0292,\n",
       "         0.0155, 0.0098, 0.0212, 0.0075, 0.0149, 0.0058, 0.0209, 0.0028, 0.0096,\n",
       "         0.0074, 0.0064, 0.0335, 0.0108, 0.0040, 0.0140, 0.0045, 0.0059, 0.0119,\n",
       "         0.0441, 0.0523, 0.0129, 0.0080, 0.0107, 0.0272, 0.0324, 0.0276, 0.0354,\n",
       "         0.0112, 0.0123, 0.0151, 0.0479, 0.0283, 0.0066, 0.0068, 0.0271, 0.0094,\n",
       "         0.0049, 0.0101, 0.0065, 0.0123, 0.0124, 0.0088, 0.0118, 0.0046, 0.0108,\n",
       "         0.0258, 0.0082, 0.0047, 0.0036, 0.0071, 0.0035, 0.0097, 0.0090, 0.0099,\n",
       "         0.0083, 0.0201, 0.0116, 0.0264], grad_fn=<MeanBackward1>),\n",
       " tensor(4.3394e-07, grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ae(torch.tensor(train_data.astype(np.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0263, 0.0125, 0.0148, 0.0101, 0.0064, 0.0454, 0.0052, 0.0080, 0.0120,\n",
       "        0.0247, 0.0137, 0.0080, 0.0300, 0.0063, 0.0035, 0.0131, 0.0389, 0.0486,\n",
       "        0.0067, 0.0134, 0.0132, 0.0095, 0.0153, 0.0066, 0.0074, 0.0109, 0.0115,\n",
       "        0.0080, 0.0242, 0.0062, 0.0063, 0.0056, 0.0133, 0.0080, 0.0113, 0.0060,\n",
       "        0.0224, 0.0114, 0.0119, 0.0057, 0.0238, 0.0098, 0.0297, 0.0084, 0.0069,\n",
       "        0.0278, 0.0128, 0.0137, 0.0118, 0.0181, 0.0080, 0.0164, 0.0028, 0.0150,\n",
       "        0.0250, 0.0312, 0.0072, 0.0114, 0.0269, 0.0062, 0.0088, 0.0291, 0.0062,\n",
       "        0.0249, 0.0071, 0.0114, 0.0066, 0.0097, 0.0086, 0.0086, 0.0101, 0.0083,\n",
       "        0.0154, 0.0187, 0.0166, 0.0097, 0.0141, 0.0320, 0.0103, 0.0328, 0.0152,\n",
       "        0.0091, 0.0111, 0.0238, 0.0377, 0.0088, 0.0133, 0.0119, 0.0303, 0.0108,\n",
       "        0.0085, 0.0093, 0.0332, 0.0230, 0.0062, 0.0067, 0.0081, 0.0066, 0.0263,\n",
       "        0.0286, 0.0240, 0.0130, 0.0064, 0.0067, 0.0081, 0.0079, 0.0104, 0.0122,\n",
       "        0.0095, 0.0100, 0.0059, 0.0075, 0.0057, 0.0105, 0.0106, 0.0189, 0.0104,\n",
       "        0.0058, 0.0134, 0.0074, 0.0425, 0.0154, 0.0143, 0.0229, 0.0076, 0.0107,\n",
       "        0.0048, 0.1198, 0.0120, 0.0104, 0.0378, 0.0430, 0.0139, 0.0269, 0.0058,\n",
       "        0.0046, 0.0151, 0.0061, 0.0150, 0.0064, 0.0059, 0.0278, 0.0081, 0.0042,\n",
       "        0.0060, 0.0070, 0.0141, 0.0117, 0.0088, 0.0058, 0.0047, 0.0057, 0.0114,\n",
       "        0.0165, 0.0115, 0.0068, 0.0049, 0.0110, 0.0094, 0.0153, 0.0170, 0.0042,\n",
       "        0.0098, 0.0101, 0.0047, 0.0033, 0.0139, 0.0077, 0.0143, 0.0056, 0.0061,\n",
       "        0.0108, 0.0101, 0.0057, 0.0260, 0.0064, 0.0281, 0.0085, 0.0066, 0.0133,\n",
       "        0.0051, 0.0065, 0.0110, 0.0040, 0.0073, 0.0026, 0.0078, 0.0055, 0.0118,\n",
       "        0.0033, 0.0129, 0.0088, 0.0056, 0.0247, 0.0467, 0.0061, 0.0071, 0.0069,\n",
       "        0.0087, 0.0079, 0.0120, 0.0044, 0.0216, 0.0104, 0.0096, 0.0071, 0.0104,\n",
       "        0.0058, 0.0065, 0.0044, 0.0255, 0.0045, 0.0321, 0.0137, 0.0316, 0.0223,\n",
       "        0.0032, 0.0072, 0.0042, 0.0040, 0.0036, 0.0059, 0.0305, 0.0046, 0.0104,\n",
       "        0.0077, 0.0049, 0.0044, 0.0052, 0.0073, 0.0104, 0.0022, 0.0068, 0.0269,\n",
       "        0.0132, 0.0038, 0.0041, 0.0072, 0.0028, 0.0440, 0.0233, 0.0028, 0.0053,\n",
       "        0.0036, 0.0071, 0.0104, 0.0081, 0.0088, 0.0036, 0.0243, 0.0220, 0.0043,\n",
       "        0.0036, 0.0034, 0.0258, 0.0053, 0.0079, 0.0241, 0.0024, 0.0036, 0.0084,\n",
       "        0.0030, 0.0091, 0.0095, 0.0055, 0.0032, 0.0032, 0.0057, 0.0036, 0.0050,\n",
       "        0.0054, 0.0063, 0.0041, 0.0028, 0.0075, 0.0028, 0.0111, 0.0097, 0.0030,\n",
       "        0.0042, 0.0043, 0.0084, 0.0027, 0.0023, 0.0060, 0.0021, 0.0094, 0.0053,\n",
       "        0.0027, 0.0088, 0.0095, 0.0260, 0.0033, 0.0058, 0.0073, 0.0067, 0.0094,\n",
       "        0.0055, 0.0114, 0.0041, 0.0082, 0.0039, 0.0016, 0.0061, 0.0049, 0.0109,\n",
       "        0.0249, 0.0027, 0.0080, 0.0030, 0.0227, 0.0213, 0.0014, 0.0067, 0.0025,\n",
       "        0.0052, 0.0023, 0.0029, 0.0030, 0.0114, 0.0146, 0.0030, 0.0063, 0.0068,\n",
       "        0.0116, 0.0082, 0.0091, 0.0034, 0.0022, 0.0027, 0.0062, 0.0200, 0.0034,\n",
       "        0.0026, 0.0085, 0.0050, 0.0025, 0.0020, 0.0039, 0.0208, 0.0206, 0.0200,\n",
       "        0.0026, 0.0055, 0.0027, 0.0058, 0.0062, 0.0038, 0.0066, 0.0077, 0.0266,\n",
       "        0.0031, 0.0047, 0.0127, 0.0097, 0.0241, 0.0067, 0.0068, 0.0060, 0.0033,\n",
       "        0.0024, 0.0038, 0.0046, 0.0030, 0.0098, 0.0159, 0.0066, 0.0213, 0.0049,\n",
       "        0.0396, 0.0055, 0.0068, 0.0077, 0.0031, 0.0513, 0.0202, 0.0074, 0.0017,\n",
       "        0.0097, 0.0103, 0.0123, 0.0079, 0.0236, 0.0088, 0.0078, 0.0080, 0.0072,\n",
       "        0.0127, 0.0259, 0.0229, 0.0081, 0.0076, 0.0035, 0.0087, 0.0050, 0.0100,\n",
       "        0.0040, 0.0030, 0.0061, 0.0059, 0.0091, 0.0040, 0.0047, 0.0032, 0.0086,\n",
       "        0.0042, 0.0043, 0.0026, 0.0052, 0.0099, 0.0028, 0.0075, 0.0204, 0.0207,\n",
       "        0.0038, 0.0186, 0.0038, 0.0030, 0.0301, 0.0138, 0.0041, 0.0268, 0.0025,\n",
       "        0.0220, 0.0034, 0.0202, 0.0078, 0.0025, 0.0017, 0.0092, 0.0024, 0.0081,\n",
       "        0.0051, 0.0027, 0.0030, 0.0040, 0.0077, 0.0099, 0.0068, 0.0111, 0.0041,\n",
       "        0.0055, 0.0060, 0.0240, 0.0196, 0.0078, 0.0124, 0.0039, 0.0312, 0.0087,\n",
       "        0.0130, 0.0030, 0.0071, 0.0071, 0.0070, 0.0059, 0.0269, 0.0030, 0.0026,\n",
       "        0.0023, 0.0062, 0.0022, 0.0025, 0.0083, 0.0232, 0.0050, 0.0113, 0.0043,\n",
       "        0.0025, 0.0018, 0.0084, 0.0271, 0.0075, 0.0040, 0.0070, 0.0032, 0.0039,\n",
       "        0.0076, 0.0043, 0.0045, 0.0195, 0.0118, 0.0083, 0.0227, 0.0079, 0.0217,\n",
       "        0.0076, 0.0254, 0.0018, 0.0080, 0.0037, 0.0245, 0.0084, 0.0058, 0.0044,\n",
       "        0.0209, 0.0092, 0.0046, 0.0307, 0.0095, 0.0032, 0.0120, 0.0253, 0.0082,\n",
       "        0.0034, 0.0084, 0.0078, 0.0107, 0.0027, 0.0110, 0.0078, 0.0064, 0.0078,\n",
       "        0.0217, 0.0078, 0.0037, 0.0179, 0.0297, 0.0024, 0.0034, 0.0076, 0.0048,\n",
       "        0.0053, 0.0083, 0.0028, 0.0226, 0.0050, 0.0082, 0.0043, 0.0103, 0.0095,\n",
       "        0.0067, 0.0114, 0.0043, 0.0067, 0.0042, 0.0043, 0.0033, 0.0070, 0.0046,\n",
       "        0.0062, 0.0400, 0.0080, 0.0024, 0.0143, 0.0086, 0.0077, 0.0023, 0.0035,\n",
       "        0.0060, 0.0053, 0.0239, 0.0045, 0.0028, 0.0049, 0.0065, 0.0240, 0.0067,\n",
       "        0.0037, 0.0075, 0.0052, 0.0030, 0.0092, 0.0251, 0.0082, 0.0043, 0.0048,\n",
       "        0.0202, 0.0069, 0.0080, 0.0112, 0.0067, 0.0073, 0.0064, 0.0070, 0.0048,\n",
       "        0.0047, 0.0095, 0.0049, 0.0062, 0.0263, 0.0034, 0.0059, 0.0053, 0.0139,\n",
       "        0.0099, 0.0032, 0.0040, 0.0045, 0.0035, 0.0226, 0.0074, 0.0181, 0.0048,\n",
       "        0.0090, 0.0308, 0.0080, 0.0072, 0.0033, 0.0257, 0.0051, 0.0047, 0.0060,\n",
       "        0.0120, 0.0078, 0.0024, 0.0045, 0.0031, 0.0157, 0.0063, 0.0050, 0.0100,\n",
       "        0.0111, 0.0051, 0.0093, 0.0055, 0.0073, 0.0080, 0.0087, 0.0069, 0.0098,\n",
       "        0.0085, 0.0235, 0.0040, 0.0044, 0.0062, 0.0127, 0.0040, 0.0109, 0.0214,\n",
       "        0.0214, 0.0043, 0.0095, 0.0291, 0.0036, 0.0061, 0.0074, 0.0169, 0.0461,\n",
       "        0.0603, 0.0581, 0.0471, 0.0532, 0.0418, 0.0440, 0.0593, 0.0484, 0.0358,\n",
       "        0.0405, 0.0369, 0.0684, 0.0505, 0.0538, 0.0426, 0.0433, 0.0566, 0.0292,\n",
       "        0.0155, 0.0098, 0.0212, 0.0075, 0.0149, 0.0058, 0.0209, 0.0028, 0.0096,\n",
       "        0.0074, 0.0064, 0.0335, 0.0108, 0.0040, 0.0140, 0.0045, 0.0059, 0.0119,\n",
       "        0.0441, 0.0523, 0.0129, 0.0080, 0.0107, 0.0272, 0.0324, 0.0276, 0.0354,\n",
       "        0.0112, 0.0123, 0.0151, 0.0479, 0.0283, 0.0066, 0.0068, 0.0271, 0.0094,\n",
       "        0.0049, 0.0101, 0.0065, 0.0123, 0.0124, 0.0088, 0.0118, 0.0046, 0.0108,\n",
       "        0.0258, 0.0082, 0.0047, 0.0036, 0.0071, 0.0035, 0.0097, 0.0090, 0.0099,\n",
       "        0.0083, 0.0201, 0.0116, 0.0264], grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class InferenceAutoencoder(Autoencoder):\n",
    "    def forward(self, x):\n",
    "        prep = self.prep_layer(x)\n",
    "        encoded = self.encoder1(prep)\n",
    "        encoded = self.encoder1_activation(encoded)\n",
    "        encoded = self.encoder2(encoded)\n",
    "        encoded = self.encoder2_activation(encoded)\n",
    "        decoded = self.decoder(encoded)\n",
    "        mse = self.mse_loss(prep, decoded)\n",
    "        return torch.mean(mse, dim=1)  # Only return mse loss\n",
    "    \n",
    "iae = InferenceAutoencoder(input_shape=(51,), l2_lambda=1e-4)\n",
    "iae.load_state_dict(ae.state_dict())\n",
    "iae(torch.tensor(train_data.astype(np.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"../models/torch\", exist_ok=True)\n",
    "\n",
    "model_path = \"../models/torch/state_dict.pth\"\n",
    "torch.save(iae.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-examples-EfRPkUGI-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
