{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessingLayer(torch.nn.Module):\n",
    "    def forward(self, inputs: torch.Tensor):\n",
    "        min_val = inputs.min(axis=0).values\n",
    "        max_val = inputs.max(axis=0).values\n",
    "        normalized_inputs = torch.where(\n",
    "            (max_val - min_val) != 0,\n",
    "            (inputs - min_val) / (max_val - min_val + 1e-8),\n",
    "            torch.zeros_like(inputs)\n",
    "        )\n",
    "        return normalized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_shape: tuple[int], l2_lambda: float):\n",
    "        super().__init__()\n",
    "        self.l2_lambda = torch.tensor(l2_lambda)\n",
    "        self.prep_layer = PreprocessingLayer()\n",
    "        self.encoder1 = torch.nn.Linear(input_shape[0], 18)\n",
    "        self.encoder1_activation = torch.nn.ReLU()\n",
    "        self.encoder2 = torch.nn.Linear(18, 8)\n",
    "        self.encoder2_activation = torch.nn.Sigmoid()\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(8, 18),\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Linear(18, input_shape[0]),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "        self.mse_loss = torch.nn.MSELoss(reduction='none')\n",
    "\n",
    "    def forward(self, x):\n",
    "        prep = self.prep_layer(x)\n",
    "        encoded = self.encoder1(prep)\n",
    "        encoded = self.encoder1_activation(encoded)\n",
    "        l2_activity_loss = self.l2_lambda * torch.sum(encoded ** 2)\n",
    "        encoded = self.encoder2(encoded)\n",
    "        encoded = self.encoder2_activation(encoded)\n",
    "        decoded = self.decoder(encoded)\n",
    "        mse = self.mse_loss(prep, decoded)\n",
    "        avg_mse_per_input = torch.mean(mse, dim=1)\n",
    "        return decoded, avg_mse_per_input, l2_activity_loss\n",
    "    \n",
    "    def fit(self, inputs: np.ndarray, batch_size: int, epochs: int, optimizer, criterion):\n",
    "        inputs = torch.tensor(inputs.astype(np.float32))\n",
    "        targets = PreprocessingLayer()(inputs)\n",
    "        batched_data = DataLoader(targets, batch_size=batch_size, shuffle=True)\n",
    "        batch_steps = len(batched_data)\n",
    "        training_loss = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.train()\n",
    "            epoch_decoder_losses = []\n",
    "            epoch_total_losses = []\n",
    "            for idx, batch in enumerate(batched_data):\n",
    "                # Forward pass\n",
    "                decoded, _, l2_loss = self(batch)\n",
    "                loss = criterion(batch, decoded)\n",
    "                loss_value = loss.item()\n",
    "                epoch_decoder_losses.append(loss_value)\n",
    "\n",
    "                # Compute total loss\n",
    "                total_loss = loss + l2_loss\n",
    "                total_loss_value = total_loss.item()  # This slows down the code if using GPU, since we convert this value from CUDA to a python float\n",
    "                epoch_total_losses.append(total_loss_value)\n",
    "                \n",
    "                # Backward pass and optimization\n",
    "                optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                print(f\"Epoch [{epoch + 1}/{epochs}], Step [{idx + 1}/{batch_steps}], decoder loss: {loss_value:.4f}, encoder1_activation l2 loss: {l2_loss:.4f}, total loss: {total_loss_value:.4f}\")\n",
    "\n",
    "            epoch_decoder_avg_loss = np.array(epoch_decoder_losses).mean()\n",
    "            epoch_total_avg_loss = np.array(epoch_total_losses).mean()\n",
    "            training_loss.append(epoch_total_avg_loss)\n",
    "\n",
    "            print(f\"Epoch [{epoch + 1}/{epochs}] (avg {batch_steps} steps), decoder loss: {epoch_decoder_avg_loss:.4f}, total loss: {epoch_total_avg_loss:.4f}\")\n",
    "        \n",
    "        return training_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1940., 1987.,  670., ...,    0.,    0.,    0.],\n",
       "       [1869., 1872.,  714., ...,    0.,    0.,    0.],\n",
       "       [1819., 1924.,  672., ...,    0.,    0.,    0.],\n",
       "       ...,\n",
       "       [1171.,  989.,  293., ...,    0.,    0.,    0.],\n",
       "       [1225.,  960.,  289., ...,    0.,    0.,    0.],\n",
       "       [1190.,  994.,  257., ...,    0.,    0.,    0.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = np.load(\"data/data_386642.npy\")\n",
    "train_label = np.load(\"data/label_386642.npy\")\n",
    "\n",
    "# We want to feed the Autoencoder with GOOD data, so we filter the data by the label == 1\n",
    "train_data = train_data[train_label == 1]\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae = Autoencoder(input_shape=(51,), l2_lambda=1e-4)\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(ae.parameters(), lr=0.001, eps=1e-07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Step [1/6], decoder loss: 0.2104, encoder1_activation l2 loss: 0.0023, total loss: 0.2127\n",
      "Epoch [1/200], Step [2/6], decoder loss: 0.2088, encoder1_activation l2 loss: 0.0022, total loss: 0.2110\n",
      "Epoch [1/200], Step [3/6], decoder loss: 0.2069, encoder1_activation l2 loss: 0.0018, total loss: 0.2088\n",
      "Epoch [1/200], Step [4/6], decoder loss: 0.2050, encoder1_activation l2 loss: 0.0018, total loss: 0.2068\n",
      "Epoch [1/200], Step [5/6], decoder loss: 0.2032, encoder1_activation l2 loss: 0.0019, total loss: 0.2051\n",
      "Epoch [1/200], Step [6/6], decoder loss: 0.2020, encoder1_activation l2 loss: 0.0009, total loss: 0.2029\n",
      "Epoch [1/200] (avg 6 steps), decoder loss: 0.2061, total loss: 0.2079\n",
      "Epoch [2/200], Step [1/6], decoder loss: 0.1994, encoder1_activation l2 loss: 0.0015, total loss: 0.2009\n",
      "Epoch [2/200], Step [2/6], decoder loss: 0.1973, encoder1_activation l2 loss: 0.0014, total loss: 0.1988\n",
      "Epoch [2/200], Step [3/6], decoder loss: 0.1947, encoder1_activation l2 loss: 0.0013, total loss: 0.1959\n",
      "Epoch [2/200], Step [4/6], decoder loss: 0.1928, encoder1_activation l2 loss: 0.0012, total loss: 0.1941\n",
      "Epoch [2/200], Step [5/6], decoder loss: 0.1913, encoder1_activation l2 loss: 0.0011, total loss: 0.1924\n",
      "Epoch [2/200], Step [6/6], decoder loss: 0.1883, encoder1_activation l2 loss: 0.0006, total loss: 0.1889\n",
      "Epoch [2/200] (avg 6 steps), decoder loss: 0.1940, total loss: 0.1952\n",
      "Epoch [3/200], Step [1/6], decoder loss: 0.1880, encoder1_activation l2 loss: 0.0009, total loss: 0.1889\n",
      "Epoch [3/200], Step [2/6], decoder loss: 0.1857, encoder1_activation l2 loss: 0.0008, total loss: 0.1865\n",
      "Epoch [3/200], Step [3/6], decoder loss: 0.1827, encoder1_activation l2 loss: 0.0008, total loss: 0.1834\n",
      "Epoch [3/200], Step [4/6], decoder loss: 0.1814, encoder1_activation l2 loss: 0.0006, total loss: 0.1820\n",
      "Epoch [3/200], Step [5/6], decoder loss: 0.1794, encoder1_activation l2 loss: 0.0006, total loss: 0.1800\n",
      "Epoch [3/200], Step [6/6], decoder loss: 0.1786, encoder1_activation l2 loss: 0.0003, total loss: 0.1789\n",
      "Epoch [3/200] (avg 6 steps), decoder loss: 0.1826, total loss: 0.1833\n",
      "Epoch [4/200], Step [1/6], decoder loss: 0.1765, encoder1_activation l2 loss: 0.0005, total loss: 0.1770\n",
      "Epoch [4/200], Step [2/6], decoder loss: 0.1740, encoder1_activation l2 loss: 0.0005, total loss: 0.1745\n",
      "Epoch [4/200], Step [3/6], decoder loss: 0.1727, encoder1_activation l2 loss: 0.0004, total loss: 0.1732\n",
      "Epoch [4/200], Step [4/6], decoder loss: 0.1705, encoder1_activation l2 loss: 0.0004, total loss: 0.1709\n",
      "Epoch [4/200], Step [5/6], decoder loss: 0.1689, encoder1_activation l2 loss: 0.0004, total loss: 0.1693\n",
      "Epoch [4/200], Step [6/6], decoder loss: 0.1666, encoder1_activation l2 loss: 0.0002, total loss: 0.1668\n",
      "Epoch [4/200] (avg 6 steps), decoder loss: 0.1715, total loss: 0.1720\n",
      "Epoch [5/200], Step [1/6], decoder loss: 0.1650, encoder1_activation l2 loss: 0.0004, total loss: 0.1654\n",
      "Epoch [5/200], Step [2/6], decoder loss: 0.1627, encoder1_activation l2 loss: 0.0003, total loss: 0.1630\n",
      "Epoch [5/200], Step [3/6], decoder loss: 0.1628, encoder1_activation l2 loss: 0.0003, total loss: 0.1631\n",
      "Epoch [5/200], Step [4/6], decoder loss: 0.1602, encoder1_activation l2 loss: 0.0002, total loss: 0.1605\n",
      "Epoch [5/200], Step [5/6], decoder loss: 0.1579, encoder1_activation l2 loss: 0.0002, total loss: 0.1581\n",
      "Epoch [5/200], Step [6/6], decoder loss: 0.1570, encoder1_activation l2 loss: 0.0001, total loss: 0.1572\n",
      "Epoch [5/200] (avg 6 steps), decoder loss: 0.1609, total loss: 0.1612\n",
      "Epoch [6/200], Step [1/6], decoder loss: 0.1546, encoder1_activation l2 loss: 0.0002, total loss: 0.1548\n",
      "Epoch [6/200], Step [2/6], decoder loss: 0.1534, encoder1_activation l2 loss: 0.0002, total loss: 0.1536\n",
      "Epoch [6/200], Step [3/6], decoder loss: 0.1519, encoder1_activation l2 loss: 0.0002, total loss: 0.1521\n",
      "Epoch [6/200], Step [4/6], decoder loss: 0.1488, encoder1_activation l2 loss: 0.0001, total loss: 0.1489\n",
      "Epoch [6/200], Step [5/6], decoder loss: 0.1487, encoder1_activation l2 loss: 0.0002, total loss: 0.1488\n",
      "Epoch [6/200], Step [6/6], decoder loss: 0.1463, encoder1_activation l2 loss: 0.0001, total loss: 0.1463\n",
      "Epoch [6/200] (avg 6 steps), decoder loss: 0.1506, total loss: 0.1508\n",
      "Epoch [7/200], Step [1/6], decoder loss: 0.1452, encoder1_activation l2 loss: 0.0002, total loss: 0.1453\n",
      "Epoch [7/200], Step [2/6], decoder loss: 0.1428, encoder1_activation l2 loss: 0.0001, total loss: 0.1428\n",
      "Epoch [7/200], Step [3/6], decoder loss: 0.1416, encoder1_activation l2 loss: 0.0001, total loss: 0.1417\n",
      "Epoch [7/200], Step [4/6], decoder loss: 0.1397, encoder1_activation l2 loss: 0.0001, total loss: 0.1398\n",
      "Epoch [7/200], Step [5/6], decoder loss: 0.1385, encoder1_activation l2 loss: 0.0001, total loss: 0.1386\n",
      "Epoch [7/200], Step [6/6], decoder loss: 0.1360, encoder1_activation l2 loss: 0.0001, total loss: 0.1360\n",
      "Epoch [7/200] (avg 6 steps), decoder loss: 0.1406, total loss: 0.1407\n",
      "Epoch [8/200], Step [1/6], decoder loss: 0.1355, encoder1_activation l2 loss: 0.0001, total loss: 0.1356\n",
      "Epoch [8/200], Step [2/6], decoder loss: 0.1333, encoder1_activation l2 loss: 0.0001, total loss: 0.1334\n",
      "Epoch [8/200], Step [3/6], decoder loss: 0.1315, encoder1_activation l2 loss: 0.0001, total loss: 0.1316\n",
      "Epoch [8/200], Step [4/6], decoder loss: 0.1306, encoder1_activation l2 loss: 0.0001, total loss: 0.1307\n",
      "Epoch [8/200], Step [5/6], decoder loss: 0.1282, encoder1_activation l2 loss: 0.0001, total loss: 0.1282\n",
      "Epoch [8/200], Step [6/6], decoder loss: 0.1273, encoder1_activation l2 loss: 0.0000, total loss: 0.1273\n",
      "Epoch [8/200] (avg 6 steps), decoder loss: 0.1311, total loss: 0.1311\n",
      "Epoch [9/200], Step [1/6], decoder loss: 0.1253, encoder1_activation l2 loss: 0.0001, total loss: 0.1254\n",
      "Epoch [9/200], Step [2/6], decoder loss: 0.1242, encoder1_activation l2 loss: 0.0000, total loss: 0.1242\n",
      "Epoch [9/200], Step [3/6], decoder loss: 0.1218, encoder1_activation l2 loss: 0.0001, total loss: 0.1218\n",
      "Epoch [9/200], Step [4/6], decoder loss: 0.1217, encoder1_activation l2 loss: 0.0001, total loss: 0.1218\n",
      "Epoch [9/200], Step [5/6], decoder loss: 0.1196, encoder1_activation l2 loss: 0.0001, total loss: 0.1197\n",
      "Epoch [9/200], Step [6/6], decoder loss: 0.1186, encoder1_activation l2 loss: 0.0000, total loss: 0.1187\n",
      "Epoch [9/200] (avg 6 steps), decoder loss: 0.1219, total loss: 0.1219\n",
      "Epoch [10/200], Step [1/6], decoder loss: 0.1179, encoder1_activation l2 loss: 0.0001, total loss: 0.1179\n",
      "Epoch [10/200], Step [2/6], decoder loss: 0.1162, encoder1_activation l2 loss: 0.0001, total loss: 0.1163\n",
      "Epoch [10/200], Step [3/6], decoder loss: 0.1130, encoder1_activation l2 loss: 0.0000, total loss: 0.1130\n",
      "Epoch [10/200], Step [4/6], decoder loss: 0.1121, encoder1_activation l2 loss: 0.0000, total loss: 0.1122\n",
      "Epoch [10/200], Step [5/6], decoder loss: 0.1102, encoder1_activation l2 loss: 0.0000, total loss: 0.1102\n",
      "Epoch [10/200], Step [6/6], decoder loss: 0.1086, encoder1_activation l2 loss: 0.0000, total loss: 0.1086\n",
      "Epoch [10/200] (avg 6 steps), decoder loss: 0.1130, total loss: 0.1130\n",
      "Epoch [11/200], Step [1/6], decoder loss: 0.1085, encoder1_activation l2 loss: 0.0000, total loss: 0.1085\n",
      "Epoch [11/200], Step [2/6], decoder loss: 0.1069, encoder1_activation l2 loss: 0.0000, total loss: 0.1070\n",
      "Epoch [11/200], Step [3/6], decoder loss: 0.1059, encoder1_activation l2 loss: 0.0000, total loss: 0.1060\n",
      "Epoch [11/200], Step [4/6], decoder loss: 0.1033, encoder1_activation l2 loss: 0.0000, total loss: 0.1033\n",
      "Epoch [11/200], Step [5/6], decoder loss: 0.1017, encoder1_activation l2 loss: 0.0000, total loss: 0.1017\n",
      "Epoch [11/200], Step [6/6], decoder loss: 0.1023, encoder1_activation l2 loss: 0.0000, total loss: 0.1023\n",
      "Epoch [11/200] (avg 6 steps), decoder loss: 0.1048, total loss: 0.1048\n",
      "Epoch [12/200], Step [1/6], decoder loss: 0.1006, encoder1_activation l2 loss: 0.0000, total loss: 0.1006\n",
      "Epoch [12/200], Step [2/6], decoder loss: 0.0987, encoder1_activation l2 loss: 0.0000, total loss: 0.0987\n",
      "Epoch [12/200], Step [3/6], decoder loss: 0.0968, encoder1_activation l2 loss: 0.0000, total loss: 0.0969\n",
      "Epoch [12/200], Step [4/6], decoder loss: 0.0965, encoder1_activation l2 loss: 0.0000, total loss: 0.0965\n",
      "Epoch [12/200], Step [5/6], decoder loss: 0.0945, encoder1_activation l2 loss: 0.0000, total loss: 0.0946\n",
      "Epoch [12/200], Step [6/6], decoder loss: 0.0942, encoder1_activation l2 loss: 0.0000, total loss: 0.0942\n",
      "Epoch [12/200] (avg 6 steps), decoder loss: 0.0969, total loss: 0.0969\n",
      "Epoch [13/200], Step [1/6], decoder loss: 0.0929, encoder1_activation l2 loss: 0.0000, total loss: 0.0929\n",
      "Epoch [13/200], Step [2/6], decoder loss: 0.0909, encoder1_activation l2 loss: 0.0000, total loss: 0.0910\n",
      "Epoch [13/200], Step [3/6], decoder loss: 0.0882, encoder1_activation l2 loss: 0.0000, total loss: 0.0882\n",
      "Epoch [13/200], Step [4/6], decoder loss: 0.0887, encoder1_activation l2 loss: 0.0000, total loss: 0.0887\n",
      "Epoch [13/200], Step [5/6], decoder loss: 0.0888, encoder1_activation l2 loss: 0.0000, total loss: 0.0888\n",
      "Epoch [13/200], Step [6/6], decoder loss: 0.0877, encoder1_activation l2 loss: 0.0000, total loss: 0.0878\n",
      "Epoch [13/200] (avg 6 steps), decoder loss: 0.0896, total loss: 0.0896\n",
      "Epoch [14/200], Step [1/6], decoder loss: 0.0854, encoder1_activation l2 loss: 0.0000, total loss: 0.0855\n",
      "Epoch [14/200], Step [2/6], decoder loss: 0.0840, encoder1_activation l2 loss: 0.0000, total loss: 0.0840\n",
      "Epoch [14/200], Step [3/6], decoder loss: 0.0834, encoder1_activation l2 loss: 0.0000, total loss: 0.0835\n",
      "Epoch [14/200], Step [4/6], decoder loss: 0.0814, encoder1_activation l2 loss: 0.0000, total loss: 0.0814\n",
      "Epoch [14/200], Step [5/6], decoder loss: 0.0812, encoder1_activation l2 loss: 0.0000, total loss: 0.0812\n",
      "Epoch [14/200], Step [6/6], decoder loss: 0.0805, encoder1_activation l2 loss: 0.0000, total loss: 0.0805\n",
      "Epoch [14/200] (avg 6 steps), decoder loss: 0.0827, total loss: 0.0827\n",
      "Epoch [15/200], Step [1/6], decoder loss: 0.0785, encoder1_activation l2 loss: 0.0000, total loss: 0.0785\n",
      "Epoch [15/200], Step [2/6], decoder loss: 0.0789, encoder1_activation l2 loss: 0.0000, total loss: 0.0790\n",
      "Epoch [15/200], Step [3/6], decoder loss: 0.0781, encoder1_activation l2 loss: 0.0000, total loss: 0.0781\n",
      "Epoch [15/200], Step [4/6], decoder loss: 0.0748, encoder1_activation l2 loss: 0.0000, total loss: 0.0748\n",
      "Epoch [15/200], Step [5/6], decoder loss: 0.0747, encoder1_activation l2 loss: 0.0000, total loss: 0.0747\n",
      "Epoch [15/200], Step [6/6], decoder loss: 0.0717, encoder1_activation l2 loss: 0.0000, total loss: 0.0717\n",
      "Epoch [15/200] (avg 6 steps), decoder loss: 0.0761, total loss: 0.0761\n",
      "Epoch [16/200], Step [1/6], decoder loss: 0.0734, encoder1_activation l2 loss: 0.0000, total loss: 0.0734\n",
      "Epoch [16/200], Step [2/6], decoder loss: 0.0714, encoder1_activation l2 loss: 0.0000, total loss: 0.0715\n",
      "Epoch [16/200], Step [3/6], decoder loss: 0.0703, encoder1_activation l2 loss: 0.0000, total loss: 0.0703\n",
      "Epoch [16/200], Step [4/6], decoder loss: 0.0707, encoder1_activation l2 loss: 0.0000, total loss: 0.0707\n",
      "Epoch [16/200], Step [5/6], decoder loss: 0.0682, encoder1_activation l2 loss: 0.0000, total loss: 0.0682\n",
      "Epoch [16/200], Step [6/6], decoder loss: 0.0685, encoder1_activation l2 loss: 0.0000, total loss: 0.0685\n",
      "Epoch [16/200] (avg 6 steps), decoder loss: 0.0704, total loss: 0.0704\n",
      "Epoch [17/200], Step [1/6], decoder loss: 0.0670, encoder1_activation l2 loss: 0.0000, total loss: 0.0670\n",
      "Epoch [17/200], Step [2/6], decoder loss: 0.0669, encoder1_activation l2 loss: 0.0000, total loss: 0.0669\n",
      "Epoch [17/200], Step [3/6], decoder loss: 0.0648, encoder1_activation l2 loss: 0.0000, total loss: 0.0648\n",
      "Epoch [17/200], Step [4/6], decoder loss: 0.0648, encoder1_activation l2 loss: 0.0000, total loss: 0.0648\n",
      "Epoch [17/200], Step [5/6], decoder loss: 0.0638, encoder1_activation l2 loss: 0.0000, total loss: 0.0638\n",
      "Epoch [17/200], Step [6/6], decoder loss: 0.0628, encoder1_activation l2 loss: 0.0000, total loss: 0.0628\n",
      "Epoch [17/200] (avg 6 steps), decoder loss: 0.0650, total loss: 0.0650\n",
      "Epoch [18/200], Step [1/6], decoder loss: 0.0627, encoder1_activation l2 loss: 0.0000, total loss: 0.0627\n",
      "Epoch [18/200], Step [2/6], decoder loss: 0.0607, encoder1_activation l2 loss: 0.0000, total loss: 0.0608\n",
      "Epoch [18/200], Step [3/6], decoder loss: 0.0596, encoder1_activation l2 loss: 0.0000, total loss: 0.0596\n",
      "Epoch [18/200], Step [4/6], decoder loss: 0.0596, encoder1_activation l2 loss: 0.0000, total loss: 0.0596\n",
      "Epoch [18/200], Step [5/6], decoder loss: 0.0600, encoder1_activation l2 loss: 0.0000, total loss: 0.0600\n",
      "Epoch [18/200], Step [6/6], decoder loss: 0.0583, encoder1_activation l2 loss: 0.0000, total loss: 0.0583\n",
      "Epoch [18/200] (avg 6 steps), decoder loss: 0.0601, total loss: 0.0602\n",
      "Epoch [19/200], Step [1/6], decoder loss: 0.0581, encoder1_activation l2 loss: 0.0000, total loss: 0.0582\n",
      "Epoch [19/200], Step [2/6], decoder loss: 0.0568, encoder1_activation l2 loss: 0.0000, total loss: 0.0568\n",
      "Epoch [19/200], Step [3/6], decoder loss: 0.0559, encoder1_activation l2 loss: 0.0000, total loss: 0.0559\n",
      "Epoch [19/200], Step [4/6], decoder loss: 0.0554, encoder1_activation l2 loss: 0.0000, total loss: 0.0554\n",
      "Epoch [19/200], Step [5/6], decoder loss: 0.0540, encoder1_activation l2 loss: 0.0000, total loss: 0.0540\n",
      "Epoch [19/200], Step [6/6], decoder loss: 0.0542, encoder1_activation l2 loss: 0.0000, total loss: 0.0542\n",
      "Epoch [19/200] (avg 6 steps), decoder loss: 0.0557, total loss: 0.0557\n",
      "Epoch [20/200], Step [1/6], decoder loss: 0.0532, encoder1_activation l2 loss: 0.0000, total loss: 0.0532\n",
      "Epoch [20/200], Step [2/6], decoder loss: 0.0524, encoder1_activation l2 loss: 0.0000, total loss: 0.0524\n",
      "Epoch [20/200], Step [3/6], decoder loss: 0.0516, encoder1_activation l2 loss: 0.0000, total loss: 0.0517\n",
      "Epoch [20/200], Step [4/6], decoder loss: 0.0517, encoder1_activation l2 loss: 0.0000, total loss: 0.0517\n",
      "Epoch [20/200], Step [5/6], decoder loss: 0.0507, encoder1_activation l2 loss: 0.0000, total loss: 0.0507\n",
      "Epoch [20/200], Step [6/6], decoder loss: 0.0510, encoder1_activation l2 loss: 0.0000, total loss: 0.0510\n",
      "Epoch [20/200] (avg 6 steps), decoder loss: 0.0517, total loss: 0.0518\n",
      "Epoch [21/200], Step [1/6], decoder loss: 0.0483, encoder1_activation l2 loss: 0.0000, total loss: 0.0483\n",
      "Epoch [21/200], Step [2/6], decoder loss: 0.0480, encoder1_activation l2 loss: 0.0000, total loss: 0.0480\n",
      "Epoch [21/200], Step [3/6], decoder loss: 0.0485, encoder1_activation l2 loss: 0.0000, total loss: 0.0485\n",
      "Epoch [21/200], Step [4/6], decoder loss: 0.0493, encoder1_activation l2 loss: 0.0000, total loss: 0.0494\n",
      "Epoch [21/200], Step [5/6], decoder loss: 0.0470, encoder1_activation l2 loss: 0.0000, total loss: 0.0470\n",
      "Epoch [21/200], Step [6/6], decoder loss: 0.0479, encoder1_activation l2 loss: 0.0000, total loss: 0.0479\n",
      "Epoch [21/200] (avg 6 steps), decoder loss: 0.0482, total loss: 0.0482\n",
      "Epoch [22/200], Step [1/6], decoder loss: 0.0462, encoder1_activation l2 loss: 0.0000, total loss: 0.0462\n",
      "Epoch [22/200], Step [2/6], decoder loss: 0.0457, encoder1_activation l2 loss: 0.0000, total loss: 0.0458\n",
      "Epoch [22/200], Step [3/6], decoder loss: 0.0461, encoder1_activation l2 loss: 0.0000, total loss: 0.0461\n",
      "Epoch [22/200], Step [4/6], decoder loss: 0.0444, encoder1_activation l2 loss: 0.0000, total loss: 0.0444\n",
      "Epoch [22/200], Step [5/6], decoder loss: 0.0433, encoder1_activation l2 loss: 0.0000, total loss: 0.0433\n",
      "Epoch [22/200], Step [6/6], decoder loss: 0.0428, encoder1_activation l2 loss: 0.0000, total loss: 0.0428\n",
      "Epoch [22/200] (avg 6 steps), decoder loss: 0.0448, total loss: 0.0448\n",
      "Epoch [23/200], Step [1/6], decoder loss: 0.0436, encoder1_activation l2 loss: 0.0000, total loss: 0.0436\n",
      "Epoch [23/200], Step [2/6], decoder loss: 0.0428, encoder1_activation l2 loss: 0.0000, total loss: 0.0428\n",
      "Epoch [23/200], Step [3/6], decoder loss: 0.0420, encoder1_activation l2 loss: 0.0000, total loss: 0.0420\n",
      "Epoch [23/200], Step [4/6], decoder loss: 0.0413, encoder1_activation l2 loss: 0.0000, total loss: 0.0413\n",
      "Epoch [23/200], Step [5/6], decoder loss: 0.0408, encoder1_activation l2 loss: 0.0000, total loss: 0.0408\n",
      "Epoch [23/200], Step [6/6], decoder loss: 0.0409, encoder1_activation l2 loss: 0.0000, total loss: 0.0409\n",
      "Epoch [23/200] (avg 6 steps), decoder loss: 0.0419, total loss: 0.0419\n",
      "Epoch [24/200], Step [1/6], decoder loss: 0.0393, encoder1_activation l2 loss: 0.0000, total loss: 0.0393\n",
      "Epoch [24/200], Step [2/6], decoder loss: 0.0400, encoder1_activation l2 loss: 0.0000, total loss: 0.0400\n",
      "Epoch [24/200], Step [3/6], decoder loss: 0.0394, encoder1_activation l2 loss: 0.0000, total loss: 0.0394\n",
      "Epoch [24/200], Step [4/6], decoder loss: 0.0386, encoder1_activation l2 loss: 0.0000, total loss: 0.0386\n",
      "Epoch [24/200], Step [5/6], decoder loss: 0.0393, encoder1_activation l2 loss: 0.0000, total loss: 0.0393\n",
      "Epoch [24/200], Step [6/6], decoder loss: 0.0394, encoder1_activation l2 loss: 0.0000, total loss: 0.0394\n",
      "Epoch [24/200] (avg 6 steps), decoder loss: 0.0393, total loss: 0.0393\n",
      "Epoch [25/200], Step [1/6], decoder loss: 0.0379, encoder1_activation l2 loss: 0.0000, total loss: 0.0379\n",
      "Epoch [25/200], Step [2/6], decoder loss: 0.0378, encoder1_activation l2 loss: 0.0000, total loss: 0.0378\n",
      "Epoch [25/200], Step [3/6], decoder loss: 0.0359, encoder1_activation l2 loss: 0.0000, total loss: 0.0359\n",
      "Epoch [25/200], Step [4/6], decoder loss: 0.0364, encoder1_activation l2 loss: 0.0000, total loss: 0.0364\n",
      "Epoch [25/200], Step [5/6], decoder loss: 0.0376, encoder1_activation l2 loss: 0.0000, total loss: 0.0376\n",
      "Epoch [25/200], Step [6/6], decoder loss: 0.0359, encoder1_activation l2 loss: 0.0000, total loss: 0.0359\n",
      "Epoch [25/200] (avg 6 steps), decoder loss: 0.0369, total loss: 0.0369\n",
      "Epoch [26/200], Step [1/6], decoder loss: 0.0352, encoder1_activation l2 loss: 0.0000, total loss: 0.0352\n",
      "Epoch [26/200], Step [2/6], decoder loss: 0.0360, encoder1_activation l2 loss: 0.0000, total loss: 0.0360\n",
      "Epoch [26/200], Step [3/6], decoder loss: 0.0352, encoder1_activation l2 loss: 0.0000, total loss: 0.0352\n",
      "Epoch [26/200], Step [4/6], decoder loss: 0.0344, encoder1_activation l2 loss: 0.0000, total loss: 0.0345\n",
      "Epoch [26/200], Step [5/6], decoder loss: 0.0340, encoder1_activation l2 loss: 0.0000, total loss: 0.0340\n",
      "Epoch [26/200], Step [6/6], decoder loss: 0.0339, encoder1_activation l2 loss: 0.0000, total loss: 0.0339\n",
      "Epoch [26/200] (avg 6 steps), decoder loss: 0.0348, total loss: 0.0348\n",
      "Epoch [27/200], Step [1/6], decoder loss: 0.0352, encoder1_activation l2 loss: 0.0000, total loss: 0.0352\n",
      "Epoch [27/200], Step [2/6], decoder loss: 0.0329, encoder1_activation l2 loss: 0.0000, total loss: 0.0329\n",
      "Epoch [27/200], Step [3/6], decoder loss: 0.0332, encoder1_activation l2 loss: 0.0000, total loss: 0.0332\n",
      "Epoch [27/200], Step [4/6], decoder loss: 0.0329, encoder1_activation l2 loss: 0.0000, total loss: 0.0329\n",
      "Epoch [27/200], Step [5/6], decoder loss: 0.0326, encoder1_activation l2 loss: 0.0000, total loss: 0.0326\n",
      "Epoch [27/200], Step [6/6], decoder loss: 0.0294, encoder1_activation l2 loss: 0.0000, total loss: 0.0294\n",
      "Epoch [27/200] (avg 6 steps), decoder loss: 0.0327, total loss: 0.0327\n",
      "Epoch [28/200], Step [1/6], decoder loss: 0.0326, encoder1_activation l2 loss: 0.0000, total loss: 0.0326\n",
      "Epoch [28/200], Step [2/6], decoder loss: 0.0316, encoder1_activation l2 loss: 0.0000, total loss: 0.0317\n",
      "Epoch [28/200], Step [3/6], decoder loss: 0.0311, encoder1_activation l2 loss: 0.0000, total loss: 0.0311\n",
      "Epoch [28/200], Step [4/6], decoder loss: 0.0310, encoder1_activation l2 loss: 0.0000, total loss: 0.0310\n",
      "Epoch [28/200], Step [5/6], decoder loss: 0.0304, encoder1_activation l2 loss: 0.0000, total loss: 0.0304\n",
      "Epoch [28/200], Step [6/6], decoder loss: 0.0305, encoder1_activation l2 loss: 0.0000, total loss: 0.0305\n",
      "Epoch [28/200] (avg 6 steps), decoder loss: 0.0312, total loss: 0.0312\n",
      "Epoch [29/200], Step [1/6], decoder loss: 0.0307, encoder1_activation l2 loss: 0.0000, total loss: 0.0307\n",
      "Epoch [29/200], Step [2/6], decoder loss: 0.0295, encoder1_activation l2 loss: 0.0000, total loss: 0.0295\n",
      "Epoch [29/200], Step [3/6], decoder loss: 0.0308, encoder1_activation l2 loss: 0.0000, total loss: 0.0308\n",
      "Epoch [29/200], Step [4/6], decoder loss: 0.0293, encoder1_activation l2 loss: 0.0000, total loss: 0.0293\n",
      "Epoch [29/200], Step [5/6], decoder loss: 0.0290, encoder1_activation l2 loss: 0.0000, total loss: 0.0290\n",
      "Epoch [29/200], Step [6/6], decoder loss: 0.0288, encoder1_activation l2 loss: 0.0000, total loss: 0.0288\n",
      "Epoch [29/200] (avg 6 steps), decoder loss: 0.0297, total loss: 0.0297\n",
      "Epoch [30/200], Step [1/6], decoder loss: 0.0287, encoder1_activation l2 loss: 0.0000, total loss: 0.0287\n",
      "Epoch [30/200], Step [2/6], decoder loss: 0.0285, encoder1_activation l2 loss: 0.0000, total loss: 0.0285\n",
      "Epoch [30/200], Step [3/6], decoder loss: 0.0290, encoder1_activation l2 loss: 0.0000, total loss: 0.0290\n",
      "Epoch [30/200], Step [4/6], decoder loss: 0.0279, encoder1_activation l2 loss: 0.0000, total loss: 0.0279\n",
      "Epoch [30/200], Step [5/6], decoder loss: 0.0288, encoder1_activation l2 loss: 0.0000, total loss: 0.0288\n",
      "Epoch [30/200], Step [6/6], decoder loss: 0.0268, encoder1_activation l2 loss: 0.0000, total loss: 0.0268\n",
      "Epoch [30/200] (avg 6 steps), decoder loss: 0.0283, total loss: 0.0283\n",
      "Epoch [31/200], Step [1/6], decoder loss: 0.0270, encoder1_activation l2 loss: 0.0000, total loss: 0.0270\n",
      "Epoch [31/200], Step [2/6], decoder loss: 0.0270, encoder1_activation l2 loss: 0.0000, total loss: 0.0270\n",
      "Epoch [31/200], Step [3/6], decoder loss: 0.0287, encoder1_activation l2 loss: 0.0000, total loss: 0.0288\n",
      "Epoch [31/200], Step [4/6], decoder loss: 0.0276, encoder1_activation l2 loss: 0.0000, total loss: 0.0276\n",
      "Epoch [31/200], Step [5/6], decoder loss: 0.0268, encoder1_activation l2 loss: 0.0000, total loss: 0.0268\n",
      "Epoch [31/200], Step [6/6], decoder loss: 0.0246, encoder1_activation l2 loss: 0.0000, total loss: 0.0246\n",
      "Epoch [31/200] (avg 6 steps), decoder loss: 0.0269, total loss: 0.0270\n",
      "Epoch [32/200], Step [1/6], decoder loss: 0.0260, encoder1_activation l2 loss: 0.0000, total loss: 0.0260\n",
      "Epoch [32/200], Step [2/6], decoder loss: 0.0262, encoder1_activation l2 loss: 0.0000, total loss: 0.0262\n",
      "Epoch [32/200], Step [3/6], decoder loss: 0.0269, encoder1_activation l2 loss: 0.0000, total loss: 0.0269\n",
      "Epoch [32/200], Step [4/6], decoder loss: 0.0266, encoder1_activation l2 loss: 0.0000, total loss: 0.0266\n",
      "Epoch [32/200], Step [5/6], decoder loss: 0.0250, encoder1_activation l2 loss: 0.0000, total loss: 0.0250\n",
      "Epoch [32/200], Step [6/6], decoder loss: 0.0251, encoder1_activation l2 loss: 0.0000, total loss: 0.0251\n",
      "Epoch [32/200] (avg 6 steps), decoder loss: 0.0259, total loss: 0.0260\n",
      "Epoch [33/200], Step [1/6], decoder loss: 0.0254, encoder1_activation l2 loss: 0.0000, total loss: 0.0254\n",
      "Epoch [33/200], Step [2/6], decoder loss: 0.0264, encoder1_activation l2 loss: 0.0000, total loss: 0.0264\n",
      "Epoch [33/200], Step [3/6], decoder loss: 0.0247, encoder1_activation l2 loss: 0.0000, total loss: 0.0247\n",
      "Epoch [33/200], Step [4/6], decoder loss: 0.0244, encoder1_activation l2 loss: 0.0000, total loss: 0.0244\n",
      "Epoch [33/200], Step [5/6], decoder loss: 0.0255, encoder1_activation l2 loss: 0.0000, total loss: 0.0255\n",
      "Epoch [33/200], Step [6/6], decoder loss: 0.0227, encoder1_activation l2 loss: 0.0000, total loss: 0.0227\n",
      "Epoch [33/200] (avg 6 steps), decoder loss: 0.0248, total loss: 0.0249\n",
      "Epoch [34/200], Step [1/6], decoder loss: 0.0242, encoder1_activation l2 loss: 0.0000, total loss: 0.0242\n",
      "Epoch [34/200], Step [2/6], decoder loss: 0.0234, encoder1_activation l2 loss: 0.0000, total loss: 0.0234\n",
      "Epoch [34/200], Step [3/6], decoder loss: 0.0234, encoder1_activation l2 loss: 0.0000, total loss: 0.0234\n",
      "Epoch [34/200], Step [4/6], decoder loss: 0.0247, encoder1_activation l2 loss: 0.0000, total loss: 0.0247\n",
      "Epoch [34/200], Step [5/6], decoder loss: 0.0253, encoder1_activation l2 loss: 0.0000, total loss: 0.0253\n",
      "Epoch [34/200], Step [6/6], decoder loss: 0.0233, encoder1_activation l2 loss: 0.0000, total loss: 0.0233\n",
      "Epoch [34/200] (avg 6 steps), decoder loss: 0.0240, total loss: 0.0240\n",
      "Epoch [35/200], Step [1/6], decoder loss: 0.0234, encoder1_activation l2 loss: 0.0000, total loss: 0.0234\n",
      "Epoch [35/200], Step [2/6], decoder loss: 0.0249, encoder1_activation l2 loss: 0.0000, total loss: 0.0249\n",
      "Epoch [35/200], Step [3/6], decoder loss: 0.0223, encoder1_activation l2 loss: 0.0000, total loss: 0.0223\n",
      "Epoch [35/200], Step [4/6], decoder loss: 0.0231, encoder1_activation l2 loss: 0.0000, total loss: 0.0231\n",
      "Epoch [35/200], Step [5/6], decoder loss: 0.0235, encoder1_activation l2 loss: 0.0000, total loss: 0.0235\n",
      "Epoch [35/200], Step [6/6], decoder loss: 0.0218, encoder1_activation l2 loss: 0.0000, total loss: 0.0218\n",
      "Epoch [35/200] (avg 6 steps), decoder loss: 0.0232, total loss: 0.0232\n",
      "Epoch [36/200], Step [1/6], decoder loss: 0.0235, encoder1_activation l2 loss: 0.0000, total loss: 0.0235\n",
      "Epoch [36/200], Step [2/6], decoder loss: 0.0227, encoder1_activation l2 loss: 0.0000, total loss: 0.0227\n",
      "Epoch [36/200], Step [3/6], decoder loss: 0.0239, encoder1_activation l2 loss: 0.0000, total loss: 0.0239\n",
      "Epoch [36/200], Step [4/6], decoder loss: 0.0220, encoder1_activation l2 loss: 0.0000, total loss: 0.0220\n",
      "Epoch [36/200], Step [5/6], decoder loss: 0.0214, encoder1_activation l2 loss: 0.0000, total loss: 0.0215\n",
      "Epoch [36/200], Step [6/6], decoder loss: 0.0210, encoder1_activation l2 loss: 0.0000, total loss: 0.0210\n",
      "Epoch [36/200] (avg 6 steps), decoder loss: 0.0224, total loss: 0.0224\n",
      "Epoch [37/200], Step [1/6], decoder loss: 0.0223, encoder1_activation l2 loss: 0.0000, total loss: 0.0223\n",
      "Epoch [37/200], Step [2/6], decoder loss: 0.0230, encoder1_activation l2 loss: 0.0000, total loss: 0.0230\n",
      "Epoch [37/200], Step [3/6], decoder loss: 0.0218, encoder1_activation l2 loss: 0.0000, total loss: 0.0218\n",
      "Epoch [37/200], Step [4/6], decoder loss: 0.0223, encoder1_activation l2 loss: 0.0000, total loss: 0.0223\n",
      "Epoch [37/200], Step [5/6], decoder loss: 0.0202, encoder1_activation l2 loss: 0.0000, total loss: 0.0202\n",
      "Epoch [37/200], Step [6/6], decoder loss: 0.0214, encoder1_activation l2 loss: 0.0000, total loss: 0.0214\n",
      "Epoch [37/200] (avg 6 steps), decoder loss: 0.0218, total loss: 0.0218\n",
      "Epoch [38/200], Step [1/6], decoder loss: 0.0204, encoder1_activation l2 loss: 0.0000, total loss: 0.0204\n",
      "Epoch [38/200], Step [2/6], decoder loss: 0.0212, encoder1_activation l2 loss: 0.0000, total loss: 0.0212\n",
      "Epoch [38/200], Step [3/6], decoder loss: 0.0208, encoder1_activation l2 loss: 0.0000, total loss: 0.0208\n",
      "Epoch [38/200], Step [4/6], decoder loss: 0.0217, encoder1_activation l2 loss: 0.0000, total loss: 0.0218\n",
      "Epoch [38/200], Step [5/6], decoder loss: 0.0214, encoder1_activation l2 loss: 0.0000, total loss: 0.0214\n",
      "Epoch [38/200], Step [6/6], decoder loss: 0.0221, encoder1_activation l2 loss: 0.0000, total loss: 0.0221\n",
      "Epoch [38/200] (avg 6 steps), decoder loss: 0.0213, total loss: 0.0213\n",
      "Epoch [39/200], Step [1/6], decoder loss: 0.0220, encoder1_activation l2 loss: 0.0000, total loss: 0.0220\n",
      "Epoch [39/200], Step [2/6], decoder loss: 0.0205, encoder1_activation l2 loss: 0.0000, total loss: 0.0205\n",
      "Epoch [39/200], Step [3/6], decoder loss: 0.0202, encoder1_activation l2 loss: 0.0000, total loss: 0.0203\n",
      "Epoch [39/200], Step [4/6], decoder loss: 0.0207, encoder1_activation l2 loss: 0.0000, total loss: 0.0207\n",
      "Epoch [39/200], Step [5/6], decoder loss: 0.0210, encoder1_activation l2 loss: 0.0000, total loss: 0.0211\n",
      "Epoch [39/200], Step [6/6], decoder loss: 0.0184, encoder1_activation l2 loss: 0.0000, total loss: 0.0184\n",
      "Epoch [39/200] (avg 6 steps), decoder loss: 0.0205, total loss: 0.0205\n",
      "Epoch [40/200], Step [1/6], decoder loss: 0.0212, encoder1_activation l2 loss: 0.0000, total loss: 0.0212\n",
      "Epoch [40/200], Step [2/6], decoder loss: 0.0210, encoder1_activation l2 loss: 0.0000, total loss: 0.0210\n",
      "Epoch [40/200], Step [3/6], decoder loss: 0.0202, encoder1_activation l2 loss: 0.0000, total loss: 0.0202\n",
      "Epoch [40/200], Step [4/6], decoder loss: 0.0193, encoder1_activation l2 loss: 0.0000, total loss: 0.0193\n",
      "Epoch [40/200], Step [5/6], decoder loss: 0.0196, encoder1_activation l2 loss: 0.0000, total loss: 0.0196\n",
      "Epoch [40/200], Step [6/6], decoder loss: 0.0189, encoder1_activation l2 loss: 0.0000, total loss: 0.0189\n",
      "Epoch [40/200] (avg 6 steps), decoder loss: 0.0200, total loss: 0.0200\n",
      "Epoch [41/200], Step [1/6], decoder loss: 0.0199, encoder1_activation l2 loss: 0.0000, total loss: 0.0199\n",
      "Epoch [41/200], Step [2/6], decoder loss: 0.0191, encoder1_activation l2 loss: 0.0000, total loss: 0.0191\n",
      "Epoch [41/200], Step [3/6], decoder loss: 0.0200, encoder1_activation l2 loss: 0.0000, total loss: 0.0200\n",
      "Epoch [41/200], Step [4/6], decoder loss: 0.0204, encoder1_activation l2 loss: 0.0000, total loss: 0.0205\n",
      "Epoch [41/200], Step [5/6], decoder loss: 0.0190, encoder1_activation l2 loss: 0.0000, total loss: 0.0190\n",
      "Epoch [41/200], Step [6/6], decoder loss: 0.0193, encoder1_activation l2 loss: 0.0000, total loss: 0.0193\n",
      "Epoch [41/200] (avg 6 steps), decoder loss: 0.0196, total loss: 0.0196\n",
      "Epoch [42/200], Step [1/6], decoder loss: 0.0192, encoder1_activation l2 loss: 0.0000, total loss: 0.0192\n",
      "Epoch [42/200], Step [2/6], decoder loss: 0.0184, encoder1_activation l2 loss: 0.0000, total loss: 0.0184\n",
      "Epoch [42/200], Step [3/6], decoder loss: 0.0196, encoder1_activation l2 loss: 0.0000, total loss: 0.0196\n",
      "Epoch [42/200], Step [4/6], decoder loss: 0.0200, encoder1_activation l2 loss: 0.0000, total loss: 0.0201\n",
      "Epoch [42/200], Step [5/6], decoder loss: 0.0183, encoder1_activation l2 loss: 0.0000, total loss: 0.0183\n",
      "Epoch [42/200], Step [6/6], decoder loss: 0.0202, encoder1_activation l2 loss: 0.0000, total loss: 0.0202\n",
      "Epoch [42/200] (avg 6 steps), decoder loss: 0.0193, total loss: 0.0193\n",
      "Epoch [43/200], Step [1/6], decoder loss: 0.0177, encoder1_activation l2 loss: 0.0000, total loss: 0.0177\n",
      "Epoch [43/200], Step [2/6], decoder loss: 0.0185, encoder1_activation l2 loss: 0.0000, total loss: 0.0185\n",
      "Epoch [43/200], Step [3/6], decoder loss: 0.0209, encoder1_activation l2 loss: 0.0000, total loss: 0.0209\n",
      "Epoch [43/200], Step [4/6], decoder loss: 0.0186, encoder1_activation l2 loss: 0.0000, total loss: 0.0186\n",
      "Epoch [43/200], Step [5/6], decoder loss: 0.0187, encoder1_activation l2 loss: 0.0000, total loss: 0.0187\n",
      "Epoch [43/200], Step [6/6], decoder loss: 0.0183, encoder1_activation l2 loss: 0.0000, total loss: 0.0183\n",
      "Epoch [43/200] (avg 6 steps), decoder loss: 0.0188, total loss: 0.0188\n",
      "Epoch [44/200], Step [1/6], decoder loss: 0.0202, encoder1_activation l2 loss: 0.0000, total loss: 0.0202\n",
      "Epoch [44/200], Step [2/6], decoder loss: 0.0180, encoder1_activation l2 loss: 0.0000, total loss: 0.0180\n",
      "Epoch [44/200], Step [3/6], decoder loss: 0.0179, encoder1_activation l2 loss: 0.0000, total loss: 0.0179\n",
      "Epoch [44/200], Step [4/6], decoder loss: 0.0182, encoder1_activation l2 loss: 0.0000, total loss: 0.0182\n",
      "Epoch [44/200], Step [5/6], decoder loss: 0.0189, encoder1_activation l2 loss: 0.0000, total loss: 0.0189\n",
      "Epoch [44/200], Step [6/6], decoder loss: 0.0165, encoder1_activation l2 loss: 0.0000, total loss: 0.0165\n",
      "Epoch [44/200] (avg 6 steps), decoder loss: 0.0183, total loss: 0.0183\n",
      "Epoch [45/200], Step [1/6], decoder loss: 0.0185, encoder1_activation l2 loss: 0.0000, total loss: 0.0185\n",
      "Epoch [45/200], Step [2/6], decoder loss: 0.0174, encoder1_activation l2 loss: 0.0000, total loss: 0.0174\n",
      "Epoch [45/200], Step [3/6], decoder loss: 0.0182, encoder1_activation l2 loss: 0.0000, total loss: 0.0182\n",
      "Epoch [45/200], Step [4/6], decoder loss: 0.0189, encoder1_activation l2 loss: 0.0000, total loss: 0.0189\n",
      "Epoch [45/200], Step [5/6], decoder loss: 0.0183, encoder1_activation l2 loss: 0.0000, total loss: 0.0183\n",
      "Epoch [45/200], Step [6/6], decoder loss: 0.0165, encoder1_activation l2 loss: 0.0000, total loss: 0.0165\n",
      "Epoch [45/200] (avg 6 steps), decoder loss: 0.0180, total loss: 0.0180\n",
      "Epoch [46/200], Step [1/6], decoder loss: 0.0177, encoder1_activation l2 loss: 0.0000, total loss: 0.0177\n",
      "Epoch [46/200], Step [2/6], decoder loss: 0.0165, encoder1_activation l2 loss: 0.0000, total loss: 0.0165\n",
      "Epoch [46/200], Step [3/6], decoder loss: 0.0166, encoder1_activation l2 loss: 0.0000, total loss: 0.0166\n",
      "Epoch [46/200], Step [4/6], decoder loss: 0.0186, encoder1_activation l2 loss: 0.0000, total loss: 0.0186\n",
      "Epoch [46/200], Step [5/6], decoder loss: 0.0192, encoder1_activation l2 loss: 0.0000, total loss: 0.0192\n",
      "Epoch [46/200], Step [6/6], decoder loss: 0.0181, encoder1_activation l2 loss: 0.0000, total loss: 0.0181\n",
      "Epoch [46/200] (avg 6 steps), decoder loss: 0.0178, total loss: 0.0178\n",
      "Epoch [47/200], Step [1/6], decoder loss: 0.0171, encoder1_activation l2 loss: 0.0000, total loss: 0.0171\n",
      "Epoch [47/200], Step [2/6], decoder loss: 0.0179, encoder1_activation l2 loss: 0.0000, total loss: 0.0179\n",
      "Epoch [47/200], Step [3/6], decoder loss: 0.0182, encoder1_activation l2 loss: 0.0000, total loss: 0.0182\n",
      "Epoch [47/200], Step [4/6], decoder loss: 0.0165, encoder1_activation l2 loss: 0.0000, total loss: 0.0165\n",
      "Epoch [47/200], Step [5/6], decoder loss: 0.0186, encoder1_activation l2 loss: 0.0000, total loss: 0.0186\n",
      "Epoch [47/200], Step [6/6], decoder loss: 0.0157, encoder1_activation l2 loss: 0.0000, total loss: 0.0157\n",
      "Epoch [47/200] (avg 6 steps), decoder loss: 0.0173, total loss: 0.0173\n",
      "Epoch [48/200], Step [1/6], decoder loss: 0.0165, encoder1_activation l2 loss: 0.0000, total loss: 0.0165\n",
      "Epoch [48/200], Step [2/6], decoder loss: 0.0161, encoder1_activation l2 loss: 0.0000, total loss: 0.0161\n",
      "Epoch [48/200], Step [3/6], decoder loss: 0.0189, encoder1_activation l2 loss: 0.0000, total loss: 0.0189\n",
      "Epoch [48/200], Step [4/6], decoder loss: 0.0176, encoder1_activation l2 loss: 0.0000, total loss: 0.0176\n",
      "Epoch [48/200], Step [5/6], decoder loss: 0.0180, encoder1_activation l2 loss: 0.0000, total loss: 0.0180\n",
      "Epoch [48/200], Step [6/6], decoder loss: 0.0151, encoder1_activation l2 loss: 0.0000, total loss: 0.0151\n",
      "Epoch [48/200] (avg 6 steps), decoder loss: 0.0170, total loss: 0.0170\n",
      "Epoch [49/200], Step [1/6], decoder loss: 0.0167, encoder1_activation l2 loss: 0.0000, total loss: 0.0167\n",
      "Epoch [49/200], Step [2/6], decoder loss: 0.0174, encoder1_activation l2 loss: 0.0000, total loss: 0.0174\n",
      "Epoch [49/200], Step [3/6], decoder loss: 0.0177, encoder1_activation l2 loss: 0.0000, total loss: 0.0177\n",
      "Epoch [49/200], Step [4/6], decoder loss: 0.0165, encoder1_activation l2 loss: 0.0000, total loss: 0.0165\n",
      "Epoch [49/200], Step [5/6], decoder loss: 0.0172, encoder1_activation l2 loss: 0.0000, total loss: 0.0172\n",
      "Epoch [49/200], Step [6/6], decoder loss: 0.0156, encoder1_activation l2 loss: 0.0000, total loss: 0.0156\n",
      "Epoch [49/200] (avg 6 steps), decoder loss: 0.0168, total loss: 0.0168\n",
      "Epoch [50/200], Step [1/6], decoder loss: 0.0155, encoder1_activation l2 loss: 0.0000, total loss: 0.0155\n",
      "Epoch [50/200], Step [2/6], decoder loss: 0.0179, encoder1_activation l2 loss: 0.0000, total loss: 0.0179\n",
      "Epoch [50/200], Step [3/6], decoder loss: 0.0187, encoder1_activation l2 loss: 0.0000, total loss: 0.0187\n",
      "Epoch [50/200], Step [4/6], decoder loss: 0.0153, encoder1_activation l2 loss: 0.0000, total loss: 0.0153\n",
      "Epoch [50/200], Step [5/6], decoder loss: 0.0170, encoder1_activation l2 loss: 0.0000, total loss: 0.0170\n",
      "Epoch [50/200], Step [6/6], decoder loss: 0.0151, encoder1_activation l2 loss: 0.0000, total loss: 0.0151\n",
      "Epoch [50/200] (avg 6 steps), decoder loss: 0.0166, total loss: 0.0166\n",
      "Epoch [51/200], Step [1/6], decoder loss: 0.0170, encoder1_activation l2 loss: 0.0000, total loss: 0.0170\n",
      "Epoch [51/200], Step [2/6], decoder loss: 0.0166, encoder1_activation l2 loss: 0.0000, total loss: 0.0166\n",
      "Epoch [51/200], Step [3/6], decoder loss: 0.0179, encoder1_activation l2 loss: 0.0000, total loss: 0.0179\n",
      "Epoch [51/200], Step [4/6], decoder loss: 0.0164, encoder1_activation l2 loss: 0.0000, total loss: 0.0164\n",
      "Epoch [51/200], Step [5/6], decoder loss: 0.0150, encoder1_activation l2 loss: 0.0000, total loss: 0.0150\n",
      "Epoch [51/200], Step [6/6], decoder loss: 0.0154, encoder1_activation l2 loss: 0.0000, total loss: 0.0154\n",
      "Epoch [51/200] (avg 6 steps), decoder loss: 0.0164, total loss: 0.0164\n",
      "Epoch [52/200], Step [1/6], decoder loss: 0.0162, encoder1_activation l2 loss: 0.0000, total loss: 0.0162\n",
      "Epoch [52/200], Step [2/6], decoder loss: 0.0165, encoder1_activation l2 loss: 0.0000, total loss: 0.0165\n",
      "Epoch [52/200], Step [3/6], decoder loss: 0.0154, encoder1_activation l2 loss: 0.0000, total loss: 0.0154\n",
      "Epoch [52/200], Step [4/6], decoder loss: 0.0165, encoder1_activation l2 loss: 0.0000, total loss: 0.0165\n",
      "Epoch [52/200], Step [5/6], decoder loss: 0.0170, encoder1_activation l2 loss: 0.0000, total loss: 0.0170\n",
      "Epoch [52/200], Step [6/6], decoder loss: 0.0156, encoder1_activation l2 loss: 0.0000, total loss: 0.0156\n",
      "Epoch [52/200] (avg 6 steps), decoder loss: 0.0162, total loss: 0.0162\n",
      "Epoch [53/200], Step [1/6], decoder loss: 0.0151, encoder1_activation l2 loss: 0.0000, total loss: 0.0151\n",
      "Epoch [53/200], Step [2/6], decoder loss: 0.0157, encoder1_activation l2 loss: 0.0000, total loss: 0.0157\n",
      "Epoch [53/200], Step [3/6], decoder loss: 0.0176, encoder1_activation l2 loss: 0.0000, total loss: 0.0176\n",
      "Epoch [53/200], Step [4/6], decoder loss: 0.0158, encoder1_activation l2 loss: 0.0000, total loss: 0.0158\n",
      "Epoch [53/200], Step [5/6], decoder loss: 0.0159, encoder1_activation l2 loss: 0.0000, total loss: 0.0159\n",
      "Epoch [53/200], Step [6/6], decoder loss: 0.0164, encoder1_activation l2 loss: 0.0000, total loss: 0.0164\n",
      "Epoch [53/200] (avg 6 steps), decoder loss: 0.0161, total loss: 0.0161\n",
      "Epoch [54/200], Step [1/6], decoder loss: 0.0162, encoder1_activation l2 loss: 0.0000, total loss: 0.0162\n",
      "Epoch [54/200], Step [2/6], decoder loss: 0.0158, encoder1_activation l2 loss: 0.0000, total loss: 0.0158\n",
      "Epoch [54/200], Step [3/6], decoder loss: 0.0152, encoder1_activation l2 loss: 0.0000, total loss: 0.0152\n",
      "Epoch [54/200], Step [4/6], decoder loss: 0.0157, encoder1_activation l2 loss: 0.0000, total loss: 0.0157\n",
      "Epoch [54/200], Step [5/6], decoder loss: 0.0162, encoder1_activation l2 loss: 0.0000, total loss: 0.0162\n",
      "Epoch [54/200], Step [6/6], decoder loss: 0.0166, encoder1_activation l2 loss: 0.0000, total loss: 0.0166\n",
      "Epoch [54/200] (avg 6 steps), decoder loss: 0.0159, total loss: 0.0159\n",
      "Epoch [55/200], Step [1/6], decoder loss: 0.0156, encoder1_activation l2 loss: 0.0000, total loss: 0.0156\n",
      "Epoch [55/200], Step [2/6], decoder loss: 0.0167, encoder1_activation l2 loss: 0.0000, total loss: 0.0167\n",
      "Epoch [55/200], Step [3/6], decoder loss: 0.0155, encoder1_activation l2 loss: 0.0000, total loss: 0.0155\n",
      "Epoch [55/200], Step [4/6], decoder loss: 0.0158, encoder1_activation l2 loss: 0.0000, total loss: 0.0158\n",
      "Epoch [55/200], Step [5/6], decoder loss: 0.0161, encoder1_activation l2 loss: 0.0000, total loss: 0.0161\n",
      "Epoch [55/200], Step [6/6], decoder loss: 0.0138, encoder1_activation l2 loss: 0.0000, total loss: 0.0138\n",
      "Epoch [55/200] (avg 6 steps), decoder loss: 0.0156, total loss: 0.0156\n",
      "Epoch [56/200], Step [1/6], decoder loss: 0.0145, encoder1_activation l2 loss: 0.0000, total loss: 0.0145\n",
      "Epoch [56/200], Step [2/6], decoder loss: 0.0156, encoder1_activation l2 loss: 0.0000, total loss: 0.0156\n",
      "Epoch [56/200], Step [3/6], decoder loss: 0.0160, encoder1_activation l2 loss: 0.0000, total loss: 0.0160\n",
      "Epoch [56/200], Step [4/6], decoder loss: 0.0160, encoder1_activation l2 loss: 0.0000, total loss: 0.0160\n",
      "Epoch [56/200], Step [5/6], decoder loss: 0.0154, encoder1_activation l2 loss: 0.0000, total loss: 0.0154\n",
      "Epoch [56/200], Step [6/6], decoder loss: 0.0160, encoder1_activation l2 loss: 0.0000, total loss: 0.0160\n",
      "Epoch [56/200] (avg 6 steps), decoder loss: 0.0156, total loss: 0.0156\n",
      "Epoch [57/200], Step [1/6], decoder loss: 0.0158, encoder1_activation l2 loss: 0.0000, total loss: 0.0158\n",
      "Epoch [57/200], Step [2/6], decoder loss: 0.0146, encoder1_activation l2 loss: 0.0000, total loss: 0.0146\n",
      "Epoch [57/200], Step [3/6], decoder loss: 0.0142, encoder1_activation l2 loss: 0.0000, total loss: 0.0142\n",
      "Epoch [57/200], Step [4/6], decoder loss: 0.0174, encoder1_activation l2 loss: 0.0000, total loss: 0.0174\n",
      "Epoch [57/200], Step [5/6], decoder loss: 0.0145, encoder1_activation l2 loss: 0.0000, total loss: 0.0145\n",
      "Epoch [57/200], Step [6/6], decoder loss: 0.0162, encoder1_activation l2 loss: 0.0000, total loss: 0.0162\n",
      "Epoch [57/200] (avg 6 steps), decoder loss: 0.0154, total loss: 0.0155\n",
      "Epoch [58/200], Step [1/6], decoder loss: 0.0161, encoder1_activation l2 loss: 0.0000, total loss: 0.0161\n",
      "Epoch [58/200], Step [2/6], decoder loss: 0.0140, encoder1_activation l2 loss: 0.0000, total loss: 0.0140\n",
      "Epoch [58/200], Step [3/6], decoder loss: 0.0170, encoder1_activation l2 loss: 0.0000, total loss: 0.0170\n",
      "Epoch [58/200], Step [4/6], decoder loss: 0.0143, encoder1_activation l2 loss: 0.0000, total loss: 0.0143\n",
      "Epoch [58/200], Step [5/6], decoder loss: 0.0147, encoder1_activation l2 loss: 0.0000, total loss: 0.0147\n",
      "Epoch [58/200], Step [6/6], decoder loss: 0.0155, encoder1_activation l2 loss: 0.0000, total loss: 0.0155\n",
      "Epoch [58/200] (avg 6 steps), decoder loss: 0.0153, total loss: 0.0153\n",
      "Epoch [59/200], Step [1/6], decoder loss: 0.0164, encoder1_activation l2 loss: 0.0000, total loss: 0.0164\n",
      "Epoch [59/200], Step [2/6], decoder loss: 0.0158, encoder1_activation l2 loss: 0.0000, total loss: 0.0158\n",
      "Epoch [59/200], Step [3/6], decoder loss: 0.0142, encoder1_activation l2 loss: 0.0000, total loss: 0.0142\n",
      "Epoch [59/200], Step [4/6], decoder loss: 0.0143, encoder1_activation l2 loss: 0.0000, total loss: 0.0143\n",
      "Epoch [59/200], Step [5/6], decoder loss: 0.0137, encoder1_activation l2 loss: 0.0000, total loss: 0.0137\n",
      "Epoch [59/200], Step [6/6], decoder loss: 0.0172, encoder1_activation l2 loss: 0.0000, total loss: 0.0172\n",
      "Epoch [59/200] (avg 6 steps), decoder loss: 0.0153, total loss: 0.0153\n",
      "Epoch [60/200], Step [1/6], decoder loss: 0.0158, encoder1_activation l2 loss: 0.0000, total loss: 0.0158\n",
      "Epoch [60/200], Step [2/6], decoder loss: 0.0140, encoder1_activation l2 loss: 0.0000, total loss: 0.0140\n",
      "Epoch [60/200], Step [3/6], decoder loss: 0.0157, encoder1_activation l2 loss: 0.0000, total loss: 0.0157\n",
      "Epoch [60/200], Step [4/6], decoder loss: 0.0140, encoder1_activation l2 loss: 0.0000, total loss: 0.0140\n",
      "Epoch [60/200], Step [5/6], decoder loss: 0.0153, encoder1_activation l2 loss: 0.0000, total loss: 0.0153\n",
      "Epoch [60/200], Step [6/6], decoder loss: 0.0151, encoder1_activation l2 loss: 0.0000, total loss: 0.0151\n",
      "Epoch [60/200] (avg 6 steps), decoder loss: 0.0150, total loss: 0.0150\n",
      "Epoch [61/200], Step [1/6], decoder loss: 0.0130, encoder1_activation l2 loss: 0.0000, total loss: 0.0130\n",
      "Epoch [61/200], Step [2/6], decoder loss: 0.0169, encoder1_activation l2 loss: 0.0000, total loss: 0.0169\n",
      "Epoch [61/200], Step [3/6], decoder loss: 0.0145, encoder1_activation l2 loss: 0.0000, total loss: 0.0145\n",
      "Epoch [61/200], Step [4/6], decoder loss: 0.0154, encoder1_activation l2 loss: 0.0000, total loss: 0.0154\n",
      "Epoch [61/200], Step [5/6], decoder loss: 0.0142, encoder1_activation l2 loss: 0.0000, total loss: 0.0142\n",
      "Epoch [61/200], Step [6/6], decoder loss: 0.0155, encoder1_activation l2 loss: 0.0000, total loss: 0.0155\n",
      "Epoch [61/200] (avg 6 steps), decoder loss: 0.0149, total loss: 0.0149\n",
      "Epoch [62/200], Step [1/6], decoder loss: 0.0149, encoder1_activation l2 loss: 0.0000, total loss: 0.0149\n",
      "Epoch [62/200], Step [2/6], decoder loss: 0.0152, encoder1_activation l2 loss: 0.0000, total loss: 0.0152\n",
      "Epoch [62/200], Step [3/6], decoder loss: 0.0138, encoder1_activation l2 loss: 0.0000, total loss: 0.0138\n",
      "Epoch [62/200], Step [4/6], decoder loss: 0.0144, encoder1_activation l2 loss: 0.0000, total loss: 0.0144\n",
      "Epoch [62/200], Step [5/6], decoder loss: 0.0160, encoder1_activation l2 loss: 0.0000, total loss: 0.0160\n",
      "Epoch [62/200], Step [6/6], decoder loss: 0.0139, encoder1_activation l2 loss: 0.0000, total loss: 0.0139\n",
      "Epoch [62/200] (avg 6 steps), decoder loss: 0.0147, total loss: 0.0147\n",
      "Epoch [63/200], Step [1/6], decoder loss: 0.0140, encoder1_activation l2 loss: 0.0000, total loss: 0.0140\n",
      "Epoch [63/200], Step [2/6], decoder loss: 0.0153, encoder1_activation l2 loss: 0.0000, total loss: 0.0153\n",
      "Epoch [63/200], Step [3/6], decoder loss: 0.0150, encoder1_activation l2 loss: 0.0000, total loss: 0.0150\n",
      "Epoch [63/200], Step [4/6], decoder loss: 0.0134, encoder1_activation l2 loss: 0.0000, total loss: 0.0134\n",
      "Epoch [63/200], Step [5/6], decoder loss: 0.0157, encoder1_activation l2 loss: 0.0000, total loss: 0.0157\n",
      "Epoch [63/200], Step [6/6], decoder loss: 0.0144, encoder1_activation l2 loss: 0.0000, total loss: 0.0144\n",
      "Epoch [63/200] (avg 6 steps), decoder loss: 0.0146, total loss: 0.0146\n",
      "Epoch [64/200], Step [1/6], decoder loss: 0.0149, encoder1_activation l2 loss: 0.0000, total loss: 0.0149\n",
      "Epoch [64/200], Step [2/6], decoder loss: 0.0129, encoder1_activation l2 loss: 0.0000, total loss: 0.0129\n",
      "Epoch [64/200], Step [3/6], decoder loss: 0.0153, encoder1_activation l2 loss: 0.0000, total loss: 0.0153\n",
      "Epoch [64/200], Step [4/6], decoder loss: 0.0151, encoder1_activation l2 loss: 0.0000, total loss: 0.0151\n",
      "Epoch [64/200], Step [5/6], decoder loss: 0.0141, encoder1_activation l2 loss: 0.0000, total loss: 0.0141\n",
      "Epoch [64/200], Step [6/6], decoder loss: 0.0154, encoder1_activation l2 loss: 0.0000, total loss: 0.0154\n",
      "Epoch [64/200] (avg 6 steps), decoder loss: 0.0146, total loss: 0.0146\n",
      "Epoch [65/200], Step [1/6], decoder loss: 0.0147, encoder1_activation l2 loss: 0.0000, total loss: 0.0147\n",
      "Epoch [65/200], Step [2/6], decoder loss: 0.0136, encoder1_activation l2 loss: 0.0000, total loss: 0.0136\n",
      "Epoch [65/200], Step [3/6], decoder loss: 0.0159, encoder1_activation l2 loss: 0.0000, total loss: 0.0159\n",
      "Epoch [65/200], Step [4/6], decoder loss: 0.0140, encoder1_activation l2 loss: 0.0000, total loss: 0.0140\n",
      "Epoch [65/200], Step [5/6], decoder loss: 0.0137, encoder1_activation l2 loss: 0.0000, total loss: 0.0137\n",
      "Epoch [65/200], Step [6/6], decoder loss: 0.0151, encoder1_activation l2 loss: 0.0000, total loss: 0.0151\n",
      "Epoch [65/200] (avg 6 steps), decoder loss: 0.0145, total loss: 0.0145\n",
      "Epoch [66/200], Step [1/6], decoder loss: 0.0144, encoder1_activation l2 loss: 0.0000, total loss: 0.0145\n",
      "Epoch [66/200], Step [2/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [66/200], Step [3/6], decoder loss: 0.0133, encoder1_activation l2 loss: 0.0000, total loss: 0.0133\n",
      "Epoch [66/200], Step [4/6], decoder loss: 0.0148, encoder1_activation l2 loss: 0.0000, total loss: 0.0148\n",
      "Epoch [66/200], Step [5/6], decoder loss: 0.0156, encoder1_activation l2 loss: 0.0000, total loss: 0.0156\n",
      "Epoch [66/200], Step [6/6], decoder loss: 0.0161, encoder1_activation l2 loss: 0.0000, total loss: 0.0161\n",
      "Epoch [66/200] (avg 6 steps), decoder loss: 0.0145, total loss: 0.0145\n",
      "Epoch [67/200], Step [1/6], decoder loss: 0.0159, encoder1_activation l2 loss: 0.0000, total loss: 0.0159\n",
      "Epoch [67/200], Step [2/6], decoder loss: 0.0132, encoder1_activation l2 loss: 0.0000, total loss: 0.0132\n",
      "Epoch [67/200], Step [3/6], decoder loss: 0.0142, encoder1_activation l2 loss: 0.0000, total loss: 0.0142\n",
      "Epoch [67/200], Step [4/6], decoder loss: 0.0150, encoder1_activation l2 loss: 0.0000, total loss: 0.0150\n",
      "Epoch [67/200], Step [5/6], decoder loss: 0.0134, encoder1_activation l2 loss: 0.0000, total loss: 0.0134\n",
      "Epoch [67/200], Step [6/6], decoder loss: 0.0135, encoder1_activation l2 loss: 0.0000, total loss: 0.0135\n",
      "Epoch [67/200] (avg 6 steps), decoder loss: 0.0142, total loss: 0.0142\n",
      "Epoch [68/200], Step [1/6], decoder loss: 0.0142, encoder1_activation l2 loss: 0.0000, total loss: 0.0142\n",
      "Epoch [68/200], Step [2/6], decoder loss: 0.0137, encoder1_activation l2 loss: 0.0000, total loss: 0.0137\n",
      "Epoch [68/200], Step [3/6], decoder loss: 0.0138, encoder1_activation l2 loss: 0.0000, total loss: 0.0138\n",
      "Epoch [68/200], Step [4/6], decoder loss: 0.0137, encoder1_activation l2 loss: 0.0000, total loss: 0.0137\n",
      "Epoch [68/200], Step [5/6], decoder loss: 0.0143, encoder1_activation l2 loss: 0.0000, total loss: 0.0143\n",
      "Epoch [68/200], Step [6/6], decoder loss: 0.0164, encoder1_activation l2 loss: 0.0000, total loss: 0.0164\n",
      "Epoch [68/200] (avg 6 steps), decoder loss: 0.0143, total loss: 0.0143\n",
      "Epoch [69/200], Step [1/6], decoder loss: 0.0138, encoder1_activation l2 loss: 0.0000, total loss: 0.0138\n",
      "Epoch [69/200], Step [2/6], decoder loss: 0.0143, encoder1_activation l2 loss: 0.0000, total loss: 0.0143\n",
      "Epoch [69/200], Step [3/6], decoder loss: 0.0138, encoder1_activation l2 loss: 0.0000, total loss: 0.0138\n",
      "Epoch [69/200], Step [4/6], decoder loss: 0.0150, encoder1_activation l2 loss: 0.0000, total loss: 0.0150\n",
      "Epoch [69/200], Step [5/6], decoder loss: 0.0130, encoder1_activation l2 loss: 0.0000, total loss: 0.0130\n",
      "Epoch [69/200], Step [6/6], decoder loss: 0.0153, encoder1_activation l2 loss: 0.0000, total loss: 0.0153\n",
      "Epoch [69/200] (avg 6 steps), decoder loss: 0.0142, total loss: 0.0142\n",
      "Epoch [70/200], Step [1/6], decoder loss: 0.0145, encoder1_activation l2 loss: 0.0000, total loss: 0.0145\n",
      "Epoch [70/200], Step [2/6], decoder loss: 0.0137, encoder1_activation l2 loss: 0.0000, total loss: 0.0137\n",
      "Epoch [70/200], Step [3/6], decoder loss: 0.0134, encoder1_activation l2 loss: 0.0000, total loss: 0.0134\n",
      "Epoch [70/200], Step [4/6], decoder loss: 0.0133, encoder1_activation l2 loss: 0.0000, total loss: 0.0133\n",
      "Epoch [70/200], Step [5/6], decoder loss: 0.0149, encoder1_activation l2 loss: 0.0000, total loss: 0.0149\n",
      "Epoch [70/200], Step [6/6], decoder loss: 0.0146, encoder1_activation l2 loss: 0.0000, total loss: 0.0146\n",
      "Epoch [70/200] (avg 6 steps), decoder loss: 0.0141, total loss: 0.0141\n",
      "Epoch [71/200], Step [1/6], decoder loss: 0.0138, encoder1_activation l2 loss: 0.0000, total loss: 0.0138\n",
      "Epoch [71/200], Step [2/6], decoder loss: 0.0136, encoder1_activation l2 loss: 0.0000, total loss: 0.0136\n",
      "Epoch [71/200], Step [3/6], decoder loss: 0.0136, encoder1_activation l2 loss: 0.0000, total loss: 0.0136\n",
      "Epoch [71/200], Step [4/6], decoder loss: 0.0147, encoder1_activation l2 loss: 0.0000, total loss: 0.0147\n",
      "Epoch [71/200], Step [5/6], decoder loss: 0.0141, encoder1_activation l2 loss: 0.0000, total loss: 0.0141\n",
      "Epoch [71/200], Step [6/6], decoder loss: 0.0141, encoder1_activation l2 loss: 0.0000, total loss: 0.0141\n",
      "Epoch [71/200] (avg 6 steps), decoder loss: 0.0140, total loss: 0.0140\n",
      "Epoch [72/200], Step [1/6], decoder loss: 0.0150, encoder1_activation l2 loss: 0.0000, total loss: 0.0150\n",
      "Epoch [72/200], Step [2/6], decoder loss: 0.0137, encoder1_activation l2 loss: 0.0000, total loss: 0.0137\n",
      "Epoch [72/200], Step [3/6], decoder loss: 0.0126, encoder1_activation l2 loss: 0.0000, total loss: 0.0126\n",
      "Epoch [72/200], Step [4/6], decoder loss: 0.0138, encoder1_activation l2 loss: 0.0000, total loss: 0.0138\n",
      "Epoch [72/200], Step [5/6], decoder loss: 0.0147, encoder1_activation l2 loss: 0.0000, total loss: 0.0147\n",
      "Epoch [72/200], Step [6/6], decoder loss: 0.0133, encoder1_activation l2 loss: 0.0000, total loss: 0.0133\n",
      "Epoch [72/200] (avg 6 steps), decoder loss: 0.0138, total loss: 0.0138\n",
      "Epoch [73/200], Step [1/6], decoder loss: 0.0140, encoder1_activation l2 loss: 0.0000, total loss: 0.0140\n",
      "Epoch [73/200], Step [2/6], decoder loss: 0.0139, encoder1_activation l2 loss: 0.0000, total loss: 0.0139\n",
      "Epoch [73/200], Step [3/6], decoder loss: 0.0139, encoder1_activation l2 loss: 0.0000, total loss: 0.0139\n",
      "Epoch [73/200], Step [4/6], decoder loss: 0.0149, encoder1_activation l2 loss: 0.0000, total loss: 0.0149\n",
      "Epoch [73/200], Step [5/6], decoder loss: 0.0134, encoder1_activation l2 loss: 0.0000, total loss: 0.0134\n",
      "Epoch [73/200], Step [6/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [73/200] (avg 6 steps), decoder loss: 0.0137, total loss: 0.0137\n",
      "Epoch [74/200], Step [1/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [74/200], Step [2/6], decoder loss: 0.0143, encoder1_activation l2 loss: 0.0000, total loss: 0.0143\n",
      "Epoch [74/200], Step [3/6], decoder loss: 0.0148, encoder1_activation l2 loss: 0.0000, total loss: 0.0148\n",
      "Epoch [74/200], Step [4/6], decoder loss: 0.0137, encoder1_activation l2 loss: 0.0000, total loss: 0.0137\n",
      "Epoch [74/200], Step [5/6], decoder loss: 0.0139, encoder1_activation l2 loss: 0.0000, total loss: 0.0139\n",
      "Epoch [74/200], Step [6/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [74/200] (avg 6 steps), decoder loss: 0.0137, total loss: 0.0137\n",
      "Epoch [75/200], Step [1/6], decoder loss: 0.0143, encoder1_activation l2 loss: 0.0000, total loss: 0.0143\n",
      "Epoch [75/200], Step [2/6], decoder loss: 0.0151, encoder1_activation l2 loss: 0.0000, total loss: 0.0151\n",
      "Epoch [75/200], Step [3/6], decoder loss: 0.0130, encoder1_activation l2 loss: 0.0000, total loss: 0.0130\n",
      "Epoch [75/200], Step [4/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [75/200], Step [5/6], decoder loss: 0.0146, encoder1_activation l2 loss: 0.0000, total loss: 0.0146\n",
      "Epoch [75/200], Step [6/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [75/200] (avg 6 steps), decoder loss: 0.0136, total loss: 0.0136\n",
      "Epoch [76/200], Step [1/6], decoder loss: 0.0152, encoder1_activation l2 loss: 0.0000, total loss: 0.0152\n",
      "Epoch [76/200], Step [2/6], decoder loss: 0.0135, encoder1_activation l2 loss: 0.0000, total loss: 0.0135\n",
      "Epoch [76/200], Step [3/6], decoder loss: 0.0142, encoder1_activation l2 loss: 0.0000, total loss: 0.0142\n",
      "Epoch [76/200], Step [4/6], decoder loss: 0.0135, encoder1_activation l2 loss: 0.0000, total loss: 0.0136\n",
      "Epoch [76/200], Step [5/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [76/200], Step [6/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [76/200] (avg 6 steps), decoder loss: 0.0136, total loss: 0.0136\n",
      "Epoch [77/200], Step [1/6], decoder loss: 0.0138, encoder1_activation l2 loss: 0.0000, total loss: 0.0138\n",
      "Epoch [77/200], Step [2/6], decoder loss: 0.0128, encoder1_activation l2 loss: 0.0000, total loss: 0.0128\n",
      "Epoch [77/200], Step [3/6], decoder loss: 0.0157, encoder1_activation l2 loss: 0.0000, total loss: 0.0157\n",
      "Epoch [77/200], Step [4/6], decoder loss: 0.0139, encoder1_activation l2 loss: 0.0000, total loss: 0.0139\n",
      "Epoch [77/200], Step [5/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [77/200], Step [6/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [77/200] (avg 6 steps), decoder loss: 0.0135, total loss: 0.0135\n",
      "Epoch [78/200], Step [1/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [78/200], Step [2/6], decoder loss: 0.0137, encoder1_activation l2 loss: 0.0000, total loss: 0.0137\n",
      "Epoch [78/200], Step [3/6], decoder loss: 0.0149, encoder1_activation l2 loss: 0.0000, total loss: 0.0149\n",
      "Epoch [78/200], Step [4/6], decoder loss: 0.0131, encoder1_activation l2 loss: 0.0000, total loss: 0.0131\n",
      "Epoch [78/200], Step [5/6], decoder loss: 0.0131, encoder1_activation l2 loss: 0.0000, total loss: 0.0131\n",
      "Epoch [78/200], Step [6/6], decoder loss: 0.0144, encoder1_activation l2 loss: 0.0000, total loss: 0.0144\n",
      "Epoch [78/200] (avg 6 steps), decoder loss: 0.0136, total loss: 0.0136\n",
      "Epoch [79/200], Step [1/6], decoder loss: 0.0129, encoder1_activation l2 loss: 0.0000, total loss: 0.0129\n",
      "Epoch [79/200], Step [2/6], decoder loss: 0.0139, encoder1_activation l2 loss: 0.0000, total loss: 0.0139\n",
      "Epoch [79/200], Step [3/6], decoder loss: 0.0149, encoder1_activation l2 loss: 0.0000, total loss: 0.0149\n",
      "Epoch [79/200], Step [4/6], decoder loss: 0.0114, encoder1_activation l2 loss: 0.0000, total loss: 0.0114\n",
      "Epoch [79/200], Step [5/6], decoder loss: 0.0135, encoder1_activation l2 loss: 0.0000, total loss: 0.0135\n",
      "Epoch [79/200], Step [6/6], decoder loss: 0.0149, encoder1_activation l2 loss: 0.0000, total loss: 0.0149\n",
      "Epoch [79/200] (avg 6 steps), decoder loss: 0.0136, total loss: 0.0136\n",
      "Epoch [80/200], Step [1/6], decoder loss: 0.0135, encoder1_activation l2 loss: 0.0000, total loss: 0.0135\n",
      "Epoch [80/200], Step [2/6], decoder loss: 0.0139, encoder1_activation l2 loss: 0.0000, total loss: 0.0139\n",
      "Epoch [80/200], Step [3/6], decoder loss: 0.0136, encoder1_activation l2 loss: 0.0000, total loss: 0.0136\n",
      "Epoch [80/200], Step [4/6], decoder loss: 0.0132, encoder1_activation l2 loss: 0.0000, total loss: 0.0132\n",
      "Epoch [80/200], Step [5/6], decoder loss: 0.0126, encoder1_activation l2 loss: 0.0000, total loss: 0.0126\n",
      "Epoch [80/200], Step [6/6], decoder loss: 0.0139, encoder1_activation l2 loss: 0.0000, total loss: 0.0139\n",
      "Epoch [80/200] (avg 6 steps), decoder loss: 0.0135, total loss: 0.0135\n",
      "Epoch [81/200], Step [1/6], decoder loss: 0.0137, encoder1_activation l2 loss: 0.0000, total loss: 0.0137\n",
      "Epoch [81/200], Step [2/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [81/200], Step [3/6], decoder loss: 0.0133, encoder1_activation l2 loss: 0.0000, total loss: 0.0133\n",
      "Epoch [81/200], Step [4/6], decoder loss: 0.0150, encoder1_activation l2 loss: 0.0000, total loss: 0.0150\n",
      "Epoch [81/200], Step [5/6], decoder loss: 0.0143, encoder1_activation l2 loss: 0.0000, total loss: 0.0143\n",
      "Epoch [81/200], Step [6/6], decoder loss: 0.0107, encoder1_activation l2 loss: 0.0000, total loss: 0.0107\n",
      "Epoch [81/200] (avg 6 steps), decoder loss: 0.0132, total loss: 0.0132\n",
      "Epoch [82/200], Step [1/6], decoder loss: 0.0140, encoder1_activation l2 loss: 0.0000, total loss: 0.0140\n",
      "Epoch [82/200], Step [2/6], decoder loss: 0.0131, encoder1_activation l2 loss: 0.0000, total loss: 0.0131\n",
      "Epoch [82/200], Step [3/6], decoder loss: 0.0126, encoder1_activation l2 loss: 0.0000, total loss: 0.0126\n",
      "Epoch [82/200], Step [4/6], decoder loss: 0.0141, encoder1_activation l2 loss: 0.0000, total loss: 0.0141\n",
      "Epoch [82/200], Step [5/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [82/200], Step [6/6], decoder loss: 0.0141, encoder1_activation l2 loss: 0.0000, total loss: 0.0141\n",
      "Epoch [82/200] (avg 6 steps), decoder loss: 0.0134, total loss: 0.0134\n",
      "Epoch [83/200], Step [1/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [83/200], Step [2/6], decoder loss: 0.0133, encoder1_activation l2 loss: 0.0000, total loss: 0.0133\n",
      "Epoch [83/200], Step [3/6], decoder loss: 0.0130, encoder1_activation l2 loss: 0.0000, total loss: 0.0130\n",
      "Epoch [83/200], Step [4/6], decoder loss: 0.0137, encoder1_activation l2 loss: 0.0000, total loss: 0.0137\n",
      "Epoch [83/200], Step [5/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [83/200], Step [6/6], decoder loss: 0.0163, encoder1_activation l2 loss: 0.0000, total loss: 0.0163\n",
      "Epoch [83/200] (avg 6 steps), decoder loss: 0.0135, total loss: 0.0135\n",
      "Epoch [84/200], Step [1/6], decoder loss: 0.0136, encoder1_activation l2 loss: 0.0000, total loss: 0.0136\n",
      "Epoch [84/200], Step [2/6], decoder loss: 0.0142, encoder1_activation l2 loss: 0.0000, total loss: 0.0142\n",
      "Epoch [84/200], Step [3/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [84/200], Step [4/6], decoder loss: 0.0128, encoder1_activation l2 loss: 0.0000, total loss: 0.0128\n",
      "Epoch [84/200], Step [5/6], decoder loss: 0.0140, encoder1_activation l2 loss: 0.0000, total loss: 0.0140\n",
      "Epoch [84/200], Step [6/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [84/200] (avg 6 steps), decoder loss: 0.0132, total loss: 0.0132\n",
      "Epoch [85/200], Step [1/6], decoder loss: 0.0126, encoder1_activation l2 loss: 0.0000, total loss: 0.0126\n",
      "Epoch [85/200], Step [2/6], decoder loss: 0.0133, encoder1_activation l2 loss: 0.0000, total loss: 0.0133\n",
      "Epoch [85/200], Step [3/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [85/200], Step [4/6], decoder loss: 0.0126, encoder1_activation l2 loss: 0.0000, total loss: 0.0126\n",
      "Epoch [85/200], Step [5/6], decoder loss: 0.0144, encoder1_activation l2 loss: 0.0000, total loss: 0.0144\n",
      "Epoch [85/200], Step [6/6], decoder loss: 0.0145, encoder1_activation l2 loss: 0.0000, total loss: 0.0145\n",
      "Epoch [85/200] (avg 6 steps), decoder loss: 0.0133, total loss: 0.0133\n",
      "Epoch [86/200], Step [1/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [86/200], Step [2/6], decoder loss: 0.0131, encoder1_activation l2 loss: 0.0000, total loss: 0.0131\n",
      "Epoch [86/200], Step [3/6], decoder loss: 0.0138, encoder1_activation l2 loss: 0.0000, total loss: 0.0138\n",
      "Epoch [86/200], Step [4/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [86/200], Step [5/6], decoder loss: 0.0126, encoder1_activation l2 loss: 0.0000, total loss: 0.0126\n",
      "Epoch [86/200], Step [6/6], decoder loss: 0.0157, encoder1_activation l2 loss: 0.0000, total loss: 0.0157\n",
      "Epoch [86/200] (avg 6 steps), decoder loss: 0.0134, total loss: 0.0134\n",
      "Epoch [87/200], Step [1/6], decoder loss: 0.0142, encoder1_activation l2 loss: 0.0000, total loss: 0.0142\n",
      "Epoch [87/200], Step [2/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [87/200], Step [3/6], decoder loss: 0.0115, encoder1_activation l2 loss: 0.0000, total loss: 0.0115\n",
      "Epoch [87/200], Step [4/6], decoder loss: 0.0151, encoder1_activation l2 loss: 0.0000, total loss: 0.0151\n",
      "Epoch [87/200], Step [5/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [87/200], Step [6/6], decoder loss: 0.0141, encoder1_activation l2 loss: 0.0000, total loss: 0.0141\n",
      "Epoch [87/200] (avg 6 steps), decoder loss: 0.0132, total loss: 0.0132\n",
      "Epoch [88/200], Step [1/6], decoder loss: 0.0126, encoder1_activation l2 loss: 0.0000, total loss: 0.0126\n",
      "Epoch [88/200], Step [2/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [88/200], Step [3/6], decoder loss: 0.0140, encoder1_activation l2 loss: 0.0000, total loss: 0.0140\n",
      "Epoch [88/200], Step [4/6], decoder loss: 0.0154, encoder1_activation l2 loss: 0.0000, total loss: 0.0154\n",
      "Epoch [88/200], Step [5/6], decoder loss: 0.0121, encoder1_activation l2 loss: 0.0000, total loss: 0.0121\n",
      "Epoch [88/200], Step [6/6], decoder loss: 0.0119, encoder1_activation l2 loss: 0.0000, total loss: 0.0119\n",
      "Epoch [88/200] (avg 6 steps), decoder loss: 0.0130, total loss: 0.0130\n",
      "Epoch [89/200], Step [1/6], decoder loss: 0.0145, encoder1_activation l2 loss: 0.0000, total loss: 0.0145\n",
      "Epoch [89/200], Step [2/6], decoder loss: 0.0121, encoder1_activation l2 loss: 0.0000, total loss: 0.0121\n",
      "Epoch [89/200], Step [3/6], decoder loss: 0.0129, encoder1_activation l2 loss: 0.0000, total loss: 0.0129\n",
      "Epoch [89/200], Step [4/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [89/200], Step [5/6], decoder loss: 0.0135, encoder1_activation l2 loss: 0.0000, total loss: 0.0135\n",
      "Epoch [89/200], Step [6/6], decoder loss: 0.0130, encoder1_activation l2 loss: 0.0000, total loss: 0.0130\n",
      "Epoch [89/200] (avg 6 steps), decoder loss: 0.0131, total loss: 0.0131\n",
      "Epoch [90/200], Step [1/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [90/200], Step [2/6], decoder loss: 0.0135, encoder1_activation l2 loss: 0.0000, total loss: 0.0135\n",
      "Epoch [90/200], Step [3/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [90/200], Step [4/6], decoder loss: 0.0148, encoder1_activation l2 loss: 0.0000, total loss: 0.0148\n",
      "Epoch [90/200], Step [5/6], decoder loss: 0.0128, encoder1_activation l2 loss: 0.0000, total loss: 0.0128\n",
      "Epoch [90/200], Step [6/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [90/200] (avg 6 steps), decoder loss: 0.0130, total loss: 0.0130\n",
      "Epoch [91/200], Step [1/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [91/200], Step [2/6], decoder loss: 0.0135, encoder1_activation l2 loss: 0.0000, total loss: 0.0135\n",
      "Epoch [91/200], Step [3/6], decoder loss: 0.0128, encoder1_activation l2 loss: 0.0000, total loss: 0.0128\n",
      "Epoch [91/200], Step [4/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [91/200], Step [5/6], decoder loss: 0.0142, encoder1_activation l2 loss: 0.0000, total loss: 0.0142\n",
      "Epoch [91/200], Step [6/6], decoder loss: 0.0126, encoder1_activation l2 loss: 0.0000, total loss: 0.0126\n",
      "Epoch [91/200] (avg 6 steps), decoder loss: 0.0130, total loss: 0.0130\n",
      "Epoch [92/200], Step [1/6], decoder loss: 0.0120, encoder1_activation l2 loss: 0.0000, total loss: 0.0120\n",
      "Epoch [92/200], Step [2/6], decoder loss: 0.0116, encoder1_activation l2 loss: 0.0000, total loss: 0.0116\n",
      "Epoch [92/200], Step [3/6], decoder loss: 0.0149, encoder1_activation l2 loss: 0.0000, total loss: 0.0149\n",
      "Epoch [92/200], Step [4/6], decoder loss: 0.0135, encoder1_activation l2 loss: 0.0000, total loss: 0.0135\n",
      "Epoch [92/200], Step [5/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [92/200], Step [6/6], decoder loss: 0.0139, encoder1_activation l2 loss: 0.0000, total loss: 0.0139\n",
      "Epoch [92/200] (avg 6 steps), decoder loss: 0.0130, total loss: 0.0130\n",
      "Epoch [93/200], Step [1/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [93/200], Step [2/6], decoder loss: 0.0119, encoder1_activation l2 loss: 0.0000, total loss: 0.0119\n",
      "Epoch [93/200], Step [3/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [93/200], Step [4/6], decoder loss: 0.0139, encoder1_activation l2 loss: 0.0000, total loss: 0.0139\n",
      "Epoch [93/200], Step [5/6], decoder loss: 0.0142, encoder1_activation l2 loss: 0.0000, total loss: 0.0142\n",
      "Epoch [93/200], Step [6/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [93/200] (avg 6 steps), decoder loss: 0.0129, total loss: 0.0129\n",
      "Epoch [94/200], Step [1/6], decoder loss: 0.0118, encoder1_activation l2 loss: 0.0000, total loss: 0.0118\n",
      "Epoch [94/200], Step [2/6], decoder loss: 0.0133, encoder1_activation l2 loss: 0.0000, total loss: 0.0133\n",
      "Epoch [94/200], Step [3/6], decoder loss: 0.0142, encoder1_activation l2 loss: 0.0000, total loss: 0.0142\n",
      "Epoch [94/200], Step [4/6], decoder loss: 0.0120, encoder1_activation l2 loss: 0.0000, total loss: 0.0120\n",
      "Epoch [94/200], Step [5/6], decoder loss: 0.0129, encoder1_activation l2 loss: 0.0000, total loss: 0.0129\n",
      "Epoch [94/200], Step [6/6], decoder loss: 0.0137, encoder1_activation l2 loss: 0.0000, total loss: 0.0137\n",
      "Epoch [94/200] (avg 6 steps), decoder loss: 0.0130, total loss: 0.0130\n",
      "Epoch [95/200], Step [1/6], decoder loss: 0.0118, encoder1_activation l2 loss: 0.0000, total loss: 0.0118\n",
      "Epoch [95/200], Step [2/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [95/200], Step [3/6], decoder loss: 0.0137, encoder1_activation l2 loss: 0.0000, total loss: 0.0137\n",
      "Epoch [95/200], Step [4/6], decoder loss: 0.0129, encoder1_activation l2 loss: 0.0000, total loss: 0.0129\n",
      "Epoch [95/200], Step [5/6], decoder loss: 0.0131, encoder1_activation l2 loss: 0.0000, total loss: 0.0131\n",
      "Epoch [95/200], Step [6/6], decoder loss: 0.0143, encoder1_activation l2 loss: 0.0000, total loss: 0.0143\n",
      "Epoch [95/200] (avg 6 steps), decoder loss: 0.0130, total loss: 0.0130\n",
      "Epoch [96/200], Step [1/6], decoder loss: 0.0143, encoder1_activation l2 loss: 0.0000, total loss: 0.0143\n",
      "Epoch [96/200], Step [2/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [96/200], Step [3/6], decoder loss: 0.0118, encoder1_activation l2 loss: 0.0000, total loss: 0.0118\n",
      "Epoch [96/200], Step [4/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [96/200], Step [5/6], decoder loss: 0.0119, encoder1_activation l2 loss: 0.0000, total loss: 0.0119\n",
      "Epoch [96/200], Step [6/6], decoder loss: 0.0152, encoder1_activation l2 loss: 0.0000, total loss: 0.0152\n",
      "Epoch [96/200] (avg 6 steps), decoder loss: 0.0130, total loss: 0.0130\n",
      "Epoch [97/200], Step [1/6], decoder loss: 0.0131, encoder1_activation l2 loss: 0.0000, total loss: 0.0131\n",
      "Epoch [97/200], Step [2/6], decoder loss: 0.0135, encoder1_activation l2 loss: 0.0000, total loss: 0.0135\n",
      "Epoch [97/200], Step [3/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [97/200], Step [4/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [97/200], Step [5/6], decoder loss: 0.0139, encoder1_activation l2 loss: 0.0000, total loss: 0.0139\n",
      "Epoch [97/200], Step [6/6], decoder loss: 0.0116, encoder1_activation l2 loss: 0.0000, total loss: 0.0116\n",
      "Epoch [97/200] (avg 6 steps), decoder loss: 0.0128, total loss: 0.0128\n",
      "Epoch [98/200], Step [1/6], decoder loss: 0.0142, encoder1_activation l2 loss: 0.0000, total loss: 0.0142\n",
      "Epoch [98/200], Step [2/6], decoder loss: 0.0132, encoder1_activation l2 loss: 0.0000, total loss: 0.0132\n",
      "Epoch [98/200], Step [3/6], decoder loss: 0.0120, encoder1_activation l2 loss: 0.0000, total loss: 0.0120\n",
      "Epoch [98/200], Step [4/6], decoder loss: 0.0117, encoder1_activation l2 loss: 0.0000, total loss: 0.0117\n",
      "Epoch [98/200], Step [5/6], decoder loss: 0.0135, encoder1_activation l2 loss: 0.0000, total loss: 0.0135\n",
      "Epoch [98/200], Step [6/6], decoder loss: 0.0118, encoder1_activation l2 loss: 0.0000, total loss: 0.0118\n",
      "Epoch [98/200] (avg 6 steps), decoder loss: 0.0127, total loss: 0.0127\n",
      "Epoch [99/200], Step [1/6], decoder loss: 0.0115, encoder1_activation l2 loss: 0.0000, total loss: 0.0115\n",
      "Epoch [99/200], Step [2/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [99/200], Step [3/6], decoder loss: 0.0152, encoder1_activation l2 loss: 0.0000, total loss: 0.0152\n",
      "Epoch [99/200], Step [4/6], decoder loss: 0.0137, encoder1_activation l2 loss: 0.0000, total loss: 0.0137\n",
      "Epoch [99/200], Step [5/6], decoder loss: 0.0107, encoder1_activation l2 loss: 0.0000, total loss: 0.0107\n",
      "Epoch [99/200], Step [6/6], decoder loss: 0.0139, encoder1_activation l2 loss: 0.0000, total loss: 0.0139\n",
      "Epoch [99/200] (avg 6 steps), decoder loss: 0.0129, total loss: 0.0129\n",
      "Epoch [100/200], Step [1/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [100/200], Step [2/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [100/200], Step [3/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [100/200], Step [4/6], decoder loss: 0.0132, encoder1_activation l2 loss: 0.0000, total loss: 0.0132\n",
      "Epoch [100/200], Step [5/6], decoder loss: 0.0134, encoder1_activation l2 loss: 0.0000, total loss: 0.0134\n",
      "Epoch [100/200], Step [6/6], decoder loss: 0.0128, encoder1_activation l2 loss: 0.0000, total loss: 0.0128\n",
      "Epoch [100/200] (avg 6 steps), decoder loss: 0.0128, total loss: 0.0128\n",
      "Epoch [101/200], Step [1/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [101/200], Step [2/6], decoder loss: 0.0129, encoder1_activation l2 loss: 0.0000, total loss: 0.0129\n",
      "Epoch [101/200], Step [3/6], decoder loss: 0.0131, encoder1_activation l2 loss: 0.0000, total loss: 0.0131\n",
      "Epoch [101/200], Step [4/6], decoder loss: 0.0133, encoder1_activation l2 loss: 0.0000, total loss: 0.0133\n",
      "Epoch [101/200], Step [5/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [101/200], Step [6/6], decoder loss: 0.0120, encoder1_activation l2 loss: 0.0000, total loss: 0.0120\n",
      "Epoch [101/200] (avg 6 steps), decoder loss: 0.0127, total loss: 0.0127\n",
      "Epoch [102/200], Step [1/6], decoder loss: 0.0145, encoder1_activation l2 loss: 0.0000, total loss: 0.0145\n",
      "Epoch [102/200], Step [2/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [102/200], Step [3/6], decoder loss: 0.0118, encoder1_activation l2 loss: 0.0000, total loss: 0.0118\n",
      "Epoch [102/200], Step [4/6], decoder loss: 0.0133, encoder1_activation l2 loss: 0.0000, total loss: 0.0133\n",
      "Epoch [102/200], Step [5/6], decoder loss: 0.0121, encoder1_activation l2 loss: 0.0000, total loss: 0.0121\n",
      "Epoch [102/200], Step [6/6], decoder loss: 0.0116, encoder1_activation l2 loss: 0.0000, total loss: 0.0116\n",
      "Epoch [102/200] (avg 6 steps), decoder loss: 0.0126, total loss: 0.0126\n",
      "Epoch [103/200], Step [1/6], decoder loss: 0.0131, encoder1_activation l2 loss: 0.0000, total loss: 0.0131\n",
      "Epoch [103/200], Step [2/6], decoder loss: 0.0137, encoder1_activation l2 loss: 0.0000, total loss: 0.0137\n",
      "Epoch [103/200], Step [3/6], decoder loss: 0.0107, encoder1_activation l2 loss: 0.0000, total loss: 0.0107\n",
      "Epoch [103/200], Step [4/6], decoder loss: 0.0117, encoder1_activation l2 loss: 0.0000, total loss: 0.0117\n",
      "Epoch [103/200], Step [5/6], decoder loss: 0.0135, encoder1_activation l2 loss: 0.0000, total loss: 0.0135\n",
      "Epoch [103/200], Step [6/6], decoder loss: 0.0141, encoder1_activation l2 loss: 0.0000, total loss: 0.0141\n",
      "Epoch [103/200] (avg 6 steps), decoder loss: 0.0128, total loss: 0.0128\n",
      "Epoch [104/200], Step [1/6], decoder loss: 0.0153, encoder1_activation l2 loss: 0.0000, total loss: 0.0153\n",
      "Epoch [104/200], Step [2/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [104/200], Step [3/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [104/200], Step [4/6], decoder loss: 0.0118, encoder1_activation l2 loss: 0.0000, total loss: 0.0118\n",
      "Epoch [104/200], Step [5/6], decoder loss: 0.0111, encoder1_activation l2 loss: 0.0000, total loss: 0.0111\n",
      "Epoch [104/200], Step [6/6], decoder loss: 0.0137, encoder1_activation l2 loss: 0.0000, total loss: 0.0137\n",
      "Epoch [104/200] (avg 6 steps), decoder loss: 0.0128, total loss: 0.0128\n",
      "Epoch [105/200], Step [1/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [105/200], Step [2/6], decoder loss: 0.0147, encoder1_activation l2 loss: 0.0000, total loss: 0.0147\n",
      "Epoch [105/200], Step [3/6], decoder loss: 0.0111, encoder1_activation l2 loss: 0.0000, total loss: 0.0111\n",
      "Epoch [105/200], Step [4/6], decoder loss: 0.0118, encoder1_activation l2 loss: 0.0000, total loss: 0.0118\n",
      "Epoch [105/200], Step [5/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [105/200], Step [6/6], decoder loss: 0.0130, encoder1_activation l2 loss: 0.0000, total loss: 0.0130\n",
      "Epoch [105/200] (avg 6 steps), decoder loss: 0.0127, total loss: 0.0127\n",
      "Epoch [106/200], Step [1/6], decoder loss: 0.0133, encoder1_activation l2 loss: 0.0000, total loss: 0.0133\n",
      "Epoch [106/200], Step [2/6], decoder loss: 0.0133, encoder1_activation l2 loss: 0.0000, total loss: 0.0133\n",
      "Epoch [106/200], Step [3/6], decoder loss: 0.0114, encoder1_activation l2 loss: 0.0000, total loss: 0.0114\n",
      "Epoch [106/200], Step [4/6], decoder loss: 0.0139, encoder1_activation l2 loss: 0.0000, total loss: 0.0139\n",
      "Epoch [106/200], Step [5/6], decoder loss: 0.0114, encoder1_activation l2 loss: 0.0000, total loss: 0.0114\n",
      "Epoch [106/200], Step [6/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [106/200] (avg 6 steps), decoder loss: 0.0126, total loss: 0.0126\n",
      "Epoch [107/200], Step [1/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [107/200], Step [2/6], decoder loss: 0.0137, encoder1_activation l2 loss: 0.0000, total loss: 0.0137\n",
      "Epoch [107/200], Step [3/6], decoder loss: 0.0129, encoder1_activation l2 loss: 0.0000, total loss: 0.0129\n",
      "Epoch [107/200], Step [4/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [107/200], Step [5/6], decoder loss: 0.0120, encoder1_activation l2 loss: 0.0000, total loss: 0.0120\n",
      "Epoch [107/200], Step [6/6], decoder loss: 0.0120, encoder1_activation l2 loss: 0.0000, total loss: 0.0120\n",
      "Epoch [107/200] (avg 6 steps), decoder loss: 0.0126, total loss: 0.0126\n",
      "Epoch [108/200], Step [1/6], decoder loss: 0.0129, encoder1_activation l2 loss: 0.0000, total loss: 0.0129\n",
      "Epoch [108/200], Step [2/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [108/200], Step [3/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [108/200], Step [4/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [108/200], Step [5/6], decoder loss: 0.0118, encoder1_activation l2 loss: 0.0000, total loss: 0.0118\n",
      "Epoch [108/200], Step [6/6], decoder loss: 0.0137, encoder1_activation l2 loss: 0.0000, total loss: 0.0137\n",
      "Epoch [108/200] (avg 6 steps), decoder loss: 0.0127, total loss: 0.0127\n",
      "Epoch [109/200], Step [1/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [109/200], Step [2/6], decoder loss: 0.0137, encoder1_activation l2 loss: 0.0000, total loss: 0.0137\n",
      "Epoch [109/200], Step [3/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [109/200], Step [4/6], decoder loss: 0.0116, encoder1_activation l2 loss: 0.0000, total loss: 0.0116\n",
      "Epoch [109/200], Step [5/6], decoder loss: 0.0130, encoder1_activation l2 loss: 0.0000, total loss: 0.0130\n",
      "Epoch [109/200], Step [6/6], decoder loss: 0.0120, encoder1_activation l2 loss: 0.0000, total loss: 0.0120\n",
      "Epoch [109/200] (avg 6 steps), decoder loss: 0.0125, total loss: 0.0125\n",
      "Epoch [110/200], Step [1/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [110/200], Step [2/6], decoder loss: 0.0120, encoder1_activation l2 loss: 0.0000, total loss: 0.0120\n",
      "Epoch [110/200], Step [3/6], decoder loss: 0.0128, encoder1_activation l2 loss: 0.0000, total loss: 0.0128\n",
      "Epoch [110/200], Step [4/6], decoder loss: 0.0118, encoder1_activation l2 loss: 0.0000, total loss: 0.0118\n",
      "Epoch [110/200], Step [5/6], decoder loss: 0.0128, encoder1_activation l2 loss: 0.0000, total loss: 0.0128\n",
      "Epoch [110/200], Step [6/6], decoder loss: 0.0142, encoder1_activation l2 loss: 0.0000, total loss: 0.0142\n",
      "Epoch [110/200] (avg 6 steps), decoder loss: 0.0127, total loss: 0.0127\n",
      "Epoch [111/200], Step [1/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [111/200], Step [2/6], decoder loss: 0.0120, encoder1_activation l2 loss: 0.0000, total loss: 0.0120\n",
      "Epoch [111/200], Step [3/6], decoder loss: 0.0110, encoder1_activation l2 loss: 0.0000, total loss: 0.0110\n",
      "Epoch [111/200], Step [4/6], decoder loss: 0.0143, encoder1_activation l2 loss: 0.0000, total loss: 0.0143\n",
      "Epoch [111/200], Step [5/6], decoder loss: 0.0128, encoder1_activation l2 loss: 0.0000, total loss: 0.0128\n",
      "Epoch [111/200], Step [6/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [111/200] (avg 6 steps), decoder loss: 0.0125, total loss: 0.0125\n",
      "Epoch [112/200], Step [1/6], decoder loss: 0.0138, encoder1_activation l2 loss: 0.0000, total loss: 0.0138\n",
      "Epoch [112/200], Step [2/6], decoder loss: 0.0139, encoder1_activation l2 loss: 0.0000, total loss: 0.0139\n",
      "Epoch [112/200], Step [3/6], decoder loss: 0.0121, encoder1_activation l2 loss: 0.0000, total loss: 0.0121\n",
      "Epoch [112/200], Step [4/6], decoder loss: 0.0128, encoder1_activation l2 loss: 0.0000, total loss: 0.0128\n",
      "Epoch [112/200], Step [5/6], decoder loss: 0.0109, encoder1_activation l2 loss: 0.0000, total loss: 0.0109\n",
      "Epoch [112/200], Step [6/6], decoder loss: 0.0111, encoder1_activation l2 loss: 0.0000, total loss: 0.0111\n",
      "Epoch [112/200] (avg 6 steps), decoder loss: 0.0124, total loss: 0.0124\n",
      "Epoch [113/200], Step [1/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [113/200], Step [2/6], decoder loss: 0.0131, encoder1_activation l2 loss: 0.0000, total loss: 0.0131\n",
      "Epoch [113/200], Step [3/6], decoder loss: 0.0132, encoder1_activation l2 loss: 0.0000, total loss: 0.0132\n",
      "Epoch [113/200], Step [4/6], decoder loss: 0.0130, encoder1_activation l2 loss: 0.0000, total loss: 0.0130\n",
      "Epoch [113/200], Step [5/6], decoder loss: 0.0115, encoder1_activation l2 loss: 0.0000, total loss: 0.0115\n",
      "Epoch [113/200], Step [6/6], decoder loss: 0.0119, encoder1_activation l2 loss: 0.0000, total loss: 0.0119\n",
      "Epoch [113/200] (avg 6 steps), decoder loss: 0.0125, total loss: 0.0125\n",
      "Epoch [114/200], Step [1/6], decoder loss: 0.0121, encoder1_activation l2 loss: 0.0000, total loss: 0.0121\n",
      "Epoch [114/200], Step [2/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [114/200], Step [3/6], decoder loss: 0.0112, encoder1_activation l2 loss: 0.0000, total loss: 0.0112\n",
      "Epoch [114/200], Step [4/6], decoder loss: 0.0145, encoder1_activation l2 loss: 0.0000, total loss: 0.0145\n",
      "Epoch [114/200], Step [5/6], decoder loss: 0.0142, encoder1_activation l2 loss: 0.0000, total loss: 0.0143\n",
      "Epoch [114/200], Step [6/6], decoder loss: 0.0090, encoder1_activation l2 loss: 0.0000, total loss: 0.0090\n",
      "Epoch [114/200] (avg 6 steps), decoder loss: 0.0123, total loss: 0.0123\n",
      "Epoch [115/200], Step [1/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [115/200], Step [2/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [115/200], Step [3/6], decoder loss: 0.0128, encoder1_activation l2 loss: 0.0000, total loss: 0.0128\n",
      "Epoch [115/200], Step [4/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [115/200], Step [5/6], decoder loss: 0.0119, encoder1_activation l2 loss: 0.0000, total loss: 0.0119\n",
      "Epoch [115/200], Step [6/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [115/200] (avg 6 steps), decoder loss: 0.0125, total loss: 0.0125\n",
      "Epoch [116/200], Step [1/6], decoder loss: 0.0137, encoder1_activation l2 loss: 0.0000, total loss: 0.0137\n",
      "Epoch [116/200], Step [2/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [116/200], Step [3/6], decoder loss: 0.0121, encoder1_activation l2 loss: 0.0000, total loss: 0.0121\n",
      "Epoch [116/200], Step [4/6], decoder loss: 0.0113, encoder1_activation l2 loss: 0.0000, total loss: 0.0113\n",
      "Epoch [116/200], Step [5/6], decoder loss: 0.0128, encoder1_activation l2 loss: 0.0000, total loss: 0.0128\n",
      "Epoch [116/200], Step [6/6], decoder loss: 0.0120, encoder1_activation l2 loss: 0.0000, total loss: 0.0120\n",
      "Epoch [116/200] (avg 6 steps), decoder loss: 0.0124, total loss: 0.0124\n",
      "Epoch [117/200], Step [1/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [117/200], Step [2/6], decoder loss: 0.0136, encoder1_activation l2 loss: 0.0000, total loss: 0.0137\n",
      "Epoch [117/200], Step [3/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [117/200], Step [4/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [117/200], Step [5/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [117/200], Step [6/6], decoder loss: 0.0111, encoder1_activation l2 loss: 0.0000, total loss: 0.0111\n",
      "Epoch [117/200] (avg 6 steps), decoder loss: 0.0124, total loss: 0.0124\n",
      "Epoch [118/200], Step [1/6], decoder loss: 0.0126, encoder1_activation l2 loss: 0.0000, total loss: 0.0126\n",
      "Epoch [118/200], Step [2/6], decoder loss: 0.0121, encoder1_activation l2 loss: 0.0000, total loss: 0.0121\n",
      "Epoch [118/200], Step [3/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [118/200], Step [4/6], decoder loss: 0.0118, encoder1_activation l2 loss: 0.0000, total loss: 0.0118\n",
      "Epoch [118/200], Step [5/6], decoder loss: 0.0129, encoder1_activation l2 loss: 0.0000, total loss: 0.0129\n",
      "Epoch [118/200], Step [6/6], decoder loss: 0.0129, encoder1_activation l2 loss: 0.0000, total loss: 0.0129\n",
      "Epoch [118/200] (avg 6 steps), decoder loss: 0.0125, total loss: 0.0125\n",
      "Epoch [119/200], Step [1/6], decoder loss: 0.0116, encoder1_activation l2 loss: 0.0000, total loss: 0.0116\n",
      "Epoch [119/200], Step [2/6], decoder loss: 0.0141, encoder1_activation l2 loss: 0.0000, total loss: 0.0141\n",
      "Epoch [119/200], Step [3/6], decoder loss: 0.0121, encoder1_activation l2 loss: 0.0000, total loss: 0.0121\n",
      "Epoch [119/200], Step [4/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [119/200], Step [5/6], decoder loss: 0.0126, encoder1_activation l2 loss: 0.0000, total loss: 0.0126\n",
      "Epoch [119/200], Step [6/6], decoder loss: 0.0113, encoder1_activation l2 loss: 0.0000, total loss: 0.0113\n",
      "Epoch [119/200] (avg 6 steps), decoder loss: 0.0124, total loss: 0.0124\n",
      "Epoch [120/200], Step [1/6], decoder loss: 0.0119, encoder1_activation l2 loss: 0.0000, total loss: 0.0119\n",
      "Epoch [120/200], Step [2/6], decoder loss: 0.0130, encoder1_activation l2 loss: 0.0000, total loss: 0.0130\n",
      "Epoch [120/200], Step [3/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [120/200], Step [4/6], decoder loss: 0.0118, encoder1_activation l2 loss: 0.0000, total loss: 0.0118\n",
      "Epoch [120/200], Step [5/6], decoder loss: 0.0135, encoder1_activation l2 loss: 0.0000, total loss: 0.0135\n",
      "Epoch [120/200], Step [6/6], decoder loss: 0.0117, encoder1_activation l2 loss: 0.0000, total loss: 0.0117\n",
      "Epoch [120/200] (avg 6 steps), decoder loss: 0.0124, total loss: 0.0124\n",
      "Epoch [121/200], Step [1/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [121/200], Step [2/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [121/200], Step [3/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [121/200], Step [4/6], decoder loss: 0.0117, encoder1_activation l2 loss: 0.0000, total loss: 0.0117\n",
      "Epoch [121/200], Step [5/6], decoder loss: 0.0118, encoder1_activation l2 loss: 0.0000, total loss: 0.0118\n",
      "Epoch [121/200], Step [6/6], decoder loss: 0.0135, encoder1_activation l2 loss: 0.0000, total loss: 0.0135\n",
      "Epoch [121/200] (avg 6 steps), decoder loss: 0.0125, total loss: 0.0125\n",
      "Epoch [122/200], Step [1/6], decoder loss: 0.0128, encoder1_activation l2 loss: 0.0000, total loss: 0.0128\n",
      "Epoch [122/200], Step [2/6], decoder loss: 0.0133, encoder1_activation l2 loss: 0.0000, total loss: 0.0133\n",
      "Epoch [122/200], Step [3/6], decoder loss: 0.0132, encoder1_activation l2 loss: 0.0000, total loss: 0.0132\n",
      "Epoch [122/200], Step [4/6], decoder loss: 0.0111, encoder1_activation l2 loss: 0.0000, total loss: 0.0111\n",
      "Epoch [122/200], Step [5/6], decoder loss: 0.0116, encoder1_activation l2 loss: 0.0000, total loss: 0.0116\n",
      "Epoch [122/200], Step [6/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [122/200] (avg 6 steps), decoder loss: 0.0124, total loss: 0.0124\n",
      "Epoch [123/200], Step [1/6], decoder loss: 0.0116, encoder1_activation l2 loss: 0.0000, total loss: 0.0116\n",
      "Epoch [123/200], Step [2/6], decoder loss: 0.0133, encoder1_activation l2 loss: 0.0000, total loss: 0.0134\n",
      "Epoch [123/200], Step [3/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [123/200], Step [4/6], decoder loss: 0.0108, encoder1_activation l2 loss: 0.0000, total loss: 0.0108\n",
      "Epoch [123/200], Step [5/6], decoder loss: 0.0132, encoder1_activation l2 loss: 0.0000, total loss: 0.0132\n",
      "Epoch [123/200], Step [6/6], decoder loss: 0.0136, encoder1_activation l2 loss: 0.0000, total loss: 0.0136\n",
      "Epoch [123/200] (avg 6 steps), decoder loss: 0.0125, total loss: 0.0125\n",
      "Epoch [124/200], Step [1/6], decoder loss: 0.0119, encoder1_activation l2 loss: 0.0000, total loss: 0.0119\n",
      "Epoch [124/200], Step [2/6], decoder loss: 0.0121, encoder1_activation l2 loss: 0.0000, total loss: 0.0121\n",
      "Epoch [124/200], Step [3/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [124/200], Step [4/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [124/200], Step [5/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [124/200], Step [6/6], decoder loss: 0.0133, encoder1_activation l2 loss: 0.0000, total loss: 0.0133\n",
      "Epoch [124/200] (avg 6 steps), decoder loss: 0.0124, total loss: 0.0124\n",
      "Epoch [125/200], Step [1/6], decoder loss: 0.0117, encoder1_activation l2 loss: 0.0000, total loss: 0.0117\n",
      "Epoch [125/200], Step [2/6], decoder loss: 0.0135, encoder1_activation l2 loss: 0.0000, total loss: 0.0135\n",
      "Epoch [125/200], Step [3/6], decoder loss: 0.0136, encoder1_activation l2 loss: 0.0000, total loss: 0.0136\n",
      "Epoch [125/200], Step [4/6], decoder loss: 0.0120, encoder1_activation l2 loss: 0.0000, total loss: 0.0120\n",
      "Epoch [125/200], Step [5/6], decoder loss: 0.0118, encoder1_activation l2 loss: 0.0000, total loss: 0.0118\n",
      "Epoch [125/200], Step [6/6], decoder loss: 0.0110, encoder1_activation l2 loss: 0.0000, total loss: 0.0110\n",
      "Epoch [125/200] (avg 6 steps), decoder loss: 0.0123, total loss: 0.0123\n",
      "Epoch [126/200], Step [1/6], decoder loss: 0.0120, encoder1_activation l2 loss: 0.0000, total loss: 0.0120\n",
      "Epoch [126/200], Step [2/6], decoder loss: 0.0137, encoder1_activation l2 loss: 0.0000, total loss: 0.0137\n",
      "Epoch [126/200], Step [3/6], decoder loss: 0.0115, encoder1_activation l2 loss: 0.0000, total loss: 0.0115\n",
      "Epoch [126/200], Step [4/6], decoder loss: 0.0130, encoder1_activation l2 loss: 0.0000, total loss: 0.0130\n",
      "Epoch [126/200], Step [5/6], decoder loss: 0.0119, encoder1_activation l2 loss: 0.0000, total loss: 0.0119\n",
      "Epoch [126/200], Step [6/6], decoder loss: 0.0117, encoder1_activation l2 loss: 0.0000, total loss: 0.0117\n",
      "Epoch [126/200] (avg 6 steps), decoder loss: 0.0123, total loss: 0.0123\n",
      "Epoch [127/200], Step [1/6], decoder loss: 0.0104, encoder1_activation l2 loss: 0.0000, total loss: 0.0104\n",
      "Epoch [127/200], Step [2/6], decoder loss: 0.0132, encoder1_activation l2 loss: 0.0000, total loss: 0.0132\n",
      "Epoch [127/200], Step [3/6], decoder loss: 0.0130, encoder1_activation l2 loss: 0.0000, total loss: 0.0130\n",
      "Epoch [127/200], Step [4/6], decoder loss: 0.0130, encoder1_activation l2 loss: 0.0000, total loss: 0.0130\n",
      "Epoch [127/200], Step [5/6], decoder loss: 0.0119, encoder1_activation l2 loss: 0.0000, total loss: 0.0119\n",
      "Epoch [127/200], Step [6/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [127/200] (avg 6 steps), decoder loss: 0.0124, total loss: 0.0124\n",
      "Epoch [128/200], Step [1/6], decoder loss: 0.0119, encoder1_activation l2 loss: 0.0000, total loss: 0.0119\n",
      "Epoch [128/200], Step [2/6], decoder loss: 0.0139, encoder1_activation l2 loss: 0.0000, total loss: 0.0139\n",
      "Epoch [128/200], Step [3/6], decoder loss: 0.0116, encoder1_activation l2 loss: 0.0000, total loss: 0.0116\n",
      "Epoch [128/200], Step [4/6], decoder loss: 0.0120, encoder1_activation l2 loss: 0.0000, total loss: 0.0120\n",
      "Epoch [128/200], Step [5/6], decoder loss: 0.0121, encoder1_activation l2 loss: 0.0000, total loss: 0.0121\n",
      "Epoch [128/200], Step [6/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [128/200] (avg 6 steps), decoder loss: 0.0123, total loss: 0.0123\n",
      "Epoch [129/200], Step [1/6], decoder loss: 0.0141, encoder1_activation l2 loss: 0.0000, total loss: 0.0141\n",
      "Epoch [129/200], Step [2/6], decoder loss: 0.0106, encoder1_activation l2 loss: 0.0000, total loss: 0.0106\n",
      "Epoch [129/200], Step [3/6], decoder loss: 0.0117, encoder1_activation l2 loss: 0.0000, total loss: 0.0117\n",
      "Epoch [129/200], Step [4/6], decoder loss: 0.0134, encoder1_activation l2 loss: 0.0000, total loss: 0.0134\n",
      "Epoch [129/200], Step [5/6], decoder loss: 0.0109, encoder1_activation l2 loss: 0.0000, total loss: 0.0109\n",
      "Epoch [129/200], Step [6/6], decoder loss: 0.0137, encoder1_activation l2 loss: 0.0000, total loss: 0.0137\n",
      "Epoch [129/200] (avg 6 steps), decoder loss: 0.0124, total loss: 0.0124\n",
      "Epoch [130/200], Step [1/6], decoder loss: 0.0143, encoder1_activation l2 loss: 0.0000, total loss: 0.0143\n",
      "Epoch [130/200], Step [2/6], decoder loss: 0.0114, encoder1_activation l2 loss: 0.0000, total loss: 0.0114\n",
      "Epoch [130/200], Step [3/6], decoder loss: 0.0109, encoder1_activation l2 loss: 0.0000, total loss: 0.0109\n",
      "Epoch [130/200], Step [4/6], decoder loss: 0.0126, encoder1_activation l2 loss: 0.0000, total loss: 0.0126\n",
      "Epoch [130/200], Step [5/6], decoder loss: 0.0121, encoder1_activation l2 loss: 0.0000, total loss: 0.0121\n",
      "Epoch [130/200], Step [6/6], decoder loss: 0.0128, encoder1_activation l2 loss: 0.0000, total loss: 0.0128\n",
      "Epoch [130/200] (avg 6 steps), decoder loss: 0.0123, total loss: 0.0123\n",
      "Epoch [131/200], Step [1/6], decoder loss: 0.0119, encoder1_activation l2 loss: 0.0000, total loss: 0.0119\n",
      "Epoch [131/200], Step [2/6], decoder loss: 0.0120, encoder1_activation l2 loss: 0.0000, total loss: 0.0120\n",
      "Epoch [131/200], Step [3/6], decoder loss: 0.0115, encoder1_activation l2 loss: 0.0000, total loss: 0.0115\n",
      "Epoch [131/200], Step [4/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [131/200], Step [5/6], decoder loss: 0.0144, encoder1_activation l2 loss: 0.0000, total loss: 0.0144\n",
      "Epoch [131/200], Step [6/6], decoder loss: 0.0114, encoder1_activation l2 loss: 0.0000, total loss: 0.0114\n",
      "Epoch [131/200] (avg 6 steps), decoder loss: 0.0122, total loss: 0.0122\n",
      "Epoch [132/200], Step [1/6], decoder loss: 0.0116, encoder1_activation l2 loss: 0.0000, total loss: 0.0116\n",
      "Epoch [132/200], Step [2/6], decoder loss: 0.0128, encoder1_activation l2 loss: 0.0000, total loss: 0.0128\n",
      "Epoch [132/200], Step [3/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [132/200], Step [4/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [132/200], Step [5/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [132/200], Step [6/6], decoder loss: 0.0118, encoder1_activation l2 loss: 0.0000, total loss: 0.0118\n",
      "Epoch [132/200] (avg 6 steps), decoder loss: 0.0123, total loss: 0.0123\n",
      "Epoch [133/200], Step [1/6], decoder loss: 0.0120, encoder1_activation l2 loss: 0.0000, total loss: 0.0120\n",
      "Epoch [133/200], Step [2/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [133/200], Step [3/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [133/200], Step [4/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [133/200], Step [5/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [133/200], Step [6/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [133/200] (avg 6 steps), decoder loss: 0.0123, total loss: 0.0123\n",
      "Epoch [134/200], Step [1/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [134/200], Step [2/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [134/200], Step [3/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [134/200], Step [4/6], decoder loss: 0.0114, encoder1_activation l2 loss: 0.0000, total loss: 0.0114\n",
      "Epoch [134/200], Step [5/6], decoder loss: 0.0133, encoder1_activation l2 loss: 0.0000, total loss: 0.0133\n",
      "Epoch [134/200], Step [6/6], decoder loss: 0.0116, encoder1_activation l2 loss: 0.0000, total loss: 0.0116\n",
      "Epoch [134/200] (avg 6 steps), decoder loss: 0.0122, total loss: 0.0122\n",
      "Epoch [135/200], Step [1/6], decoder loss: 0.0107, encoder1_activation l2 loss: 0.0000, total loss: 0.0107\n",
      "Epoch [135/200], Step [2/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [135/200], Step [3/6], decoder loss: 0.0117, encoder1_activation l2 loss: 0.0000, total loss: 0.0117\n",
      "Epoch [135/200], Step [4/6], decoder loss: 0.0143, encoder1_activation l2 loss: 0.0000, total loss: 0.0143\n",
      "Epoch [135/200], Step [5/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [135/200], Step [6/6], decoder loss: 0.0121, encoder1_activation l2 loss: 0.0000, total loss: 0.0121\n",
      "Epoch [135/200] (avg 6 steps), decoder loss: 0.0122, total loss: 0.0122\n",
      "Epoch [136/200], Step [1/6], decoder loss: 0.0120, encoder1_activation l2 loss: 0.0000, total loss: 0.0120\n",
      "Epoch [136/200], Step [2/6], decoder loss: 0.0132, encoder1_activation l2 loss: 0.0000, total loss: 0.0132\n",
      "Epoch [136/200], Step [3/6], decoder loss: 0.0118, encoder1_activation l2 loss: 0.0000, total loss: 0.0118\n",
      "Epoch [136/200], Step [4/6], decoder loss: 0.0109, encoder1_activation l2 loss: 0.0000, total loss: 0.0109\n",
      "Epoch [136/200], Step [5/6], decoder loss: 0.0126, encoder1_activation l2 loss: 0.0000, total loss: 0.0126\n",
      "Epoch [136/200], Step [6/6], decoder loss: 0.0134, encoder1_activation l2 loss: 0.0000, total loss: 0.0134\n",
      "Epoch [136/200] (avg 6 steps), decoder loss: 0.0123, total loss: 0.0123\n",
      "Epoch [137/200], Step [1/6], decoder loss: 0.0131, encoder1_activation l2 loss: 0.0000, total loss: 0.0131\n",
      "Epoch [137/200], Step [2/6], decoder loss: 0.0109, encoder1_activation l2 loss: 0.0000, total loss: 0.0109\n",
      "Epoch [137/200], Step [3/6], decoder loss: 0.0140, encoder1_activation l2 loss: 0.0000, total loss: 0.0140\n",
      "Epoch [137/200], Step [4/6], decoder loss: 0.0107, encoder1_activation l2 loss: 0.0000, total loss: 0.0107\n",
      "Epoch [137/200], Step [5/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [137/200], Step [6/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [137/200] (avg 6 steps), decoder loss: 0.0123, total loss: 0.0123\n",
      "Epoch [138/200], Step [1/6], decoder loss: 0.0132, encoder1_activation l2 loss: 0.0000, total loss: 0.0132\n",
      "Epoch [138/200], Step [2/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [138/200], Step [3/6], decoder loss: 0.0117, encoder1_activation l2 loss: 0.0000, total loss: 0.0117\n",
      "Epoch [138/200], Step [4/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [138/200], Step [5/6], decoder loss: 0.0115, encoder1_activation l2 loss: 0.0000, total loss: 0.0115\n",
      "Epoch [138/200], Step [6/6], decoder loss: 0.0121, encoder1_activation l2 loss: 0.0000, total loss: 0.0121\n",
      "Epoch [138/200] (avg 6 steps), decoder loss: 0.0122, total loss: 0.0122\n",
      "Epoch [139/200], Step [1/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [139/200], Step [2/6], decoder loss: 0.0128, encoder1_activation l2 loss: 0.0000, total loss: 0.0128\n",
      "Epoch [139/200], Step [3/6], decoder loss: 0.0117, encoder1_activation l2 loss: 0.0000, total loss: 0.0117\n",
      "Epoch [139/200], Step [4/6], decoder loss: 0.0111, encoder1_activation l2 loss: 0.0000, total loss: 0.0111\n",
      "Epoch [139/200], Step [5/6], decoder loss: 0.0130, encoder1_activation l2 loss: 0.0000, total loss: 0.0130\n",
      "Epoch [139/200], Step [6/6], decoder loss: 0.0126, encoder1_activation l2 loss: 0.0000, total loss: 0.0126\n",
      "Epoch [139/200] (avg 6 steps), decoder loss: 0.0122, total loss: 0.0123\n",
      "Epoch [140/200], Step [1/6], decoder loss: 0.0149, encoder1_activation l2 loss: 0.0000, total loss: 0.0149\n",
      "Epoch [140/200], Step [2/6], decoder loss: 0.0107, encoder1_activation l2 loss: 0.0000, total loss: 0.0107\n",
      "Epoch [140/200], Step [3/6], decoder loss: 0.0114, encoder1_activation l2 loss: 0.0000, total loss: 0.0114\n",
      "Epoch [140/200], Step [4/6], decoder loss: 0.0106, encoder1_activation l2 loss: 0.0000, total loss: 0.0106\n",
      "Epoch [140/200], Step [5/6], decoder loss: 0.0130, encoder1_activation l2 loss: 0.0000, total loss: 0.0130\n",
      "Epoch [140/200], Step [6/6], decoder loss: 0.0132, encoder1_activation l2 loss: 0.0000, total loss: 0.0132\n",
      "Epoch [140/200] (avg 6 steps), decoder loss: 0.0123, total loss: 0.0123\n",
      "Epoch [141/200], Step [1/6], decoder loss: 0.0141, encoder1_activation l2 loss: 0.0000, total loss: 0.0141\n",
      "Epoch [141/200], Step [2/6], decoder loss: 0.0135, encoder1_activation l2 loss: 0.0000, total loss: 0.0135\n",
      "Epoch [141/200], Step [3/6], decoder loss: 0.0101, encoder1_activation l2 loss: 0.0000, total loss: 0.0101\n",
      "Epoch [141/200], Step [4/6], decoder loss: 0.0112, encoder1_activation l2 loss: 0.0000, total loss: 0.0112\n",
      "Epoch [141/200], Step [5/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [141/200], Step [6/6], decoder loss: 0.0113, encoder1_activation l2 loss: 0.0000, total loss: 0.0113\n",
      "Epoch [141/200] (avg 6 steps), decoder loss: 0.0121, total loss: 0.0122\n",
      "Epoch [142/200], Step [1/6], decoder loss: 0.0120, encoder1_activation l2 loss: 0.0000, total loss: 0.0120\n",
      "Epoch [142/200], Step [2/6], decoder loss: 0.0115, encoder1_activation l2 loss: 0.0000, total loss: 0.0115\n",
      "Epoch [142/200], Step [3/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [142/200], Step [4/6], decoder loss: 0.0132, encoder1_activation l2 loss: 0.0000, total loss: 0.0132\n",
      "Epoch [142/200], Step [5/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [142/200], Step [6/6], decoder loss: 0.0104, encoder1_activation l2 loss: 0.0000, total loss: 0.0104\n",
      "Epoch [142/200] (avg 6 steps), decoder loss: 0.0121, total loss: 0.0121\n",
      "Epoch [143/200], Step [1/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [143/200], Step [2/6], decoder loss: 0.0110, encoder1_activation l2 loss: 0.0000, total loss: 0.0110\n",
      "Epoch [143/200], Step [3/6], decoder loss: 0.0131, encoder1_activation l2 loss: 0.0000, total loss: 0.0131\n",
      "Epoch [143/200], Step [4/6], decoder loss: 0.0118, encoder1_activation l2 loss: 0.0000, total loss: 0.0118\n",
      "Epoch [143/200], Step [5/6], decoder loss: 0.0117, encoder1_activation l2 loss: 0.0000, total loss: 0.0117\n",
      "Epoch [143/200], Step [6/6], decoder loss: 0.0140, encoder1_activation l2 loss: 0.0000, total loss: 0.0140\n",
      "Epoch [143/200] (avg 6 steps), decoder loss: 0.0123, total loss: 0.0123\n",
      "Epoch [144/200], Step [1/6], decoder loss: 0.0120, encoder1_activation l2 loss: 0.0000, total loss: 0.0120\n",
      "Epoch [144/200], Step [2/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [144/200], Step [3/6], decoder loss: 0.0144, encoder1_activation l2 loss: 0.0000, total loss: 0.0144\n",
      "Epoch [144/200], Step [4/6], decoder loss: 0.0111, encoder1_activation l2 loss: 0.0000, total loss: 0.0111\n",
      "Epoch [144/200], Step [5/6], decoder loss: 0.0117, encoder1_activation l2 loss: 0.0000, total loss: 0.0117\n",
      "Epoch [144/200], Step [6/6], decoder loss: 0.0114, encoder1_activation l2 loss: 0.0000, total loss: 0.0114\n",
      "Epoch [144/200] (avg 6 steps), decoder loss: 0.0121, total loss: 0.0121\n",
      "Epoch [145/200], Step [1/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [145/200], Step [2/6], decoder loss: 0.0103, encoder1_activation l2 loss: 0.0000, total loss: 0.0103\n",
      "Epoch [145/200], Step [3/6], decoder loss: 0.0130, encoder1_activation l2 loss: 0.0000, total loss: 0.0130\n",
      "Epoch [145/200], Step [4/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [145/200], Step [5/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [145/200], Step [6/6], decoder loss: 0.0134, encoder1_activation l2 loss: 0.0000, total loss: 0.0134\n",
      "Epoch [145/200] (avg 6 steps), decoder loss: 0.0123, total loss: 0.0123\n",
      "Epoch [146/200], Step [1/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [146/200], Step [2/6], decoder loss: 0.0102, encoder1_activation l2 loss: 0.0000, total loss: 0.0102\n",
      "Epoch [146/200], Step [3/6], decoder loss: 0.0139, encoder1_activation l2 loss: 0.0000, total loss: 0.0139\n",
      "Epoch [146/200], Step [4/6], decoder loss: 0.0131, encoder1_activation l2 loss: 0.0000, total loss: 0.0131\n",
      "Epoch [146/200], Step [5/6], decoder loss: 0.0116, encoder1_activation l2 loss: 0.0000, total loss: 0.0116\n",
      "Epoch [146/200], Step [6/6], decoder loss: 0.0116, encoder1_activation l2 loss: 0.0000, total loss: 0.0116\n",
      "Epoch [146/200] (avg 6 steps), decoder loss: 0.0121, total loss: 0.0121\n",
      "Epoch [147/200], Step [1/6], decoder loss: 0.0112, encoder1_activation l2 loss: 0.0000, total loss: 0.0112\n",
      "Epoch [147/200], Step [2/6], decoder loss: 0.0132, encoder1_activation l2 loss: 0.0000, total loss: 0.0132\n",
      "Epoch [147/200], Step [3/6], decoder loss: 0.0110, encoder1_activation l2 loss: 0.0000, total loss: 0.0110\n",
      "Epoch [147/200], Step [4/6], decoder loss: 0.0118, encoder1_activation l2 loss: 0.0000, total loss: 0.0118\n",
      "Epoch [147/200], Step [5/6], decoder loss: 0.0126, encoder1_activation l2 loss: 0.0000, total loss: 0.0126\n",
      "Epoch [147/200], Step [6/6], decoder loss: 0.0139, encoder1_activation l2 loss: 0.0000, total loss: 0.0139\n",
      "Epoch [147/200] (avg 6 steps), decoder loss: 0.0123, total loss: 0.0123\n",
      "Epoch [148/200], Step [1/6], decoder loss: 0.0134, encoder1_activation l2 loss: 0.0000, total loss: 0.0134\n",
      "Epoch [148/200], Step [2/6], decoder loss: 0.0120, encoder1_activation l2 loss: 0.0000, total loss: 0.0120\n",
      "Epoch [148/200], Step [3/6], decoder loss: 0.0112, encoder1_activation l2 loss: 0.0000, total loss: 0.0112\n",
      "Epoch [148/200], Step [4/6], decoder loss: 0.0121, encoder1_activation l2 loss: 0.0000, total loss: 0.0121\n",
      "Epoch [148/200], Step [5/6], decoder loss: 0.0119, encoder1_activation l2 loss: 0.0000, total loss: 0.0119\n",
      "Epoch [148/200], Step [6/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [148/200] (avg 6 steps), decoder loss: 0.0122, total loss: 0.0122\n",
      "Epoch [149/200], Step [1/6], decoder loss: 0.0116, encoder1_activation l2 loss: 0.0000, total loss: 0.0116\n",
      "Epoch [149/200], Step [2/6], decoder loss: 0.0113, encoder1_activation l2 loss: 0.0000, total loss: 0.0113\n",
      "Epoch [149/200], Step [3/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [149/200], Step [4/6], decoder loss: 0.0119, encoder1_activation l2 loss: 0.0000, total loss: 0.0119\n",
      "Epoch [149/200], Step [5/6], decoder loss: 0.0130, encoder1_activation l2 loss: 0.0000, total loss: 0.0130\n",
      "Epoch [149/200], Step [6/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [149/200] (avg 6 steps), decoder loss: 0.0122, total loss: 0.0122\n",
      "Epoch [150/200], Step [1/6], decoder loss: 0.0121, encoder1_activation l2 loss: 0.0000, total loss: 0.0121\n",
      "Epoch [150/200], Step [2/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [150/200], Step [3/6], decoder loss: 0.0126, encoder1_activation l2 loss: 0.0000, total loss: 0.0126\n",
      "Epoch [150/200], Step [4/6], decoder loss: 0.0120, encoder1_activation l2 loss: 0.0000, total loss: 0.0120\n",
      "Epoch [150/200], Step [5/6], decoder loss: 0.0115, encoder1_activation l2 loss: 0.0000, total loss: 0.0115\n",
      "Epoch [150/200], Step [6/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [150/200] (avg 6 steps), decoder loss: 0.0121, total loss: 0.0121\n",
      "Epoch [151/200], Step [1/6], decoder loss: 0.0135, encoder1_activation l2 loss: 0.0000, total loss: 0.0135\n",
      "Epoch [151/200], Step [2/6], decoder loss: 0.0115, encoder1_activation l2 loss: 0.0000, total loss: 0.0115\n",
      "Epoch [151/200], Step [3/6], decoder loss: 0.0111, encoder1_activation l2 loss: 0.0000, total loss: 0.0111\n",
      "Epoch [151/200], Step [4/6], decoder loss: 0.0132, encoder1_activation l2 loss: 0.0000, total loss: 0.0132\n",
      "Epoch [151/200], Step [5/6], decoder loss: 0.0121, encoder1_activation l2 loss: 0.0000, total loss: 0.0121\n",
      "Epoch [151/200], Step [6/6], decoder loss: 0.0110, encoder1_activation l2 loss: 0.0000, total loss: 0.0110\n",
      "Epoch [151/200] (avg 6 steps), decoder loss: 0.0121, total loss: 0.0121\n",
      "Epoch [152/200], Step [1/6], decoder loss: 0.0119, encoder1_activation l2 loss: 0.0000, total loss: 0.0119\n",
      "Epoch [152/200], Step [2/6], decoder loss: 0.0119, encoder1_activation l2 loss: 0.0000, total loss: 0.0119\n",
      "Epoch [152/200], Step [3/6], decoder loss: 0.0111, encoder1_activation l2 loss: 0.0000, total loss: 0.0111\n",
      "Epoch [152/200], Step [4/6], decoder loss: 0.0130, encoder1_activation l2 loss: 0.0000, total loss: 0.0130\n",
      "Epoch [152/200], Step [5/6], decoder loss: 0.0132, encoder1_activation l2 loss: 0.0000, total loss: 0.0132\n",
      "Epoch [152/200], Step [6/6], decoder loss: 0.0114, encoder1_activation l2 loss: 0.0000, total loss: 0.0114\n",
      "Epoch [152/200] (avg 6 steps), decoder loss: 0.0121, total loss: 0.0121\n",
      "Epoch [153/200], Step [1/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [153/200], Step [2/6], decoder loss: 0.0126, encoder1_activation l2 loss: 0.0000, total loss: 0.0126\n",
      "Epoch [153/200], Step [3/6], decoder loss: 0.0117, encoder1_activation l2 loss: 0.0000, total loss: 0.0117\n",
      "Epoch [153/200], Step [4/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [153/200], Step [5/6], decoder loss: 0.0117, encoder1_activation l2 loss: 0.0000, total loss: 0.0117\n",
      "Epoch [153/200], Step [6/6], decoder loss: 0.0117, encoder1_activation l2 loss: 0.0000, total loss: 0.0117\n",
      "Epoch [153/200] (avg 6 steps), decoder loss: 0.0121, total loss: 0.0121\n",
      "Epoch [154/200], Step [1/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [154/200], Step [2/6], decoder loss: 0.0132, encoder1_activation l2 loss: 0.0000, total loss: 0.0132\n",
      "Epoch [154/200], Step [3/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [154/200], Step [4/6], decoder loss: 0.0118, encoder1_activation l2 loss: 0.0000, total loss: 0.0118\n",
      "Epoch [154/200], Step [5/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [154/200], Step [6/6], decoder loss: 0.0098, encoder1_activation l2 loss: 0.0000, total loss: 0.0098\n",
      "Epoch [154/200] (avg 6 steps), decoder loss: 0.0120, total loss: 0.0120\n",
      "Epoch [155/200], Step [1/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [155/200], Step [2/6], decoder loss: 0.0119, encoder1_activation l2 loss: 0.0000, total loss: 0.0119\n",
      "Epoch [155/200], Step [3/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [155/200], Step [4/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [155/200], Step [5/6], decoder loss: 0.0120, encoder1_activation l2 loss: 0.0000, total loss: 0.0120\n",
      "Epoch [155/200], Step [6/6], decoder loss: 0.0114, encoder1_activation l2 loss: 0.0000, total loss: 0.0114\n",
      "Epoch [155/200] (avg 6 steps), decoder loss: 0.0121, total loss: 0.0121\n",
      "Epoch [156/200], Step [1/6], decoder loss: 0.0126, encoder1_activation l2 loss: 0.0000, total loss: 0.0126\n",
      "Epoch [156/200], Step [2/6], decoder loss: 0.0116, encoder1_activation l2 loss: 0.0000, total loss: 0.0116\n",
      "Epoch [156/200], Step [3/6], decoder loss: 0.0114, encoder1_activation l2 loss: 0.0000, total loss: 0.0114\n",
      "Epoch [156/200], Step [4/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [156/200], Step [5/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [156/200], Step [6/6], decoder loss: 0.0129, encoder1_activation l2 loss: 0.0000, total loss: 0.0129\n",
      "Epoch [156/200] (avg 6 steps), decoder loss: 0.0122, total loss: 0.0122\n",
      "Epoch [157/200], Step [1/6], decoder loss: 0.0106, encoder1_activation l2 loss: 0.0000, total loss: 0.0106\n",
      "Epoch [157/200], Step [2/6], decoder loss: 0.0128, encoder1_activation l2 loss: 0.0000, total loss: 0.0128\n",
      "Epoch [157/200], Step [3/6], decoder loss: 0.0121, encoder1_activation l2 loss: 0.0000, total loss: 0.0121\n",
      "Epoch [157/200], Step [4/6], decoder loss: 0.0121, encoder1_activation l2 loss: 0.0000, total loss: 0.0121\n",
      "Epoch [157/200], Step [5/6], decoder loss: 0.0116, encoder1_activation l2 loss: 0.0000, total loss: 0.0116\n",
      "Epoch [157/200], Step [6/6], decoder loss: 0.0145, encoder1_activation l2 loss: 0.0000, total loss: 0.0145\n",
      "Epoch [157/200] (avg 6 steps), decoder loss: 0.0123, total loss: 0.0123\n",
      "Epoch [158/200], Step [1/6], decoder loss: 0.0115, encoder1_activation l2 loss: 0.0000, total loss: 0.0115\n",
      "Epoch [158/200], Step [2/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [158/200], Step [3/6], decoder loss: 0.0115, encoder1_activation l2 loss: 0.0000, total loss: 0.0115\n",
      "Epoch [158/200], Step [4/6], decoder loss: 0.0128, encoder1_activation l2 loss: 0.0000, total loss: 0.0128\n",
      "Epoch [158/200], Step [5/6], decoder loss: 0.0110, encoder1_activation l2 loss: 0.0000, total loss: 0.0110\n",
      "Epoch [158/200], Step [6/6], decoder loss: 0.0143, encoder1_activation l2 loss: 0.0000, total loss: 0.0143\n",
      "Epoch [158/200] (avg 6 steps), decoder loss: 0.0122, total loss: 0.0122\n",
      "Epoch [159/200], Step [1/6], decoder loss: 0.0138, encoder1_activation l2 loss: 0.0000, total loss: 0.0138\n",
      "Epoch [159/200], Step [2/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [159/200], Step [3/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [159/200], Step [4/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [159/200], Step [5/6], decoder loss: 0.0099, encoder1_activation l2 loss: 0.0000, total loss: 0.0099\n",
      "Epoch [159/200], Step [6/6], decoder loss: 0.0111, encoder1_activation l2 loss: 0.0000, total loss: 0.0111\n",
      "Epoch [159/200] (avg 6 steps), decoder loss: 0.0120, total loss: 0.0120\n",
      "Epoch [160/200], Step [1/6], decoder loss: 0.0109, encoder1_activation l2 loss: 0.0000, total loss: 0.0109\n",
      "Epoch [160/200], Step [2/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [160/200], Step [3/6], decoder loss: 0.0134, encoder1_activation l2 loss: 0.0000, total loss: 0.0134\n",
      "Epoch [160/200], Step [4/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [160/200], Step [5/6], decoder loss: 0.0115, encoder1_activation l2 loss: 0.0000, total loss: 0.0115\n",
      "Epoch [160/200], Step [6/6], decoder loss: 0.0117, encoder1_activation l2 loss: 0.0000, total loss: 0.0117\n",
      "Epoch [160/200] (avg 6 steps), decoder loss: 0.0121, total loss: 0.0121\n",
      "Epoch [161/200], Step [1/6], decoder loss: 0.0105, encoder1_activation l2 loss: 0.0000, total loss: 0.0105\n",
      "Epoch [161/200], Step [2/6], decoder loss: 0.0133, encoder1_activation l2 loss: 0.0000, total loss: 0.0133\n",
      "Epoch [161/200], Step [3/6], decoder loss: 0.0111, encoder1_activation l2 loss: 0.0000, total loss: 0.0111\n",
      "Epoch [161/200], Step [4/6], decoder loss: 0.0129, encoder1_activation l2 loss: 0.0000, total loss: 0.0129\n",
      "Epoch [161/200], Step [5/6], decoder loss: 0.0139, encoder1_activation l2 loss: 0.0000, total loss: 0.0139\n",
      "Epoch [161/200], Step [6/6], decoder loss: 0.0098, encoder1_activation l2 loss: 0.0000, total loss: 0.0098\n",
      "Epoch [161/200] (avg 6 steps), decoder loss: 0.0119, total loss: 0.0119\n",
      "Epoch [162/200], Step [1/6], decoder loss: 0.0118, encoder1_activation l2 loss: 0.0000, total loss: 0.0118\n",
      "Epoch [162/200], Step [2/6], decoder loss: 0.0144, encoder1_activation l2 loss: 0.0000, total loss: 0.0144\n",
      "Epoch [162/200], Step [3/6], decoder loss: 0.0119, encoder1_activation l2 loss: 0.0000, total loss: 0.0119\n",
      "Epoch [162/200], Step [4/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [162/200], Step [5/6], decoder loss: 0.0108, encoder1_activation l2 loss: 0.0000, total loss: 0.0108\n",
      "Epoch [162/200], Step [6/6], decoder loss: 0.0108, encoder1_activation l2 loss: 0.0000, total loss: 0.0108\n",
      "Epoch [162/200] (avg 6 steps), decoder loss: 0.0120, total loss: 0.0120\n",
      "Epoch [163/200], Step [1/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [163/200], Step [2/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [163/200], Step [3/6], decoder loss: 0.0107, encoder1_activation l2 loss: 0.0000, total loss: 0.0107\n",
      "Epoch [163/200], Step [4/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [163/200], Step [5/6], decoder loss: 0.0126, encoder1_activation l2 loss: 0.0000, total loss: 0.0126\n",
      "Epoch [163/200], Step [6/6], decoder loss: 0.0116, encoder1_activation l2 loss: 0.0000, total loss: 0.0116\n",
      "Epoch [163/200] (avg 6 steps), decoder loss: 0.0120, total loss: 0.0120\n",
      "Epoch [164/200], Step [1/6], decoder loss: 0.0112, encoder1_activation l2 loss: 0.0000, total loss: 0.0112\n",
      "Epoch [164/200], Step [2/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [164/200], Step [3/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [164/200], Step [4/6], decoder loss: 0.0117, encoder1_activation l2 loss: 0.0000, total loss: 0.0117\n",
      "Epoch [164/200], Step [5/6], decoder loss: 0.0134, encoder1_activation l2 loss: 0.0000, total loss: 0.0134\n",
      "Epoch [164/200], Step [6/6], decoder loss: 0.0113, encoder1_activation l2 loss: 0.0000, total loss: 0.0113\n",
      "Epoch [164/200] (avg 6 steps), decoder loss: 0.0120, total loss: 0.0120\n",
      "Epoch [165/200], Step [1/6], decoder loss: 0.0120, encoder1_activation l2 loss: 0.0000, total loss: 0.0120\n",
      "Epoch [165/200], Step [2/6], decoder loss: 0.0107, encoder1_activation l2 loss: 0.0000, total loss: 0.0107\n",
      "Epoch [165/200], Step [3/6], decoder loss: 0.0131, encoder1_activation l2 loss: 0.0000, total loss: 0.0131\n",
      "Epoch [165/200], Step [4/6], decoder loss: 0.0116, encoder1_activation l2 loss: 0.0000, total loss: 0.0116\n",
      "Epoch [165/200], Step [5/6], decoder loss: 0.0113, encoder1_activation l2 loss: 0.0000, total loss: 0.0113\n",
      "Epoch [165/200], Step [6/6], decoder loss: 0.0146, encoder1_activation l2 loss: 0.0000, total loss: 0.0146\n",
      "Epoch [165/200] (avg 6 steps), decoder loss: 0.0122, total loss: 0.0122\n",
      "Epoch [166/200], Step [1/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [166/200], Step [2/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [166/200], Step [3/6], decoder loss: 0.0112, encoder1_activation l2 loss: 0.0000, total loss: 0.0112\n",
      "Epoch [166/200], Step [4/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [166/200], Step [5/6], decoder loss: 0.0116, encoder1_activation l2 loss: 0.0000, total loss: 0.0116\n",
      "Epoch [166/200], Step [6/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [166/200] (avg 6 steps), decoder loss: 0.0121, total loss: 0.0121\n",
      "Epoch [167/200], Step [1/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [167/200], Step [2/6], decoder loss: 0.0113, encoder1_activation l2 loss: 0.0000, total loss: 0.0113\n",
      "Epoch [167/200], Step [3/6], decoder loss: 0.0114, encoder1_activation l2 loss: 0.0000, total loss: 0.0114\n",
      "Epoch [167/200], Step [4/6], decoder loss: 0.0109, encoder1_activation l2 loss: 0.0000, total loss: 0.0109\n",
      "Epoch [167/200], Step [5/6], decoder loss: 0.0120, encoder1_activation l2 loss: 0.0000, total loss: 0.0120\n",
      "Epoch [167/200], Step [6/6], decoder loss: 0.0158, encoder1_activation l2 loss: 0.0000, total loss: 0.0158\n",
      "Epoch [167/200] (avg 6 steps), decoder loss: 0.0123, total loss: 0.0123\n",
      "Epoch [168/200], Step [1/6], decoder loss: 0.0111, encoder1_activation l2 loss: 0.0000, total loss: 0.0111\n",
      "Epoch [168/200], Step [2/6], decoder loss: 0.0108, encoder1_activation l2 loss: 0.0000, total loss: 0.0108\n",
      "Epoch [168/200], Step [3/6], decoder loss: 0.0135, encoder1_activation l2 loss: 0.0000, total loss: 0.0135\n",
      "Epoch [168/200], Step [4/6], decoder loss: 0.0110, encoder1_activation l2 loss: 0.0000, total loss: 0.0110\n",
      "Epoch [168/200], Step [5/6], decoder loss: 0.0144, encoder1_activation l2 loss: 0.0000, total loss: 0.0144\n",
      "Epoch [168/200], Step [6/6], decoder loss: 0.0113, encoder1_activation l2 loss: 0.0000, total loss: 0.0113\n",
      "Epoch [168/200] (avg 6 steps), decoder loss: 0.0120, total loss: 0.0120\n",
      "Epoch [169/200], Step [1/6], decoder loss: 0.0105, encoder1_activation l2 loss: 0.0000, total loss: 0.0105\n",
      "Epoch [169/200], Step [2/6], decoder loss: 0.0130, encoder1_activation l2 loss: 0.0000, total loss: 0.0130\n",
      "Epoch [169/200], Step [3/6], decoder loss: 0.0114, encoder1_activation l2 loss: 0.0000, total loss: 0.0114\n",
      "Epoch [169/200], Step [4/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [169/200], Step [5/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [169/200], Step [6/6], decoder loss: 0.0134, encoder1_activation l2 loss: 0.0000, total loss: 0.0134\n",
      "Epoch [169/200] (avg 6 steps), decoder loss: 0.0121, total loss: 0.0121\n",
      "Epoch [170/200], Step [1/6], decoder loss: 0.0121, encoder1_activation l2 loss: 0.0000, total loss: 0.0121\n",
      "Epoch [170/200], Step [2/6], decoder loss: 0.0116, encoder1_activation l2 loss: 0.0000, total loss: 0.0116\n",
      "Epoch [170/200], Step [3/6], decoder loss: 0.0115, encoder1_activation l2 loss: 0.0000, total loss: 0.0115\n",
      "Epoch [170/200], Step [4/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [170/200], Step [5/6], decoder loss: 0.0119, encoder1_activation l2 loss: 0.0000, total loss: 0.0119\n",
      "Epoch [170/200], Step [6/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [170/200] (avg 6 steps), decoder loss: 0.0121, total loss: 0.0121\n",
      "Epoch [171/200], Step [1/6], decoder loss: 0.0110, encoder1_activation l2 loss: 0.0000, total loss: 0.0110\n",
      "Epoch [171/200], Step [2/6], decoder loss: 0.0121, encoder1_activation l2 loss: 0.0000, total loss: 0.0121\n",
      "Epoch [171/200], Step [3/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [171/200], Step [4/6], decoder loss: 0.0117, encoder1_activation l2 loss: 0.0000, total loss: 0.0117\n",
      "Epoch [171/200], Step [5/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [171/200], Step [6/6], decoder loss: 0.0126, encoder1_activation l2 loss: 0.0000, total loss: 0.0126\n",
      "Epoch [171/200] (avg 6 steps), decoder loss: 0.0121, total loss: 0.0121\n",
      "Epoch [172/200], Step [1/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [172/200], Step [2/6], decoder loss: 0.0121, encoder1_activation l2 loss: 0.0000, total loss: 0.0121\n",
      "Epoch [172/200], Step [3/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [172/200], Step [4/6], decoder loss: 0.0107, encoder1_activation l2 loss: 0.0000, total loss: 0.0107\n",
      "Epoch [172/200], Step [5/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [172/200], Step [6/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [172/200] (avg 6 steps), decoder loss: 0.0121, total loss: 0.0121\n",
      "Epoch [173/200], Step [1/6], decoder loss: 0.0121, encoder1_activation l2 loss: 0.0000, total loss: 0.0121\n",
      "Epoch [173/200], Step [2/6], decoder loss: 0.0120, encoder1_activation l2 loss: 0.0000, total loss: 0.0120\n",
      "Epoch [173/200], Step [3/6], decoder loss: 0.0118, encoder1_activation l2 loss: 0.0000, total loss: 0.0118\n",
      "Epoch [173/200], Step [4/6], decoder loss: 0.0104, encoder1_activation l2 loss: 0.0000, total loss: 0.0104\n",
      "Epoch [173/200], Step [5/6], decoder loss: 0.0137, encoder1_activation l2 loss: 0.0000, total loss: 0.0137\n",
      "Epoch [173/200], Step [6/6], decoder loss: 0.0121, encoder1_activation l2 loss: 0.0000, total loss: 0.0121\n",
      "Epoch [173/200] (avg 6 steps), decoder loss: 0.0120, total loss: 0.0120\n",
      "Epoch [174/200], Step [1/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [174/200], Step [2/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [174/200], Step [3/6], decoder loss: 0.0121, encoder1_activation l2 loss: 0.0000, total loss: 0.0121\n",
      "Epoch [174/200], Step [4/6], decoder loss: 0.0118, encoder1_activation l2 loss: 0.0000, total loss: 0.0118\n",
      "Epoch [174/200], Step [5/6], decoder loss: 0.0119, encoder1_activation l2 loss: 0.0000, total loss: 0.0119\n",
      "Epoch [174/200], Step [6/6], decoder loss: 0.0110, encoder1_activation l2 loss: 0.0000, total loss: 0.0110\n",
      "Epoch [174/200] (avg 6 steps), decoder loss: 0.0120, total loss: 0.0120\n",
      "Epoch [175/200], Step [1/6], decoder loss: 0.0107, encoder1_activation l2 loss: 0.0000, total loss: 0.0107\n",
      "Epoch [175/200], Step [2/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [175/200], Step [3/6], decoder loss: 0.0126, encoder1_activation l2 loss: 0.0000, total loss: 0.0126\n",
      "Epoch [175/200], Step [4/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [175/200], Step [5/6], decoder loss: 0.0118, encoder1_activation l2 loss: 0.0000, total loss: 0.0118\n",
      "Epoch [175/200], Step [6/6], decoder loss: 0.0120, encoder1_activation l2 loss: 0.0000, total loss: 0.0120\n",
      "Epoch [175/200] (avg 6 steps), decoder loss: 0.0120, total loss: 0.0120\n",
      "Epoch [176/200], Step [1/6], decoder loss: 0.0119, encoder1_activation l2 loss: 0.0000, total loss: 0.0119\n",
      "Epoch [176/200], Step [2/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [176/200], Step [3/6], decoder loss: 0.0116, encoder1_activation l2 loss: 0.0000, total loss: 0.0116\n",
      "Epoch [176/200], Step [4/6], decoder loss: 0.0119, encoder1_activation l2 loss: 0.0000, total loss: 0.0119\n",
      "Epoch [176/200], Step [5/6], decoder loss: 0.0112, encoder1_activation l2 loss: 0.0000, total loss: 0.0112\n",
      "Epoch [176/200], Step [6/6], decoder loss: 0.0138, encoder1_activation l2 loss: 0.0000, total loss: 0.0138\n",
      "Epoch [176/200] (avg 6 steps), decoder loss: 0.0121, total loss: 0.0121\n",
      "Epoch [177/200], Step [1/6], decoder loss: 0.0109, encoder1_activation l2 loss: 0.0000, total loss: 0.0109\n",
      "Epoch [177/200], Step [2/6], decoder loss: 0.0131, encoder1_activation l2 loss: 0.0000, total loss: 0.0131\n",
      "Epoch [177/200], Step [3/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [177/200], Step [4/6], decoder loss: 0.0128, encoder1_activation l2 loss: 0.0000, total loss: 0.0128\n",
      "Epoch [177/200], Step [5/6], decoder loss: 0.0114, encoder1_activation l2 loss: 0.0000, total loss: 0.0114\n",
      "Epoch [177/200], Step [6/6], decoder loss: 0.0112, encoder1_activation l2 loss: 0.0000, total loss: 0.0112\n",
      "Epoch [177/200] (avg 6 steps), decoder loss: 0.0119, total loss: 0.0119\n",
      "Epoch [178/200], Step [1/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [178/200], Step [2/6], decoder loss: 0.0111, encoder1_activation l2 loss: 0.0000, total loss: 0.0111\n",
      "Epoch [178/200], Step [3/6], decoder loss: 0.0111, encoder1_activation l2 loss: 0.0000, total loss: 0.0111\n",
      "Epoch [178/200], Step [4/6], decoder loss: 0.0134, encoder1_activation l2 loss: 0.0000, total loss: 0.0134\n",
      "Epoch [178/200], Step [5/6], decoder loss: 0.0107, encoder1_activation l2 loss: 0.0000, total loss: 0.0107\n",
      "Epoch [178/200], Step [6/6], decoder loss: 0.0143, encoder1_activation l2 loss: 0.0000, total loss: 0.0143\n",
      "Epoch [178/200] (avg 6 steps), decoder loss: 0.0122, total loss: 0.0122\n",
      "Epoch [179/200], Step [1/6], decoder loss: 0.0116, encoder1_activation l2 loss: 0.0000, total loss: 0.0116\n",
      "Epoch [179/200], Step [2/6], decoder loss: 0.0126, encoder1_activation l2 loss: 0.0000, total loss: 0.0126\n",
      "Epoch [179/200], Step [3/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [179/200], Step [4/6], decoder loss: 0.0111, encoder1_activation l2 loss: 0.0000, total loss: 0.0111\n",
      "Epoch [179/200], Step [5/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [179/200], Step [6/6], decoder loss: 0.0117, encoder1_activation l2 loss: 0.0000, total loss: 0.0117\n",
      "Epoch [179/200] (avg 6 steps), decoder loss: 0.0120, total loss: 0.0120\n",
      "Epoch [180/200], Step [1/6], decoder loss: 0.0128, encoder1_activation l2 loss: 0.0000, total loss: 0.0128\n",
      "Epoch [180/200], Step [2/6], decoder loss: 0.0114, encoder1_activation l2 loss: 0.0000, total loss: 0.0114\n",
      "Epoch [180/200], Step [3/6], decoder loss: 0.0105, encoder1_activation l2 loss: 0.0000, total loss: 0.0105\n",
      "Epoch [180/200], Step [4/6], decoder loss: 0.0116, encoder1_activation l2 loss: 0.0000, total loss: 0.0116\n",
      "Epoch [180/200], Step [5/6], decoder loss: 0.0133, encoder1_activation l2 loss: 0.0000, total loss: 0.0133\n",
      "Epoch [180/200], Step [6/6], decoder loss: 0.0126, encoder1_activation l2 loss: 0.0000, total loss: 0.0126\n",
      "Epoch [180/200] (avg 6 steps), decoder loss: 0.0120, total loss: 0.0120\n",
      "Epoch [181/200], Step [1/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [181/200], Step [2/6], decoder loss: 0.0128, encoder1_activation l2 loss: 0.0000, total loss: 0.0128\n",
      "Epoch [181/200], Step [3/6], decoder loss: 0.0120, encoder1_activation l2 loss: 0.0000, total loss: 0.0120\n",
      "Epoch [181/200], Step [4/6], decoder loss: 0.0119, encoder1_activation l2 loss: 0.0000, total loss: 0.0119\n",
      "Epoch [181/200], Step [5/6], decoder loss: 0.0110, encoder1_activation l2 loss: 0.0000, total loss: 0.0110\n",
      "Epoch [181/200], Step [6/6], decoder loss: 0.0115, encoder1_activation l2 loss: 0.0000, total loss: 0.0115\n",
      "Epoch [181/200] (avg 6 steps), decoder loss: 0.0120, total loss: 0.0120\n",
      "Epoch [182/200], Step [1/6], decoder loss: 0.0120, encoder1_activation l2 loss: 0.0000, total loss: 0.0120\n",
      "Epoch [182/200], Step [2/6], decoder loss: 0.0121, encoder1_activation l2 loss: 0.0000, total loss: 0.0121\n",
      "Epoch [182/200], Step [3/6], decoder loss: 0.0120, encoder1_activation l2 loss: 0.0000, total loss: 0.0120\n",
      "Epoch [182/200], Step [4/6], decoder loss: 0.0116, encoder1_activation l2 loss: 0.0000, total loss: 0.0116\n",
      "Epoch [182/200], Step [5/6], decoder loss: 0.0112, encoder1_activation l2 loss: 0.0000, total loss: 0.0112\n",
      "Epoch [182/200], Step [6/6], decoder loss: 0.0139, encoder1_activation l2 loss: 0.0000, total loss: 0.0139\n",
      "Epoch [182/200] (avg 6 steps), decoder loss: 0.0121, total loss: 0.0121\n",
      "Epoch [183/200], Step [1/6], decoder loss: 0.0117, encoder1_activation l2 loss: 0.0000, total loss: 0.0117\n",
      "Epoch [183/200], Step [2/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [183/200], Step [3/6], decoder loss: 0.0116, encoder1_activation l2 loss: 0.0000, total loss: 0.0116\n",
      "Epoch [183/200], Step [4/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [183/200], Step [5/6], decoder loss: 0.0107, encoder1_activation l2 loss: 0.0000, total loss: 0.0107\n",
      "Epoch [183/200], Step [6/6], decoder loss: 0.0140, encoder1_activation l2 loss: 0.0000, total loss: 0.0140\n",
      "Epoch [183/200] (avg 6 steps), decoder loss: 0.0121, total loss: 0.0121\n",
      "Epoch [184/200], Step [1/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [184/200], Step [2/6], decoder loss: 0.0131, encoder1_activation l2 loss: 0.0000, total loss: 0.0131\n",
      "Epoch [184/200], Step [3/6], decoder loss: 0.0107, encoder1_activation l2 loss: 0.0000, total loss: 0.0107\n",
      "Epoch [184/200], Step [4/6], decoder loss: 0.0120, encoder1_activation l2 loss: 0.0000, total loss: 0.0120\n",
      "Epoch [184/200], Step [5/6], decoder loss: 0.0120, encoder1_activation l2 loss: 0.0000, total loss: 0.0120\n",
      "Epoch [184/200], Step [6/6], decoder loss: 0.0113, encoder1_activation l2 loss: 0.0000, total loss: 0.0113\n",
      "Epoch [184/200] (avg 6 steps), decoder loss: 0.0119, total loss: 0.0119\n",
      "Epoch [185/200], Step [1/6], decoder loss: 0.0113, encoder1_activation l2 loss: 0.0000, total loss: 0.0113\n",
      "Epoch [185/200], Step [2/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [185/200], Step [3/6], decoder loss: 0.0114, encoder1_activation l2 loss: 0.0000, total loss: 0.0114\n",
      "Epoch [185/200], Step [4/6], decoder loss: 0.0116, encoder1_activation l2 loss: 0.0000, total loss: 0.0116\n",
      "Epoch [185/200], Step [5/6], decoder loss: 0.0130, encoder1_activation l2 loss: 0.0000, total loss: 0.0130\n",
      "Epoch [185/200], Step [6/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [185/200] (avg 6 steps), decoder loss: 0.0120, total loss: 0.0120\n",
      "Epoch [186/200], Step [1/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [186/200], Step [2/6], decoder loss: 0.0115, encoder1_activation l2 loss: 0.0000, total loss: 0.0115\n",
      "Epoch [186/200], Step [3/6], decoder loss: 0.0116, encoder1_activation l2 loss: 0.0000, total loss: 0.0116\n",
      "Epoch [186/200], Step [4/6], decoder loss: 0.0119, encoder1_activation l2 loss: 0.0000, total loss: 0.0119\n",
      "Epoch [186/200], Step [5/6], decoder loss: 0.0120, encoder1_activation l2 loss: 0.0000, total loss: 0.0120\n",
      "Epoch [186/200], Step [6/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [186/200] (avg 6 steps), decoder loss: 0.0120, total loss: 0.0120\n",
      "Epoch [187/200], Step [1/6], decoder loss: 0.0118, encoder1_activation l2 loss: 0.0000, total loss: 0.0118\n",
      "Epoch [187/200], Step [2/6], decoder loss: 0.0107, encoder1_activation l2 loss: 0.0000, total loss: 0.0107\n",
      "Epoch [187/200], Step [3/6], decoder loss: 0.0114, encoder1_activation l2 loss: 0.0000, total loss: 0.0114\n",
      "Epoch [187/200], Step [4/6], decoder loss: 0.0117, encoder1_activation l2 loss: 0.0000, total loss: 0.0117\n",
      "Epoch [187/200], Step [5/6], decoder loss: 0.0135, encoder1_activation l2 loss: 0.0000, total loss: 0.0135\n",
      "Epoch [187/200], Step [6/6], decoder loss: 0.0132, encoder1_activation l2 loss: 0.0000, total loss: 0.0132\n",
      "Epoch [187/200] (avg 6 steps), decoder loss: 0.0121, total loss: 0.0121\n",
      "Epoch [188/200], Step [1/6], decoder loss: 0.0109, encoder1_activation l2 loss: 0.0000, total loss: 0.0109\n",
      "Epoch [188/200], Step [2/6], decoder loss: 0.0114, encoder1_activation l2 loss: 0.0000, total loss: 0.0114\n",
      "Epoch [188/200], Step [3/6], decoder loss: 0.0126, encoder1_activation l2 loss: 0.0000, total loss: 0.0126\n",
      "Epoch [188/200], Step [4/6], decoder loss: 0.0119, encoder1_activation l2 loss: 0.0000, total loss: 0.0119\n",
      "Epoch [188/200], Step [5/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [188/200], Step [6/6], decoder loss: 0.0133, encoder1_activation l2 loss: 0.0000, total loss: 0.0133\n",
      "Epoch [188/200] (avg 6 steps), decoder loss: 0.0121, total loss: 0.0121\n",
      "Epoch [189/200], Step [1/6], decoder loss: 0.0112, encoder1_activation l2 loss: 0.0000, total loss: 0.0112\n",
      "Epoch [189/200], Step [2/6], decoder loss: 0.0113, encoder1_activation l2 loss: 0.0000, total loss: 0.0113\n",
      "Epoch [189/200], Step [3/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [189/200], Step [4/6], decoder loss: 0.0130, encoder1_activation l2 loss: 0.0000, total loss: 0.0130\n",
      "Epoch [189/200], Step [5/6], decoder loss: 0.0116, encoder1_activation l2 loss: 0.0000, total loss: 0.0116\n",
      "Epoch [189/200], Step [6/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [189/200] (avg 6 steps), decoder loss: 0.0120, total loss: 0.0120\n",
      "Epoch [190/200], Step [1/6], decoder loss: 0.0107, encoder1_activation l2 loss: 0.0000, total loss: 0.0107\n",
      "Epoch [190/200], Step [2/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [190/200], Step [3/6], decoder loss: 0.0128, encoder1_activation l2 loss: 0.0000, total loss: 0.0128\n",
      "Epoch [190/200], Step [4/6], decoder loss: 0.0111, encoder1_activation l2 loss: 0.0000, total loss: 0.0111\n",
      "Epoch [190/200], Step [5/6], decoder loss: 0.0135, encoder1_activation l2 loss: 0.0000, total loss: 0.0135\n",
      "Epoch [190/200], Step [6/6], decoder loss: 0.0106, encoder1_activation l2 loss: 0.0000, total loss: 0.0106\n",
      "Epoch [190/200] (avg 6 steps), decoder loss: 0.0119, total loss: 0.0119\n",
      "Epoch [191/200], Step [1/6], decoder loss: 0.0119, encoder1_activation l2 loss: 0.0000, total loss: 0.0119\n",
      "Epoch [191/200], Step [2/6], decoder loss: 0.0118, encoder1_activation l2 loss: 0.0000, total loss: 0.0118\n",
      "Epoch [191/200], Step [3/6], decoder loss: 0.0115, encoder1_activation l2 loss: 0.0000, total loss: 0.0115\n",
      "Epoch [191/200], Step [4/6], decoder loss: 0.0117, encoder1_activation l2 loss: 0.0000, total loss: 0.0117\n",
      "Epoch [191/200], Step [5/6], decoder loss: 0.0134, encoder1_activation l2 loss: 0.0000, total loss: 0.0134\n",
      "Epoch [191/200], Step [6/6], decoder loss: 0.0111, encoder1_activation l2 loss: 0.0000, total loss: 0.0111\n",
      "Epoch [191/200] (avg 6 steps), decoder loss: 0.0119, total loss: 0.0119\n",
      "Epoch [192/200], Step [1/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [192/200], Step [2/6], decoder loss: 0.0129, encoder1_activation l2 loss: 0.0000, total loss: 0.0129\n",
      "Epoch [192/200], Step [3/6], decoder loss: 0.0111, encoder1_activation l2 loss: 0.0000, total loss: 0.0111\n",
      "Epoch [192/200], Step [4/6], decoder loss: 0.0115, encoder1_activation l2 loss: 0.0000, total loss: 0.0115\n",
      "Epoch [192/200], Step [5/6], decoder loss: 0.0128, encoder1_activation l2 loss: 0.0000, total loss: 0.0128\n",
      "Epoch [192/200], Step [6/6], decoder loss: 0.0109, encoder1_activation l2 loss: 0.0000, total loss: 0.0109\n",
      "Epoch [192/200] (avg 6 steps), decoder loss: 0.0119, total loss: 0.0119\n",
      "Epoch [193/200], Step [1/6], decoder loss: 0.0128, encoder1_activation l2 loss: 0.0000, total loss: 0.0128\n",
      "Epoch [193/200], Step [2/6], decoder loss: 0.0099, encoder1_activation l2 loss: 0.0000, total loss: 0.0099\n",
      "Epoch [193/200], Step [3/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [193/200], Step [4/6], decoder loss: 0.0122, encoder1_activation l2 loss: 0.0000, total loss: 0.0122\n",
      "Epoch [193/200], Step [5/6], decoder loss: 0.0116, encoder1_activation l2 loss: 0.0000, total loss: 0.0116\n",
      "Epoch [193/200], Step [6/6], decoder loss: 0.0133, encoder1_activation l2 loss: 0.0000, total loss: 0.0133\n",
      "Epoch [193/200] (avg 6 steps), decoder loss: 0.0120, total loss: 0.0120\n",
      "Epoch [194/200], Step [1/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [194/200], Step [2/6], decoder loss: 0.0105, encoder1_activation l2 loss: 0.0000, total loss: 0.0105\n",
      "Epoch [194/200], Step [3/6], decoder loss: 0.0129, encoder1_activation l2 loss: 0.0000, total loss: 0.0129\n",
      "Epoch [194/200], Step [4/6], decoder loss: 0.0118, encoder1_activation l2 loss: 0.0000, total loss: 0.0118\n",
      "Epoch [194/200], Step [5/6], decoder loss: 0.0113, encoder1_activation l2 loss: 0.0000, total loss: 0.0113\n",
      "Epoch [194/200], Step [6/6], decoder loss: 0.0129, encoder1_activation l2 loss: 0.0000, total loss: 0.0129\n",
      "Epoch [194/200] (avg 6 steps), decoder loss: 0.0120, total loss: 0.0120\n",
      "Epoch [195/200], Step [1/6], decoder loss: 0.0116, encoder1_activation l2 loss: 0.0000, total loss: 0.0116\n",
      "Epoch [195/200], Step [2/6], decoder loss: 0.0118, encoder1_activation l2 loss: 0.0000, total loss: 0.0118\n",
      "Epoch [195/200], Step [3/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [195/200], Step [4/6], decoder loss: 0.0128, encoder1_activation l2 loss: 0.0000, total loss: 0.0128\n",
      "Epoch [195/200], Step [5/6], decoder loss: 0.0127, encoder1_activation l2 loss: 0.0000, total loss: 0.0127\n",
      "Epoch [195/200], Step [6/6], decoder loss: 0.0095, encoder1_activation l2 loss: 0.0000, total loss: 0.0095\n",
      "Epoch [195/200] (avg 6 steps), decoder loss: 0.0118, total loss: 0.0118\n",
      "Epoch [196/200], Step [1/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [196/200], Step [2/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [196/200], Step [3/6], decoder loss: 0.0121, encoder1_activation l2 loss: 0.0000, total loss: 0.0121\n",
      "Epoch [196/200], Step [4/6], decoder loss: 0.0126, encoder1_activation l2 loss: 0.0000, total loss: 0.0126\n",
      "Epoch [196/200], Step [5/6], decoder loss: 0.0107, encoder1_activation l2 loss: 0.0000, total loss: 0.0107\n",
      "Epoch [196/200], Step [6/6], decoder loss: 0.0113, encoder1_activation l2 loss: 0.0000, total loss: 0.0113\n",
      "Epoch [196/200] (avg 6 steps), decoder loss: 0.0119, total loss: 0.0119\n",
      "Epoch [197/200], Step [1/6], decoder loss: 0.0125, encoder1_activation l2 loss: 0.0000, total loss: 0.0125\n",
      "Epoch [197/200], Step [2/6], decoder loss: 0.0115, encoder1_activation l2 loss: 0.0000, total loss: 0.0115\n",
      "Epoch [197/200], Step [3/6], decoder loss: 0.0120, encoder1_activation l2 loss: 0.0000, total loss: 0.0120\n",
      "Epoch [197/200], Step [4/6], decoder loss: 0.0115, encoder1_activation l2 loss: 0.0000, total loss: 0.0115\n",
      "Epoch [197/200], Step [5/6], decoder loss: 0.0116, encoder1_activation l2 loss: 0.0000, total loss: 0.0116\n",
      "Epoch [197/200], Step [6/6], decoder loss: 0.0130, encoder1_activation l2 loss: 0.0000, total loss: 0.0130\n",
      "Epoch [197/200] (avg 6 steps), decoder loss: 0.0120, total loss: 0.0120\n",
      "Epoch [198/200], Step [1/6], decoder loss: 0.0102, encoder1_activation l2 loss: 0.0000, total loss: 0.0102\n",
      "Epoch [198/200], Step [2/6], decoder loss: 0.0117, encoder1_activation l2 loss: 0.0000, total loss: 0.0117\n",
      "Epoch [198/200], Step [3/6], decoder loss: 0.0105, encoder1_activation l2 loss: 0.0000, total loss: 0.0105\n",
      "Epoch [198/200], Step [4/6], decoder loss: 0.0142, encoder1_activation l2 loss: 0.0000, total loss: 0.0142\n",
      "Epoch [198/200], Step [5/6], decoder loss: 0.0134, encoder1_activation l2 loss: 0.0000, total loss: 0.0134\n",
      "Epoch [198/200], Step [6/6], decoder loss: 0.0117, encoder1_activation l2 loss: 0.0000, total loss: 0.0117\n",
      "Epoch [198/200] (avg 6 steps), decoder loss: 0.0119, total loss: 0.0119\n",
      "Epoch [199/200], Step [1/6], decoder loss: 0.0119, encoder1_activation l2 loss: 0.0000, total loss: 0.0119\n",
      "Epoch [199/200], Step [2/6], decoder loss: 0.0136, encoder1_activation l2 loss: 0.0000, total loss: 0.0136\n",
      "Epoch [199/200], Step [3/6], decoder loss: 0.0131, encoder1_activation l2 loss: 0.0000, total loss: 0.0131\n",
      "Epoch [199/200], Step [4/6], decoder loss: 0.0115, encoder1_activation l2 loss: 0.0000, total loss: 0.0115\n",
      "Epoch [199/200], Step [5/6], decoder loss: 0.0107, encoder1_activation l2 loss: 0.0000, total loss: 0.0107\n",
      "Epoch [199/200], Step [6/6], decoder loss: 0.0100, encoder1_activation l2 loss: 0.0000, total loss: 0.0100\n",
      "Epoch [199/200] (avg 6 steps), decoder loss: 0.0118, total loss: 0.0118\n",
      "Epoch [200/200], Step [1/6], decoder loss: 0.0124, encoder1_activation l2 loss: 0.0000, total loss: 0.0124\n",
      "Epoch [200/200], Step [2/6], decoder loss: 0.0123, encoder1_activation l2 loss: 0.0000, total loss: 0.0123\n",
      "Epoch [200/200], Step [3/6], decoder loss: 0.0128, encoder1_activation l2 loss: 0.0000, total loss: 0.0128\n",
      "Epoch [200/200], Step [4/6], decoder loss: 0.0097, encoder1_activation l2 loss: 0.0000, total loss: 0.0097\n",
      "Epoch [200/200], Step [5/6], decoder loss: 0.0116, encoder1_activation l2 loss: 0.0000, total loss: 0.0116\n",
      "Epoch [200/200], Step [6/6], decoder loss: 0.0132, encoder1_activation l2 loss: 0.0000, total loss: 0.0132\n",
      "Epoch [200/200] (avg 6 steps), decoder loss: 0.0120, total loss: 0.0120\n"
     ]
    }
   ],
   "source": [
    "training_loss = ae.fit(train_data, 128, 200, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_loss, label=\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.7748, 0.7017, 0.5585,  ..., 0.0260, 0.0185, 0.0224],\n",
       "         [0.7748, 0.7017, 0.5585,  ..., 0.0260, 0.0185, 0.0224],\n",
       "         [0.7748, 0.7017, 0.5585,  ..., 0.0260, 0.0185, 0.0224],\n",
       "         ...,\n",
       "         [0.7748, 0.7017, 0.5585,  ..., 0.0260, 0.0185, 0.0224],\n",
       "         [0.7748, 0.7017, 0.5585,  ..., 0.0260, 0.0185, 0.0224],\n",
       "         [0.7748, 0.7017, 0.5585,  ..., 0.0260, 0.0185, 0.0224]],\n",
       "        grad_fn=<SigmoidBackward0>),\n",
       " tensor([0.0262, 0.0126, 0.0147, 0.0100, 0.0064, 0.0453, 0.0052, 0.0081, 0.0120,\n",
       "         0.0246, 0.0139, 0.0081, 0.0299, 0.0064, 0.0035, 0.0131, 0.0389, 0.0483,\n",
       "         0.0067, 0.0135, 0.0132, 0.0096, 0.0154, 0.0067, 0.0075, 0.0109, 0.0115,\n",
       "         0.0081, 0.0246, 0.0062, 0.0064, 0.0056, 0.0133, 0.0081, 0.0112, 0.0061,\n",
       "         0.0223, 0.0115, 0.0119, 0.0058, 0.0238, 0.0098, 0.0298, 0.0084, 0.0069,\n",
       "         0.0274, 0.0127, 0.0137, 0.0119, 0.0181, 0.0080, 0.0166, 0.0028, 0.0151,\n",
       "         0.0249, 0.0313, 0.0073, 0.0115, 0.0269, 0.0063, 0.0089, 0.0290, 0.0062,\n",
       "         0.0246, 0.0071, 0.0115, 0.0067, 0.0098, 0.0087, 0.0087, 0.0102, 0.0084,\n",
       "         0.0155, 0.0189, 0.0167, 0.0098, 0.0143, 0.0321, 0.0104, 0.0330, 0.0152,\n",
       "         0.0092, 0.0111, 0.0238, 0.0375, 0.0088, 0.0132, 0.0120, 0.0300, 0.0106,\n",
       "         0.0085, 0.0093, 0.0333, 0.0230, 0.0062, 0.0067, 0.0082, 0.0066, 0.0263,\n",
       "         0.0290, 0.0244, 0.0131, 0.0065, 0.0067, 0.0081, 0.0079, 0.0105, 0.0123,\n",
       "         0.0096, 0.0101, 0.0059, 0.0076, 0.0057, 0.0106, 0.0106, 0.0189, 0.0105,\n",
       "         0.0058, 0.0134, 0.0075, 0.0425, 0.0156, 0.0144, 0.0229, 0.0077, 0.0107,\n",
       "         0.0048, 0.1197, 0.0121, 0.0104, 0.0378, 0.0425, 0.0140, 0.0272, 0.0059,\n",
       "         0.0046, 0.0153, 0.0062, 0.0150, 0.0064, 0.0060, 0.0275, 0.0081, 0.0042,\n",
       "         0.0060, 0.0071, 0.0140, 0.0117, 0.0089, 0.0059, 0.0048, 0.0057, 0.0114,\n",
       "         0.0166, 0.0116, 0.0068, 0.0050, 0.0110, 0.0095, 0.0155, 0.0171, 0.0042,\n",
       "         0.0098, 0.0102, 0.0047, 0.0034, 0.0140, 0.0078, 0.0144, 0.0057, 0.0061,\n",
       "         0.0107, 0.0101, 0.0057, 0.0261, 0.0065, 0.0281, 0.0085, 0.0066, 0.0133,\n",
       "         0.0052, 0.0067, 0.0110, 0.0040, 0.0074, 0.0026, 0.0078, 0.0055, 0.0118,\n",
       "         0.0033, 0.0131, 0.0089, 0.0056, 0.0246, 0.0469, 0.0062, 0.0071, 0.0069,\n",
       "         0.0089, 0.0079, 0.0121, 0.0044, 0.0216, 0.0103, 0.0097, 0.0072, 0.0104,\n",
       "         0.0058, 0.0065, 0.0045, 0.0252, 0.0046, 0.0321, 0.0137, 0.0318, 0.0223,\n",
       "         0.0032, 0.0071, 0.0042, 0.0040, 0.0037, 0.0059, 0.0302, 0.0046, 0.0103,\n",
       "         0.0078, 0.0049, 0.0044, 0.0052, 0.0073, 0.0102, 0.0023, 0.0068, 0.0267,\n",
       "         0.0131, 0.0038, 0.0041, 0.0073, 0.0028, 0.0442, 0.0233, 0.0028, 0.0053,\n",
       "         0.0037, 0.0071, 0.0103, 0.0081, 0.0088, 0.0036, 0.0246, 0.0217, 0.0043,\n",
       "         0.0036, 0.0035, 0.0261, 0.0053, 0.0079, 0.0238, 0.0024, 0.0036, 0.0085,\n",
       "         0.0030, 0.0092, 0.0094, 0.0055, 0.0032, 0.0032, 0.0058, 0.0036, 0.0050,\n",
       "         0.0054, 0.0063, 0.0041, 0.0028, 0.0075, 0.0028, 0.0111, 0.0097, 0.0030,\n",
       "         0.0042, 0.0043, 0.0084, 0.0027, 0.0023, 0.0060, 0.0021, 0.0094, 0.0054,\n",
       "         0.0027, 0.0088, 0.0096, 0.0261, 0.0033, 0.0060, 0.0073, 0.0067, 0.0094,\n",
       "         0.0056, 0.0115, 0.0041, 0.0084, 0.0039, 0.0016, 0.0060, 0.0049, 0.0110,\n",
       "         0.0246, 0.0027, 0.0080, 0.0030, 0.0227, 0.0212, 0.0014, 0.0068, 0.0025,\n",
       "         0.0052, 0.0023, 0.0029, 0.0030, 0.0114, 0.0146, 0.0030, 0.0063, 0.0068,\n",
       "         0.0115, 0.0082, 0.0091, 0.0034, 0.0022, 0.0028, 0.0063, 0.0200, 0.0034,\n",
       "         0.0027, 0.0086, 0.0050, 0.0026, 0.0020, 0.0040, 0.0204, 0.0209, 0.0200,\n",
       "         0.0026, 0.0055, 0.0028, 0.0058, 0.0062, 0.0038, 0.0067, 0.0077, 0.0269,\n",
       "         0.0031, 0.0047, 0.0128, 0.0098, 0.0241, 0.0067, 0.0068, 0.0060, 0.0033,\n",
       "         0.0024, 0.0038, 0.0047, 0.0030, 0.0099, 0.0160, 0.0066, 0.0212, 0.0050,\n",
       "         0.0395, 0.0055, 0.0068, 0.0077, 0.0031, 0.0511, 0.0201, 0.0074, 0.0017,\n",
       "         0.0097, 0.0103, 0.0123, 0.0078, 0.0235, 0.0089, 0.0079, 0.0080, 0.0073,\n",
       "         0.0128, 0.0258, 0.0228, 0.0080, 0.0077, 0.0035, 0.0087, 0.0050, 0.0101,\n",
       "         0.0040, 0.0030, 0.0061, 0.0059, 0.0091, 0.0039, 0.0047, 0.0032, 0.0085,\n",
       "         0.0041, 0.0043, 0.0026, 0.0052, 0.0099, 0.0027, 0.0075, 0.0202, 0.0206,\n",
       "         0.0038, 0.0186, 0.0038, 0.0030, 0.0297, 0.0138, 0.0041, 0.0264, 0.0025,\n",
       "         0.0219, 0.0034, 0.0201, 0.0078, 0.0025, 0.0017, 0.0092, 0.0024, 0.0081,\n",
       "         0.0051, 0.0027, 0.0031, 0.0040, 0.0078, 0.0099, 0.0068, 0.0111, 0.0041,\n",
       "         0.0055, 0.0060, 0.0239, 0.0197, 0.0079, 0.0124, 0.0039, 0.0309, 0.0087,\n",
       "         0.0132, 0.0030, 0.0070, 0.0071, 0.0069, 0.0059, 0.0269, 0.0030, 0.0026,\n",
       "         0.0023, 0.0062, 0.0022, 0.0025, 0.0084, 0.0235, 0.0050, 0.0113, 0.0043,\n",
       "         0.0025, 0.0019, 0.0084, 0.0270, 0.0075, 0.0040, 0.0070, 0.0032, 0.0039,\n",
       "         0.0077, 0.0042, 0.0045, 0.0194, 0.0118, 0.0083, 0.0227, 0.0079, 0.0217,\n",
       "         0.0076, 0.0252, 0.0018, 0.0080, 0.0037, 0.0245, 0.0084, 0.0058, 0.0044,\n",
       "         0.0209, 0.0092, 0.0046, 0.0304, 0.0095, 0.0032, 0.0121, 0.0251, 0.0083,\n",
       "         0.0034, 0.0084, 0.0078, 0.0107, 0.0027, 0.0111, 0.0078, 0.0064, 0.0078,\n",
       "         0.0218, 0.0077, 0.0037, 0.0178, 0.0296, 0.0024, 0.0034, 0.0077, 0.0048,\n",
       "         0.0053, 0.0083, 0.0028, 0.0227, 0.0050, 0.0081, 0.0043, 0.0104, 0.0096,\n",
       "         0.0066, 0.0114, 0.0043, 0.0067, 0.0042, 0.0043, 0.0033, 0.0070, 0.0046,\n",
       "         0.0062, 0.0394, 0.0080, 0.0023, 0.0144, 0.0086, 0.0077, 0.0023, 0.0035,\n",
       "         0.0060, 0.0053, 0.0235, 0.0045, 0.0028, 0.0049, 0.0065, 0.0238, 0.0067,\n",
       "         0.0037, 0.0075, 0.0051, 0.0030, 0.0091, 0.0254, 0.0080, 0.0043, 0.0047,\n",
       "         0.0201, 0.0069, 0.0079, 0.0111, 0.0067, 0.0073, 0.0064, 0.0069, 0.0048,\n",
       "         0.0046, 0.0095, 0.0048, 0.0062, 0.0262, 0.0034, 0.0059, 0.0053, 0.0138,\n",
       "         0.0100, 0.0032, 0.0040, 0.0045, 0.0035, 0.0222, 0.0074, 0.0182, 0.0048,\n",
       "         0.0090, 0.0309, 0.0079, 0.0073, 0.0033, 0.0253, 0.0051, 0.0047, 0.0060,\n",
       "         0.0120, 0.0078, 0.0024, 0.0044, 0.0031, 0.0157, 0.0063, 0.0050, 0.0100,\n",
       "         0.0110, 0.0051, 0.0093, 0.0054, 0.0073, 0.0080, 0.0087, 0.0069, 0.0098,\n",
       "         0.0085, 0.0234, 0.0039, 0.0043, 0.0062, 0.0127, 0.0040, 0.0109, 0.0214,\n",
       "         0.0213, 0.0043, 0.0096, 0.0292, 0.0036, 0.0061, 0.0073, 0.0168, 0.0460,\n",
       "         0.0602, 0.0580, 0.0471, 0.0530, 0.0418, 0.0439, 0.0593, 0.0483, 0.0357,\n",
       "         0.0404, 0.0368, 0.0682, 0.0504, 0.0538, 0.0425, 0.0432, 0.0564, 0.0291,\n",
       "         0.0155, 0.0098, 0.0212, 0.0075, 0.0148, 0.0058, 0.0209, 0.0028, 0.0097,\n",
       "         0.0074, 0.0064, 0.0333, 0.0108, 0.0040, 0.0141, 0.0044, 0.0058, 0.0119,\n",
       "         0.0435, 0.0522, 0.0128, 0.0080, 0.0106, 0.0271, 0.0323, 0.0276, 0.0353,\n",
       "         0.0112, 0.0124, 0.0150, 0.0478, 0.0282, 0.0065, 0.0068, 0.0273, 0.0094,\n",
       "         0.0049, 0.0101, 0.0065, 0.0123, 0.0122, 0.0088, 0.0119, 0.0046, 0.0108,\n",
       "         0.0256, 0.0082, 0.0047, 0.0035, 0.0070, 0.0035, 0.0097, 0.0090, 0.0100,\n",
       "         0.0083, 0.0202, 0.0117, 0.0267], grad_fn=<MeanBackward1>),\n",
       " tensor(5.1107e-07, grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ae(torch.tensor(train_data.astype(np.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0262, 0.0126, 0.0147, 0.0100, 0.0064, 0.0453, 0.0052, 0.0081, 0.0120,\n",
       "        0.0246, 0.0139, 0.0081, 0.0299, 0.0064, 0.0035, 0.0131, 0.0389, 0.0483,\n",
       "        0.0067, 0.0135, 0.0132, 0.0096, 0.0154, 0.0067, 0.0075, 0.0109, 0.0115,\n",
       "        0.0081, 0.0246, 0.0062, 0.0064, 0.0056, 0.0133, 0.0081, 0.0112, 0.0061,\n",
       "        0.0223, 0.0115, 0.0119, 0.0058, 0.0238, 0.0098, 0.0298, 0.0084, 0.0069,\n",
       "        0.0274, 0.0127, 0.0137, 0.0119, 0.0181, 0.0080, 0.0166, 0.0028, 0.0151,\n",
       "        0.0249, 0.0313, 0.0073, 0.0115, 0.0269, 0.0063, 0.0089, 0.0290, 0.0062,\n",
       "        0.0246, 0.0071, 0.0115, 0.0067, 0.0098, 0.0087, 0.0087, 0.0102, 0.0084,\n",
       "        0.0155, 0.0189, 0.0167, 0.0098, 0.0143, 0.0321, 0.0104, 0.0330, 0.0152,\n",
       "        0.0092, 0.0111, 0.0238, 0.0375, 0.0088, 0.0132, 0.0120, 0.0300, 0.0106,\n",
       "        0.0085, 0.0093, 0.0333, 0.0230, 0.0062, 0.0067, 0.0082, 0.0066, 0.0263,\n",
       "        0.0290, 0.0244, 0.0131, 0.0065, 0.0067, 0.0081, 0.0079, 0.0105, 0.0123,\n",
       "        0.0096, 0.0101, 0.0059, 0.0076, 0.0057, 0.0106, 0.0106, 0.0189, 0.0105,\n",
       "        0.0058, 0.0134, 0.0075, 0.0425, 0.0156, 0.0144, 0.0229, 0.0077, 0.0107,\n",
       "        0.0048, 0.1197, 0.0121, 0.0104, 0.0378, 0.0425, 0.0140, 0.0272, 0.0059,\n",
       "        0.0046, 0.0153, 0.0062, 0.0150, 0.0064, 0.0060, 0.0275, 0.0081, 0.0042,\n",
       "        0.0060, 0.0071, 0.0140, 0.0117, 0.0089, 0.0059, 0.0048, 0.0057, 0.0114,\n",
       "        0.0166, 0.0116, 0.0068, 0.0050, 0.0110, 0.0095, 0.0155, 0.0171, 0.0042,\n",
       "        0.0098, 0.0102, 0.0047, 0.0034, 0.0140, 0.0078, 0.0144, 0.0057, 0.0061,\n",
       "        0.0107, 0.0101, 0.0057, 0.0261, 0.0065, 0.0281, 0.0085, 0.0066, 0.0133,\n",
       "        0.0052, 0.0067, 0.0110, 0.0040, 0.0074, 0.0026, 0.0078, 0.0055, 0.0118,\n",
       "        0.0033, 0.0131, 0.0089, 0.0056, 0.0246, 0.0469, 0.0062, 0.0071, 0.0069,\n",
       "        0.0089, 0.0079, 0.0121, 0.0044, 0.0216, 0.0103, 0.0097, 0.0072, 0.0104,\n",
       "        0.0058, 0.0065, 0.0045, 0.0252, 0.0046, 0.0321, 0.0137, 0.0318, 0.0223,\n",
       "        0.0032, 0.0071, 0.0042, 0.0040, 0.0037, 0.0059, 0.0302, 0.0046, 0.0103,\n",
       "        0.0078, 0.0049, 0.0044, 0.0052, 0.0073, 0.0102, 0.0023, 0.0068, 0.0267,\n",
       "        0.0131, 0.0038, 0.0041, 0.0073, 0.0028, 0.0442, 0.0233, 0.0028, 0.0053,\n",
       "        0.0037, 0.0071, 0.0103, 0.0081, 0.0088, 0.0036, 0.0246, 0.0217, 0.0043,\n",
       "        0.0036, 0.0035, 0.0261, 0.0053, 0.0079, 0.0238, 0.0024, 0.0036, 0.0085,\n",
       "        0.0030, 0.0092, 0.0094, 0.0055, 0.0032, 0.0032, 0.0058, 0.0036, 0.0050,\n",
       "        0.0054, 0.0063, 0.0041, 0.0028, 0.0075, 0.0028, 0.0111, 0.0097, 0.0030,\n",
       "        0.0042, 0.0043, 0.0084, 0.0027, 0.0023, 0.0060, 0.0021, 0.0094, 0.0054,\n",
       "        0.0027, 0.0088, 0.0096, 0.0261, 0.0033, 0.0060, 0.0073, 0.0067, 0.0094,\n",
       "        0.0056, 0.0115, 0.0041, 0.0084, 0.0039, 0.0016, 0.0060, 0.0049, 0.0110,\n",
       "        0.0246, 0.0027, 0.0080, 0.0030, 0.0227, 0.0212, 0.0014, 0.0068, 0.0025,\n",
       "        0.0052, 0.0023, 0.0029, 0.0030, 0.0114, 0.0146, 0.0030, 0.0063, 0.0068,\n",
       "        0.0115, 0.0082, 0.0091, 0.0034, 0.0022, 0.0028, 0.0063, 0.0200, 0.0034,\n",
       "        0.0027, 0.0086, 0.0050, 0.0026, 0.0020, 0.0040, 0.0204, 0.0209, 0.0200,\n",
       "        0.0026, 0.0055, 0.0028, 0.0058, 0.0062, 0.0038, 0.0067, 0.0077, 0.0269,\n",
       "        0.0031, 0.0047, 0.0128, 0.0098, 0.0241, 0.0067, 0.0068, 0.0060, 0.0033,\n",
       "        0.0024, 0.0038, 0.0047, 0.0030, 0.0099, 0.0160, 0.0066, 0.0212, 0.0050,\n",
       "        0.0395, 0.0055, 0.0068, 0.0077, 0.0031, 0.0511, 0.0201, 0.0074, 0.0017,\n",
       "        0.0097, 0.0103, 0.0123, 0.0078, 0.0235, 0.0089, 0.0079, 0.0080, 0.0073,\n",
       "        0.0128, 0.0258, 0.0228, 0.0080, 0.0077, 0.0035, 0.0087, 0.0050, 0.0101,\n",
       "        0.0040, 0.0030, 0.0061, 0.0059, 0.0091, 0.0039, 0.0047, 0.0032, 0.0085,\n",
       "        0.0041, 0.0043, 0.0026, 0.0052, 0.0099, 0.0027, 0.0075, 0.0202, 0.0206,\n",
       "        0.0038, 0.0186, 0.0038, 0.0030, 0.0297, 0.0138, 0.0041, 0.0264, 0.0025,\n",
       "        0.0219, 0.0034, 0.0201, 0.0078, 0.0025, 0.0017, 0.0092, 0.0024, 0.0081,\n",
       "        0.0051, 0.0027, 0.0031, 0.0040, 0.0078, 0.0099, 0.0068, 0.0111, 0.0041,\n",
       "        0.0055, 0.0060, 0.0239, 0.0197, 0.0079, 0.0124, 0.0039, 0.0309, 0.0087,\n",
       "        0.0132, 0.0030, 0.0070, 0.0071, 0.0069, 0.0059, 0.0269, 0.0030, 0.0026,\n",
       "        0.0023, 0.0062, 0.0022, 0.0025, 0.0084, 0.0235, 0.0050, 0.0113, 0.0043,\n",
       "        0.0025, 0.0019, 0.0084, 0.0270, 0.0075, 0.0040, 0.0070, 0.0032, 0.0039,\n",
       "        0.0077, 0.0042, 0.0045, 0.0194, 0.0118, 0.0083, 0.0227, 0.0079, 0.0217,\n",
       "        0.0076, 0.0252, 0.0018, 0.0080, 0.0037, 0.0245, 0.0084, 0.0058, 0.0044,\n",
       "        0.0209, 0.0092, 0.0046, 0.0304, 0.0095, 0.0032, 0.0121, 0.0251, 0.0083,\n",
       "        0.0034, 0.0084, 0.0078, 0.0107, 0.0027, 0.0111, 0.0078, 0.0064, 0.0078,\n",
       "        0.0218, 0.0077, 0.0037, 0.0178, 0.0296, 0.0024, 0.0034, 0.0077, 0.0048,\n",
       "        0.0053, 0.0083, 0.0028, 0.0227, 0.0050, 0.0081, 0.0043, 0.0104, 0.0096,\n",
       "        0.0066, 0.0114, 0.0043, 0.0067, 0.0042, 0.0043, 0.0033, 0.0070, 0.0046,\n",
       "        0.0062, 0.0394, 0.0080, 0.0023, 0.0144, 0.0086, 0.0077, 0.0023, 0.0035,\n",
       "        0.0060, 0.0053, 0.0235, 0.0045, 0.0028, 0.0049, 0.0065, 0.0238, 0.0067,\n",
       "        0.0037, 0.0075, 0.0051, 0.0030, 0.0091, 0.0254, 0.0080, 0.0043, 0.0047,\n",
       "        0.0201, 0.0069, 0.0079, 0.0111, 0.0067, 0.0073, 0.0064, 0.0069, 0.0048,\n",
       "        0.0046, 0.0095, 0.0048, 0.0062, 0.0262, 0.0034, 0.0059, 0.0053, 0.0138,\n",
       "        0.0100, 0.0032, 0.0040, 0.0045, 0.0035, 0.0222, 0.0074, 0.0182, 0.0048,\n",
       "        0.0090, 0.0309, 0.0079, 0.0073, 0.0033, 0.0253, 0.0051, 0.0047, 0.0060,\n",
       "        0.0120, 0.0078, 0.0024, 0.0044, 0.0031, 0.0157, 0.0063, 0.0050, 0.0100,\n",
       "        0.0110, 0.0051, 0.0093, 0.0054, 0.0073, 0.0080, 0.0087, 0.0069, 0.0098,\n",
       "        0.0085, 0.0234, 0.0039, 0.0043, 0.0062, 0.0127, 0.0040, 0.0109, 0.0214,\n",
       "        0.0213, 0.0043, 0.0096, 0.0292, 0.0036, 0.0061, 0.0073, 0.0168, 0.0460,\n",
       "        0.0602, 0.0580, 0.0471, 0.0530, 0.0418, 0.0439, 0.0593, 0.0483, 0.0357,\n",
       "        0.0404, 0.0368, 0.0682, 0.0504, 0.0538, 0.0425, 0.0432, 0.0564, 0.0291,\n",
       "        0.0155, 0.0098, 0.0212, 0.0075, 0.0148, 0.0058, 0.0209, 0.0028, 0.0097,\n",
       "        0.0074, 0.0064, 0.0333, 0.0108, 0.0040, 0.0141, 0.0044, 0.0058, 0.0119,\n",
       "        0.0435, 0.0522, 0.0128, 0.0080, 0.0106, 0.0271, 0.0323, 0.0276, 0.0353,\n",
       "        0.0112, 0.0124, 0.0150, 0.0478, 0.0282, 0.0065, 0.0068, 0.0273, 0.0094,\n",
       "        0.0049, 0.0101, 0.0065, 0.0123, 0.0122, 0.0088, 0.0119, 0.0046, 0.0108,\n",
       "        0.0256, 0.0082, 0.0047, 0.0035, 0.0070, 0.0035, 0.0097, 0.0090, 0.0100,\n",
       "        0.0083, 0.0202, 0.0117, 0.0267], grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class InferenceAutoencoder(Autoencoder):\n",
    "    def forward(self, x):\n",
    "        prep = self.prep_layer(x)\n",
    "        encoded = self.encoder1(prep)\n",
    "        encoded = self.encoder1_activation(encoded)\n",
    "        encoded = self.encoder2(encoded)\n",
    "        encoded = self.encoder2_activation(encoded)\n",
    "        decoded = self.decoder(encoded)\n",
    "        mse = self.mse_loss(prep, decoded)\n",
    "        return torch.mean(mse, dim=1)  # Only return mse loss\n",
    "    \n",
    "iae = InferenceAutoencoder(input_shape=(51,), l2_lambda=1e-4)\n",
    "iae.load_state_dict(ae.state_dict())\n",
    "iae(torch.tensor(train_data.astype(np.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"./models/torch\", exist_ok=True)\n",
    "\n",
    "model_path = \"./models/torch/state_dict.pth\"\n",
    "torch.save(ae.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-examples-EfRPkUGI-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
