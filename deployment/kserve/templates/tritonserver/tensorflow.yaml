apiVersion: "serving.kserve.io/v1beta1"
kind: "InferenceService"
metadata:
  name: {{ inference_service_resource_name }}
  annotations:
    sidecar.istio.io/inject: "false"
spec:
  predictor:
    serviceAccountName: {{ service_account_resource_name }}
    model:
      protocolVersion: v2
      modelFormat:
        name: tensorflow
      runtime: kserve-tritonserver
      storageUri: {{ s3_model_root_path }}